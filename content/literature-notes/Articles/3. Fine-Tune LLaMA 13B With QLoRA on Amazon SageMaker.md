---
author: [[Philipp Schmid]]
title: "3. Fine-Tune LLaMA 13B With QLoRA on Amazon SageMaker"
tags: 
- articles
- literature-note
---
# 3. Fine-Tune LLaMA 13B With QLoRA on Amazon SageMaker

![rw-book-cover](https://www.philschmid.de/static/blog/sagemaker-llama2-qlora/thumbnail.jpg)

## Metadata
- Author: [[Philipp Schmid]]
- Full Title: 3. Fine-Tune LLaMA 13B With QLoRA on Amazon SageMaker
- Category: #articles
- URL: https://www.philschmid.de/sagemaker-llama2-qlora

## Highlights
- Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters ([View Highlight](https://read.readwise.io/read/01h5q363sryqytrc465zjz76cp))
- QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:
  • Quantize the pretrained model to 4 bits and freezing it.
  • Attach small, trainable adapter layers. (LoRA)
  • Finetune only the adapter layers, while using the frozen quantized model for context. ([View Highlight](https://read.readwise.io/read/01h5q385mhse2bpdcwtwp0d95r))
