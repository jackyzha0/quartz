# Econ 378 Lecture Notes 

# Joseph McMurray 

## L0 Introduction 

1. Opening Prayer 

2. About me 

 a. Raised in Salt Lake City, mission in Seoul Korea, Economics major at BYU, met my wife at 

 BYU, PhD at University of Rochester, research in political economics (also teach Econ 

 477), 4 kids, sincerely believe in the Gospel of Jesus Christ and the mission of BYU. 

 b. I enjoy teaching Econ 378 because the material is so useful for students, which is 

 rewarding. It is also hard, so making it interesting and easy is a fun challenge. 

3. Data analysis in Economics 

 a. Scientific method: observe patterns, theorize, test theories, policy implications, policy 

 calibration 

 b. Theory: Econ 110, 380-382, 400+ 

 c. Evidence: Econ 378, 388, 400+, Research, internships, jobs (big data industrial transition) 

 d. Economics is both (recommend Econ 210 for exploring careers in Economics, also MATH 

 213/215 linear algebra) 

4. Probability and statistics 

 a. Often care about population but observe only sample. 

 Canâ€™t know whatâ€™s true, but can know whatâ€™s probably true 

 b. Probability is the language of uncertainty 

 c. Probability theory also useful in theoretical models of risk/uncertainty (e.g. insurance, 

 investments, search, asymmetric information) 

5. Syllabus 

 a. Materials, participation, homework, exams, project 

 b. How to choose a research topic 

 c. Finish part 1 (data collection) on time! After the midterm, homework will include data 

 analysis from your own projects. 

---

## L1 Math Preview 

1. Spiritual thought: like Joseph in Egypt, your time at BYU is 7 years of plenty (spiritual abundance). Likely less so when you go to graduate school or workforce. Store up all you can now (e.g. devotionals, religion classes, student ward, ministering), like wise virgins with oil lamps, to sustain you as you â€œgo forth to serveâ€ 
 2. In a similar (but temporal) way, this lecture and HW 1 seek to fill your â€œmath lampsâ€ in 
 preparation for the rest of the semester.  
3. Factorials 
 a. 5!= 5 â‹… 4 â‹… 3 â‹… 2 â‹… 1  
 b. 0!= 1  
4. Exponents 
 a. ğ‘’ğ‘’â‰ˆ2. 7 denotes growth 
	 i. $1 invested at 100% interest, compound annually, equals $2 a year later 
	 ii. $1 invested at 100% interest, compound continuously, equals $2.72 a year later 

Expression | Simplified / Rewritten 
---|---
$x^-1$|$1/x$ 
$x^1/2$|$\sqrt{x}$
$x^0$|$1$
$x^2x^3$|$x^5$
$(x^2)^3$|$x^6$
$e^xe^y$|$e^{x+y}$
$e^x/e^y$|$e^{x-y}$
$e^{x+y}$|$e^xe^y$
$e^{x-y}$|$e^x/e^y$

5. Logarithms 
	1.  $Log_{10}100 = 2$(How many powers of 10 give you 100 ?)
		1. $log(. 01)=âˆ’ 2$  
	2.  $ln(100)â‰ˆ4.6$(How many powers of ğ‘’ğ‘’â‰ˆ2. 7 give you 100 ?) 
		1. $ln(. 01)=âˆ’4. 6$
	 3. Logs makes huge numbers smaller, miniscule numbers (e.g. probabilities) bigger 
	 4. Inverse of exponents 
		1. $ln(e^5)=5$ (How many powers of ğ‘’ğ‘’ does it take to reach $e^5?$ 
		2. $e^{ln(5)}=5$ (It takes ln(5) powers of ğ‘’ to make 5; what if we take ğ‘’ to that many powers?) 
		
Expression | Simplified / Rewritten 
---|---
$ln(xy)$|$ln(x)+ln(y)$
$ln(\frac{x}{y})$|$ln(x)-ln(y)$
6. Summation 

 a. âˆ‘ ğ‘˜ğ‘˜ 

 5 2 

 ğ‘˜ğ‘˜=1 

#### = 1 

 2 

#### + 2 

 2 

#### + 3 

 2 

#### + 4 

 2 

#### + 5 

 2 

#### = 55 

 b. Column of ğ‘›ğ‘›= 500 observations can be denoted by ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 , with ğ‘–ğ‘–=1,...,ğ‘›ğ‘› 

 c. 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 denotes the average 

 d. âˆ‘ 3 ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 3 âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 e. 

#### âˆ‘ 

#### (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### +ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### + 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 f. 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### + 3 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### + 

#### âˆ‘ 

#### 3 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### + 3 ğ‘›ğ‘› 

 g. Does âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 ? No! 

 i. e.g. 2 â‹…3 + 5 â‹… 4 â‰ (2 +5)(3 +4) 

7. Derivatives 

---

 a. Intuition: limit of slope 

 b. Finding maximum/minimum 

 i. First-order condition: ğ‘“ğ‘“ 

 â€² 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### = 0 

 ii. Second-order condition: ğ‘“ğ‘“ 

 â€²â€² 

 (ğ‘¥ğ‘¥) negative for max (slope is decreasing, function 

 makes a frown), positive for min (slope is increasing, function makes a smile) 

 c. Simple derivatives 

### ğ‘“ğ‘“ 

### ( 

### ğ‘¥ğ‘¥ 

### ) 

### ğ‘“ğ‘“ 

 â€² 

### ( 

### ğ‘¥ğ‘¥ 

### ) 

### ğ‘¥ğ‘¥ 

 3 

#### 3 ğ‘¥ğ‘¥ 

 2 

### 4 ğ‘¥ğ‘¥ 

#### 4 

### 4 

#### 0 

### 1 

### ğ‘¥ğ‘¥ 

#### âˆ’ 

#### 1 

#### ğ‘¥ğ‘¥ 

 2 

### âˆšğ‘¥ğ‘¥ 

#### 1 

#### 2 

#### ğ‘¥ğ‘¥ 

 âˆ’ 

 1 

 2 = 

#### 1 

#### 2 

#### âˆš 

#### ğ‘¥ğ‘¥ 

### ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

#### ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

### ln (ğ‘¥ğ‘¥) 

#### 1 

#### ğ‘¥ğ‘¥ 

 d. Product rule: 

 ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

#### [ 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘”ğ‘”(ğ‘¥ğ‘¥) 

#### ] 

#### =ğ‘“ğ‘“ 

 â€² 

#### (ğ‘¥ğ‘¥)ğ‘”ğ‘”(ğ‘¥ğ‘¥)+ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘”ğ‘”â€²(ğ‘¥ğ‘¥) 

 i. 

 ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

#### ğ‘¥ğ‘¥ 

 2 

 ln(ğ‘¥ğ‘¥)= 2 ğ‘¥ğ‘¥ln(ğ‘¥ğ‘¥)+ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 1 

 ğ‘¥ğ‘¥ 

#### ï¿½ 

 ii. Same pattern for products of 100 terms 

 e. Chain rule: 

 ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

#### ğ‘“ğ‘“ï¿½ğ‘”ğ‘”(ğ‘¥ğ‘¥)ï¿½=ğ‘“ğ‘“ 

 â€² 

#### ï¿½ğ‘”ğ‘”(ğ‘¥ğ‘¥)ï¿½ğ‘”ğ‘” 

 â€² 

#### (ğ‘¥ğ‘¥)= 

 ğ‘‘ğ‘‘ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 i. Example: 

 ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 ln(ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 3 ğ‘¥ğ‘¥+ 1 )= 

 1 

 ğ‘¥ğ‘¥ 

 2 

 âˆ’3ğ‘¥ğ‘¥+1 

#### â‹…(2ğ‘¥ğ‘¥ âˆ’3) 

 ii. Example: 

 ğ‘‘ğ‘‘ 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

#### ğ‘’ğ‘’ 

 âˆ’3ğ‘¥ğ‘¥ 

 2 

#### =ğ‘’ğ‘’ 

 âˆ’3ğ‘¥ğ‘¥ 

 2 

#### (âˆ’ 6 ğ‘¥ğ‘¥) 

 iii. Same pattern for longer chains 

 f. [The Quotient rule is useful as well, but I wonâ€™t require it here.] 

8. Integrals 

---

g. Intuition 

 i. â€œsumâ€/area under ğ‘“ğ‘“ (negative if ğ‘“ğ‘“< 0 ) 

 ii. Anti-derivative: add up âˆ« 

#### ğ‘“ğ‘“â€²(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 ğ‘ğ‘ 

 ğ‘ğ‘ 

 to get ğ‘“ğ‘“(ğ‘ğ‘)âˆ’ğ‘“ğ‘“(ğ‘ğ‘) 

 ğ‘“ğ‘“(ğ‘¥ğ‘¥) Anti-derivative of ğ‘“ğ‘“(ğ‘¥ğ‘¥) 

#### ğ‘¥ğ‘¥ 

 2 

 1 

#### 3 

#### ğ‘¥ğ‘¥ 

 3 

#### 4 4 ğ‘¥ğ‘¥ 

#### 1 

#### ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 

#### 1 

#### ğ‘¥ğ‘¥ 

#### âˆš 

#### ğ‘¥ğ‘¥ 

#### 2 

#### 3 

#### ğ‘¥ğ‘¥ 

 3 

 2 

#### ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

#### ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

#### ğ‘¥ğ‘¥(ğ‘¥ğ‘¥âˆ’ 1 ) 

#### 1 

#### 3 

#### ğ‘¥ğ‘¥ 

 3 

#### âˆ’ 

#### 1 

#### 2 

#### ğ‘¥ğ‘¥ 

 2 

h. Definite integral âˆ« ğ‘¥ğ‘¥ 

 2 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 7 

 4 

#### =ï¿½ 

 1 

 3 

#### ğ‘¥ğ‘¥ 

 3 

#### ï¿½ 

 ğ‘¥ğ‘¥=4 

 7 

#### = 

 1 

 3 

#### ( 7 ) 

 3 

#### âˆ’ 

 1 

 3 

#### ( 4 ) 

 3 

#### = 

 343 

 3 

#### âˆ’ 

 64 

 3 

#### = 93 

 i. âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 4 

 7 

#### = 

 64 

 3 

#### âˆ’ 

 343 

 3 

#### =âˆ’ 93 

i. Useful techniques that I wonâ€™t cover (or expect you to know) 

 i. ğ‘¢ğ‘¢-substitution (chain rule in reverse) 

 ii. Integration by Parts (product rule in reverse) 

j. Double Integrals 

 i. Simple: inside integral then outside integral 

#### ï¿½ ï¿½ ğ‘¥ğ‘¥ 

 2 

#### ğ‘¥ğ‘¥ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 3 

 ğ‘¦ğ‘¦=1 

#### =ï¿½ ï¿½ 

#### ğ‘¥ğ‘¥ 

#### 3 

#### ğ‘¥ğ‘¥ 

 3 

#### ï¿½ 

 ğ‘¥ğ‘¥=0 

 ğ‘¥ğ‘¥=2 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 3 

 ğ‘¦ğ‘¦=1 

#### =ï¿½ 

#### 8 

#### 3 

#### ğ‘¥ğ‘¥ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 3 

 ğ‘¦ğ‘¦=1 

#### =â‹¯= 

#### 32 

#### 3 

 ii. Note: for rectangular bounds (i.e. bounds of ğ‘¥ğ‘¥ donâ€™t depend on ğ‘¥ğ‘¥, and vice 

 versa), can integrate in reverse order 

#### ï¿½ ï¿½ ğ‘¥ğ‘¥ 

 2 

#### ğ‘¥ğ‘¥ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 3 

 ğ‘¦ğ‘¦=1 

 2 

 ğ‘¥ğ‘¥=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥=ï¿½ ï¿½ 

#### 1 

#### 2 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 ğ‘¦ğ‘¦=1 

 ğ‘¦ğ‘¦=3 2 

 ğ‘¥ğ‘¥=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥=ï¿½ 4 ğ‘¥ğ‘¥ 

 2 

 2 

 ğ‘¥ğ‘¥=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥=â‹¯= 

#### 32 

#### 3 

 iii. Practice âˆ« âˆ« 

 ğ‘¥ğ‘¥ 

 ğ‘¦ğ‘¦ 

 2 

 ğ‘¥ğ‘¥=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 3 

 ğ‘¦ğ‘¦=1 

---

## L2 Statistics preview, Excel 

Introduction 

1. We recently revised the Econ 378 curriculum. Formerly, we started with basic theory and the 

 basic tools based on it, introduced complex theory with complex tools, then more complex 

 theory and more complex tools. This seemed reasonable, but I realized: â€œStill to this day, Iâ€™ve 

 never learned to build a car, but even without knowing how to build one, I managed to learn to 

 drive one.â€ 

2. Now: learn basic, complex, and more complex tools upfront. Then go learn the underlying 

 theory. 

Spreadsheets 

1. Unit of observation 

2. Quantitative variables 

3. Binary variables 

 a. Categorical variables as binary (or â€œdummyâ€) variables 

Excel basics 

1. Calculations 

 a. Arithmetic 

 b. Average, etc. 

2. Formulas 

 a. Example: convert GDP to per capita GDP 

 b. Example: convert per capita GDP to per capita GDP change 

 c. Example: convert per capita GDP change to per capita GDP % change 

3. Help files 

4. Transpose 

5. Sort 

6. Filter 

7. Boolean 

Data Visualization 

1. Single variables 

 a. Binary variables: Pie charts 

 b. Quantitative variables: Histograms 

2. Interactions 

 a. Two binary variables: Double pie charts 

 b. Binary and quantitative: bar chart 

 c. Two quantitative: scatter chart 

 i. Quantitative & time: line graph 

---

 d. Three variables 

 i. Two binary & quantitative: clustered bar chart 

 ii. Two quantitative & binary: color-coded scatter chart 

 iii. Three quantitative: bubble chart 

Summary statistics 

1. Proportions 

2. Mean 

 a. From histogram, eyeball center of gravity 

3. Median/percentiles 

4. Mode 

5. Standard deviation 

 a. Rule of thumb: two standard deviations 

 b. Chebyshevâ€™s inequality: % of population outside ğ‘˜ğ‘˜ standard deviations canâ€™t exceed 

 1 

 ğ‘˜ğ‘˜ 

 2 

6. Correlation coefficient 

7. Regressions 

 a. Slope & intercept 

 i. Predict ğ‘¥ğ‘¥ for any ğ‘¥ğ‘¥ 

 ii. Predict future! 

 iii. Counterfactual â€œexperimentsâ€ (way less costly than real experiments) 

 b. ğ‘…ğ‘… 

 2 

 (coefficient of determination) 

 c. Error terms / detrended data 

 d. Multiple regression 

Estimation 

1. Population / samples 

 a. Importance of representative sample 

2. Point estimates, interval estimates / margin of error 

3. Hypothesis test 

Learning Statistics 

1. We just finished semester (overview). You can now do everything by computer. Rest of 

 semester, weâ€™ll go back and do them by hand. 

2. Why work by hand? Important to know what computer is doing. (GIGO) 

 a. Pushing a button works great unless a situation arises when the standard button is the 

 wrong one to use. We need to know the limitations of the standard techniques and 

 how to modify them appropriately. 

 b. Car analogy: why insist on building cars when we could just drive them? Analogy 

 incomplete: I can objectively verify that Iâ€™ve correctly driven a car; I canâ€™t objectively 

 verify that Iâ€™ve correctly used statistics. In the real world of research projects, 

---

 internships, and jobs, there is no answer key! We only know our answers are correct if 

 we know weâ€™ve done them correctly, and thatâ€™s only possible if we understand deeply 

 what theoretical basis underlies the tools weâ€™re using. 

3. Simple things (e.g. margin of error) mask extremely complex background. Understanding entire 

 background is essential for confidence that weâ€™re using statistical formulas appropriately. 

 (Sometimes, tweaks are necessary.) 

4. Spiritual analogy: the â€œwhyâ€ of the gospel. If atheist friend is kind and righteous already, why 

 need doctrine? Even more happiness. Example: doctrine of eternal marriage informs decisions 

 to resolve conflicts, versus divorce. 

## L3 Research Design 

Research questions 

1. There are two important ingredients to a good research study: a good question, and a good 

 methodology for finding an answer 

2. Question selection 

 a. What ideas do you already have for data analysis projects? 

 b. What (topic) are you excited / passionate about? 

 c. If you had a crystal ball, what would you ask? 

 d. What if you had a crystal ball that could answer anything but that? What would you 

 need to ask so that you can figure out your own answer to the main question? 

 e. Continue until so narrow you can collect your own data (the more specific, the better) 

 f. Given (time and money) budget constraints, your project may need to settle for similar 

 data 

 i. Similar variables 

 ii. A few observations 

 iii. â€œPilot studyâ€: this is often what is done in real world 

 iv. Proof of concept (consulting sales pitch): can even use fake data 

3. Data mining 

 a. Given data (e.g. from a private business, a consulting client, etc.), ask, â€œwhat can we 

 learn?â€ and â€œwho is interested?â€ 

 b. Example: private business data 

 c. Typically needs to be paired with research question process above 

 d. Example: what would CEO want to know? 

---

Correlation may not mean causation! 

1. Three possibilities 

2. Causation: ğ‘‹ğ‘‹ â‡’ğ‘Œğ‘Œ 

 Theory: cell phones â‡’ distraction â‡’ accidents 

 Policy implication: banning cell phones will reduce car accidents 

3. Reverse causation: ğ‘‹ğ‘‹â‡ğ‘Œğ‘Œ 

 Not likely in this case (car accidents cause cell phone use?) 

4. Lurking variable: ğ‘‹ğ‘‹â‡ğ‘ğ‘â‡’ğ‘Œğ‘Œ 

 Example: careless (teenage?) drivers are prone independently both to use cell phones and 

 (regardless of cell phone use) to get in accidents 

 Policy implication: banning cell phones will not reduce car accidents 

5. Historic instances of conflating correlation with causation 

 a. The â€œPhillips curveâ€ documented a negative correlation between inflation and 

 unemployment, suggesting to policy makers that monetary policy could only avoid one 

 problem by embracing the other. They printed more money in the 1970s, hoping to 

 lower unemployment, but discovered â€œstagflationâ€: the coincidence of high 

 unemployment and high inflation. 

 b. Documenting a positive correlation between on-the-job computer use and income, 

 Krueger (QJE 1993) concluded that computers increase productivity, and recommended 

 policies to increase computer use. Using similar data, however, DiNardo and Pischke 

 (QJE 1997) showed that income is also correlated with pencil use on the job and argued 

 (tongue-in-cheek) that subsidizing pencils would be a much more cost-effective 

 intervention. 

 c. Can also conflate lack of correlation with lack of causation: in yesterdayâ€™s covid example, 

 we derived that ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ğ‘ğ‘ğ‘ 

#### | 

#### ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ 

#### ) 

#### =5. 3âˆ— 10 

 âˆ’5 

 and ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ğ‘ğ‘ğ‘ 

#### | 

#### ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ 

#### ) 

#### = 

 214 

 1 , 302 , 912 

#### = 16 .4âˆ— 10 

 âˆ’5 

#### , 

 so vaccine is 68% effective. Correlation of ğ‘ğ‘ğ‘ğ‘ and ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ is negative, but weak. If further 

 condition on age (<50 vs. >50): 

 i. ğ‘ƒğ‘ƒ(ğ‘ğ‘ğ‘ğ‘|ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥< 50 )= 

 11 

 3 , 501 , 118 

#### =.3âˆ— 10 

 âˆ’5 

#### ğ‘ƒğ‘ƒ(ğ‘ğ‘ğ‘ğ‘|ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥< 50 )= 

 43 

 1 , 116 , 834 

#### =3. 9âˆ— 10 

 âˆ’5 

 Vaccine 92% effective for this group. 

---

 ii. ğ‘ƒğ‘ƒ(ğ‘ğ‘ğ‘ğ‘|ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥> 50 )= 

 290 

 2 , 133 , 516 

#### = 13 .6âˆ— 10 

 âˆ’5 

#### ğ‘ƒğ‘ƒ(ğ‘ğ‘ğ‘ğ‘|ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥> 50 )= 

 171 

 186 , 078 

#### = 91 .9âˆ— 10 

 âˆ’5 

 Vaccine 85% effective for this group. 

 iii. If condition further, vaccine efficacy by age: 

 Age Vaccine efficacy Age Vaccine efficacy 

#### 12 15 100% 50 59 93% 

#### 16 19 100% 60 69 89% 

#### 20 29 100% 70 79 90% 

#### 30 39 97% 80 89 81% 

#### 40 49 94% 90+ 92% 

 iv. Biggest determinant of covid is age (overall, 90+ over 1000 times more likely 

 than 12-15 to be hospitalized with covid), not vaccine. Since people of all ages 

 got vaccinated, corr(vax,cv) gets weaker when not conditioning in age than 

 when conditioning on age. But even for oldest groups (where most 

 â€œbreakthroughâ€ cases are occurring), vaccinated do way better than 

 unvaccinated. 

 d. These examples underscore importance of careful data work, understanding statistics! 

 Good intentions can easily be led astray by statistical subtleties. 

Establishing causation (this is most of the work in economics) 

1. Random experiment 

 a. Best method 

 b. Example: force group A to drive with cell phone, group B to not 

 c. Often impractical, ethically and/or logistically (e.g. only one national economy; no 

 control group) or even impossible (e.g. race/gender) 

 d. Natural experiment: wait for nature to run experiments 

 i. These are rare, might wait a long time 

 ii. Government policy randomly allocates permits for some drivers to use cell 

 phones. 

 iii. Angrist and Evans (1998): Does having more children affect motherâ€™s income? 

 Lurking variables and reverse causation both likely. But parents whose second 

---

 child was (randomly) same gender as first child were more likely to have third 

 child, (temporarily) reduced (poor) motherâ€™s income 

 iv. Angrist (1990): What impact (positive or negative) did military service have on 

 men with (randomly) high Vietnam draft numbers had 15% lower incomes years 

 later. 

 v. Clever researchers keep eyes open for natural randomness, ask â€œwhat can we 

 learn?â€ 

 vi. Sources of randomness: lottery numbers (e.g. gambling, school choice, scarce 

 social program), random executive decisions (e.g. dorm rooms, judge 

 assignment, advertising), weather, earthquakes, accidents, terrorist attacks 

2. Second best: quasi-experiment 

 a. Example: cell phones legal in one state, illegal in another 

 b. Problem: other reasons for differential accidents (e.g. speed limits, enforcement, roads, 

 recklessness?) 

 c. Refinement: large number of states; before/after cell phone law change 

 d. Pope (1989, BYU): Geneva Steel closed then reopened six months later, concomitant 

 decrease then increase in local hospitalizations for pneumonia, pleurisy, bronchitis, 

 asthma. (Landmark study in air pollution.) 

 e. Sargeant et al. (2004): Restaurant smoking ban in Montana, repealed after six months. 

 Heart attacks dropped 40%, then went back up. 

 f. Lee et al. (2004): How does politician (Democrat/Republican) affect policy outcomes? 

 Random election? No. But in close elections (e.g. 48-52% votes), winning or losing was 

 plausibly random. 

 g. Possible sources of quasi-randomness: cutoffs (e.g. grades, income thresholds, 

 performance thresholds, birth date), bureaucratic decisions that are not literally random 

 but seem arbitrary (e.g. regulatory decisions, tax levels, regularly/tax hike timing, pre

 /post-construction project, mission assignment) 

3. Controls 

 a. Compare sub-populations to â€œcontrolâ€ for lurking variables 

 b. Most common method (since others infeasible) 

 c. Example: compare cell phone use and accidents among teenagers/adults 

 vii. Other proxies for recklessness: grades? debt? Often imperfect 

---

 d. Econ 378: restrict sample (Econ 388: regressions) 

Prediction 

1. If correlation does not reflect causation, ğ‘‹ğ‘‹ cannot be used to control ğ‘Œğ‘Œ, but still can be used to 

 predict ğ‘Œğ‘Œ 

 a. Example: reduced car insurance premiums for good grades, females, good driving 

 history, yellow cars 

 b. Ethics of â€œstatistical discriminationâ€ (e.g. traffic stops for blacks, airport scrutiny for 

 Arabs) 

 c. Role of theory is to posit reasons for correlation; essential if anything changes (e.g. cell 

 phones get cheaper). 

Research Design for Causal Inference 

1. Many of the topics weâ€™re interested in seek to establish cause/effect relationships. 

 a. What examples did you come up with? (e.g. Do masks reduce covid spread?) 

 b. Were there any topics you came up with that do not involve cause/effect relationships? 

 (Probably not.) 

2. What is a cause/effect relationship you would like to discover? 

3. Which variables might have a simple correlation that suggests the relationship above? 

4. Are there any competing forces that might produce the opposite correlation? If the correlation 

 turns out to be consistent with a hypothesized cause/effect relationship, the hypothesized 

 relationship might outweigh any competing forces. 

5. But are there other mechanisms that could produce the same correlation? If so, finding a 

 correlation where you expected it does not guarantee that the hypothesized cause/effect 

 relationship is valid. 

6. This raises a new question: where could we look for evidence of the hypothesized cause/effect 

 relationship that would not pick up correlations for these other reasons? 

7. This is the key question of research design. Note that you can (and should!) think through and 

 plan out your response to these issues before you ever look at the data. 

Research design in the quest for spiritual truth 

---

1. A friend, skeptical of spiritual things, recommended the following experiment: go to a hospital, 

 pray for people in every other room. See if they recover more quickly/fully than the others. (His 

 prediction: no.) Is this a valid statistical test of the validity of prayer? Why or why not? 

2. Research design is important in answering spiritual questions, too: 

 a. What do the scriptures say about experiments to uncover spiritual truth? 

 b. â€œIf any of you lack wisdom, let him ask of God, who giveth to all men liberally, and 

 upbraideth not; and it shall be given him. But let him ask in faith , nothing wavering.â€ 

 (James 1:5-6, emphasis added) 

 c. â€œAnd when ye shall receive these things, I would exhort you that ye would ask God, the 

 Eternal Father, in the name of Christ, if these things are not true; and if ye shall ask with 

 a sincere heart, with real intent, having faith in Christ , he will manifest the truth of it 

 unto you, by the power of the Holy Ghost.â€ (Moroni 10:4, emphasis added) 

 d. â€œNow, we will compare the word unto a seed. Now, if ye give place, that a seed may be 

 planted in your heart , behold, if it be a true seed, or a good seed, if ye do not cast it out 

 by your unbelief, that ye will resist the Spirit of the Lord , behold, it will begin to swell 

 within your breasts; and when you feel these swelling motions, ye will begin to say 

 within yourselvesâ€”It must needs be that this is a good seed, or that the word is good, 

 for it beginneth to enlarge my soul; yea, it beginneth to enlighten my understanding, 

 yea, it beginneth to be delicious to me.â€ (Alma 32:28, emphasis added) 

 e. To me, asking in faith means an honest willingness to follow the promptings received. If 

 I donâ€™t honestly intend to follow impressions that are given, the experiment is void. 

 f. â€œIf any man will do his will , he shall know of the doctrine, whether it be of God, or 

 whether I speak of myself.â€ (John 7:17, emphasis added) 

 g. Mission friend: finally prayed about the Book of Mormon but â€œnothing happenedâ€. 

 Zone leader: real intent might mean praying more than once. After continued prayer, 

 he received confirming witness. 

## L4 Stata: Basics 

Stata 

1. Introduction 

---

 a. Stata especially well-suited to economics (regression analysis). 

 b. But expensive. 

 c. Other stats packages are available (e.g. R, SAS, SPSS), can program in Python, C, Matlab, 

 Java). But learning new stats package or programming language is like using a Casio 

 calculator when youâ€™re familiar with TIâ€”all the buttons do all the same things, theyâ€™re 

 just located in different places. So learning one language helps you pick up others 

 quickly, as needed. (Stata has specific value for Economics research assistants, future 

 PhDs.) 

2. Basic user interface 

 a. Open Stata. Very different GUI (graphical user interface) than Excel, but same idea. 

 Feels more like computer programming software, for good reason. 

 b. Click on Data Editor (Edit), enter data by hand: numbers 1-10 in column 1, make up ten 

 values in column 2. 

 i. Row entries are called observations (numbered automatically on left side) 

 ii. Column entries are called variables, default named â€œvar1â€ and â€œvar2â€ 

 c. Close data editor. Notice in main screen, a running list of individual commands (and 

 their results), based on what we just did manually. 

 i. With our first action, we actually implemented three commands: set â€œobsâ€ from 

 0 to 1, generate variable â€œvar1â€, and set the value of var1 for observation 1. 

 ii. As we added more data, we set â€œobsâ€ to 2, 3, ..., 10, replaced var1 for 

 observations 2, 3, ..., 10, and then generated a second variable, â€œvar2â€, and 

 replaced its values for observations 1-10. (Note, we no longer needed to 

 expand â€œobsâ€ at that point.) 

 d. Command line 

 i. We can add additional commands using the command line. Type: generate 

 newvar 

 ii. Note: in Stata commands, shortcuts are indicated by underlining the minimal 

 number of letters to convey the same meaning: â€œgenâ€ or â€œgeneâ€ or â€œgenerâ€ or 

 â€œgeneraâ€ or â€œgeneratâ€ or â€œgenerateâ€ are all equivalent, but â€œgeâ€ is ambiguous, 

 and will therefore not work. 

 iii. Error: when we generate a new variable, we need to define its values. Letâ€™s try 

 again: generate varthree=3 

---

 iv. Now open Data Editor again to see the result. We have defined a new variable 

 â€œvarthreeâ€ to equal 3, not just for a particular observation, but for every 

 observation. This is the power of the programming approach to data: you can 

 do lots of things at once! (In Excel, there are shortcuts to copy and fill, but 

 nothing this quick.) Most of the time, we wonâ€™t interact with the spreadsheet 

 directly; weâ€™ll just be programming. 

 v. To see this again, close Data Editor and type: replace varthree=2*var2 if var1>7. 

 We see that three real changes were made, and if we open the Data Editor 

 again, we can see what that looks like. 

 vi. What if we meant to type varthree=2*var2 if var1>5? We could type over again, 

 or we can push â€œPgUpâ€ to repeat the previous command (and edit it before 

 hitting return). Pushing this multiple times allows us to repeat earlier 

 commands. 

 vii. We can also use the command line as a calculator: type display sqrt(8*2) 

 e. Left and right panels 

 i. On top right-hand side is a list of the variables we have so far: var1, var2, and 

 newvar. 

 ii. The bottom right-hand side summarizes our data: we have three variables and 

 10 observations. 

 iii. (If you ever need to, you can resize these panels by dragging their borders.) 

 iv. Notice on left-hand side is list of commands weâ€™ve used so far (red for the ones 

 that returned error codes). Letâ€™s highlight the first 26 lines (click on the first, 

 then shift-click on line 26) and push Ctrl+C to copy these. (This creates var1 and 

 var2 and replaces all 10 values of var1 but only the first 5 values of var2.) 

3. Do Files 

 a. Click on â€œNew Do File Editorâ€ at the top of the screen (looks like a Word document, with 

 a pencil). This opens a new window, with a text editor. 

 b. Type Ctrl+V to paste the command lines that we copied earlier. This becomes a short 

 computer program that, once we execute, will create two variables and replace their 

 values. (In computer programming, a â€œcommandâ€ tells the computer to do something. 

 A â€œscriptâ€ is a list of commands, to be executed in order. Scripts in Stata are called Do 

 files.) 

---

 c. Note: some programming languages require a semicolon or other punctuation to 

 denote the end of one command and the beginning of the next. â€œReturnâ€ plays that role 

 in Stata, so writing on separate commands on separate lines is sufficient. 

 d. Always use a script, not the command line! 

 i. Often, after many steps of manipulating data, you realize that you should have 

 done step 3 differently. In Excel, you might be able to hit â€œundoâ€ repeatedly (if 

 you havenâ€™t saved and closed the program yet), but once you correct step 3, 

 youâ€™ll then have to redo all subsequent steps. With a script, you can edit step 3 

 and recompile in moments. 

 ii. A script is also useful for collaborators and replicators, as well as yourself when 

 you come back to a project after a long pause. I learned this the hard way: I 

 downloaded data, manipulated it repeatedly using the command line, found 

 results that were really interesting, and copied and pasted them into my 

 research paper. A more urgent project came up, so it was months before I came 

 back to this. When I came back, I did (what I thought were) the same 

 manipulations but got totally different results. I couldnâ€™t figure out how to 

 reproduce the results that I had recorded earlier. Maybe I had been making a 

 mistake? Maybe Iâ€™m making a mistake now, but was previously correct? If I had 

 a paper trail of all my data manipulations at the time, I could compare my work 

 now and then to see how they differ, and which is more reliable. ...But I donâ€™t, 

 and may never publish that paper. 

 iii. For your data project, you will need to submit your Do file, not just your results. 

 iv. Related programming tip: Never overwrite your original data set. Use a script to 

 open your original data set, make whatever edits need to made, and then save 

 the revised data as a new data set with a different file name. That way, every 

 time you run your script, it pulls the same original data. 

4. .dta files 

 a. Computer programs are designed to recognize and interpret information, but only in 

 specific formats, specified by file type (e.g. .pdf, .docx, .mp3). 

 b. For example, Spreadsheets often saved as .csv (â€œcomma separated valuesâ€) 

 i. year,growth;2021,2%;2020,1%;2019,1.5% can be understood to be a 3x2 table 

 c. Excel recognizes .csv or .xlsx, which adds formatting information. 

---

 d. Stata stores data using .dta and stores scripts using .do 

Topics: 

- Help 

- Lookfor 

- Resize 

- Break 

- Pwd/cd 

- Display 

- Ctl+Shft+Right click -> â€œcopy as pathâ€ 

File types 

Variable types 

Scripts 

Saving data 

## L5 Stata: Summarizing Data 

Topics: 

Script tips 

Describe 

Tabulate (+twoway) 

Summarize 

Destring/Tostring 

Comments 

Generate 

Replace 

If 

---

## L6 Stata: Data Visualization 

Bysort 

Histogram 

Help 

Scatter 

Geodist 

Keep/Drop 

Collapse 

Line 

Import 

Net 

Twoway line 

Graph options 

Regress 

Predict 

## L7 Probability, Combinatorics (WMS 2.1-6) 

1. How many have already collected data? How many know what data to collect? 

2. Spiritual thought: many of our greatest priorities are not time sensitive, so itâ€™s easy to 

 procrastinate till â€œlaterâ€, but often this postpones blessings. Let your daily activities match your 

 eternal priorities! (e.g. Donâ€™t delay repentance, marriage/children, promptings, family history, 

 making life plans, for movies/hobbies or even school/work) 

3. Probability 

 a. Language of uncertainty 

 b. Economic applications: risk/insurance, investments, shopping/learning 

 c. Data analysis (huge): infer population characteristics from sample 

 d. Fundamentally, probability is merely ratio 

 e. Sample space / Universe ğ‘†ğ‘†={1, 2, 3, 4, 5, 6} 

---

 f. Subset / Event 

 i. ğµğµ= 

#### { 

#### 4, 5, 6 

#### } 

 ii. ğ¸ğ¸={2, 4, 6} 

 g. Probability function ğ‘ƒğ‘ƒ(ğ¸ğ¸)= 

 ğ‘›ğ‘› ğ¸ğ¸ 

 ğ‘›ğ‘› ğ‘†ğ‘† 

#### = 

 3 

 6 

 i. A function is like a machine; in this case, output is number but input is set 

 ii. ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘†ğ‘† 

#### ) 

#### = 1 

 h. Categorical descriptions: 50/500 unemployed = 10% unemployment rate / probability 

4. Set Notation 

 a. Element 

 i. 5 âˆˆğµğµ, 5 âˆ‰ğ¸ğ¸ 

 b. Subset 

 i. ğµğµ âŠ†ğ‘†ğ‘† 

 c. Empty set âˆ… 

 d. Complement 

 i. ğµğµ 

#### ï¿½ 

#### = 

#### { 

#### 1, 2, 3 

#### } 

 ii. ğ¸ğ¸ 

#### ï¿½ 

#### ={1, 3, 5} 

 iii. ğ‘ƒğ‘ƒ 

#### ( 

#### ğ¸ğ¸ 

#### ï¿½ 

#### ) 

#### = 1 âˆ’ğ‘ƒğ‘ƒ 

#### ( 

#### ğ¸ğ¸ 

#### ï¿½ 

#### ) 

 iv. Note: this is a simple but useful tool. Sometimes itâ€™s easier to derive ğ‘ƒğ‘ƒ(ğ¸ğ¸ 

#### ï¿½ 

 ) than 

 to derive ğ‘ƒğ‘ƒ(ğ¸ğ¸). For example: the probability that two or more students in Econ 

 378 share birthdays is very difficult to find directly, but the complementary 

 event (i.e. that no two students share a birthday) is not as bad. 

 e. Intersection (â€œandâ€) 

 i. ğµğµâˆ©ğ¸ğ¸={4, 6} 

 ii. ğµğµ 

#### ï¿½ 

#### âˆ©ğ¸ğ¸ 

#### ï¿½ 

#### = 

#### { 

#### 1, 3 

#### } 

 f. Union (â€œorâ€, â€œat least oneâ€) 

 i. ğµğµâˆªğ¸ğ¸={2, 4, 5,6} 

 ii. ğµğµ 

#### ï¿½ 

#### âˆªğ¸ğ¸ 

#### ï¿½ 

#### = 

#### { 

#### 1, 2, 3,5 

#### } 

 g. Mutually exclusive 

 i. ğ´ğ´âˆ©ğµğµ=âˆ… 

 ii. ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´âˆ©ğµğµ 

#### ) 

#### = 0 

 h. Collectively exhaustive 

 i. ğ´ğ´âˆªğµğµ=ğ‘†ğ‘† 

---

 ii. ğ‘ƒğ‘ƒ(ğ´ğ´âˆªğµğµ)= 1 

5. Total probability: ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´âˆªğµğµ 

#### ) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### ) 

#### +ğ‘ƒğ‘ƒ 

#### ( 

#### ğµğµ 

#### ) 

#### âˆ’ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğµğµ) 

 a. Including both ğ‘ƒğ‘ƒ(ğ´ğ´) and ğ‘ƒğ‘ƒ(ğµğµ) â€œdouble countsâ€ ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğµğµ) 

 b. Example: among set ğ‘†ğ‘† of workers in particular industry, unemployment rate ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆ)= 

 .10, women ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š)=.25, intersection ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆâˆ©ğ‘Šğ‘Š)=.05; ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆâˆªğ‘Šğ‘Š)=.1 +.25âˆ’.05= 

#### .3 

6. Independence: ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´âˆ©ğµğµ 

#### ) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### ) 

#### ğ‘ƒğ‘ƒ(ğµğµ) 

 a. Example: ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆ)ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š)=(. 10)(. 25)=.025â‰ .05=ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆâˆ©ğ‘Šğ‘Š) 

 b. Not the same as mutually exclusive! (Mutually exclusive events are highly negatively 

 correlated) 

7. Combinatorics 

 a. â€œğ‘šğ‘šğ‘›ğ‘› ruleâ€ 

 i. 6 pants (2 brown), 10 shirts (3 green); probability that random outfit consists of 

 brown pants and green shirt is ğ‘ƒğ‘ƒ(ğµğµâˆ©ğºğº)= 

 # 

 { ğµğµâˆ©ğºğº ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘‘ğ‘‘ğ‘–ğ‘– ğ‘œğ‘œğ‘œğ‘œ 

 } 

 # 

 { ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ ğ‘‘ğ‘‘ğ‘–ğ‘– ğ‘œğ‘œğ‘œğ‘œ 

 } 

#### = 

 2â‹…3 

 6â‹…10 

#### =.1 

 ii. Equivalently, since independent, ğ‘ƒğ‘ƒ(ğµğµâˆ©ğºğº)=ğ‘ƒğ‘ƒ(ğµğµ)ğ‘ƒğ‘ƒ(ğºğº)= 

 2 

 6 

#### â‹… 

 3 

 10 

#### =.1 

 b. Number of ways to permute (i.e. order) 10 students: 10 !â‰ˆ3. 6 ğ‘šğ‘šğ‘–ğ‘– ğ‘™ğ‘™ğ‘™ğ‘™ğ‘–ğ‘–ğ‘›ğ‘›ğ‘›ğ‘› 

 c. Number of ways to choose 3 out of 10 students: ğ¶ğ¶ 

 3 

 10 

#### =ï¿½ 

#### 10 

#### 3 

#### ï¿½= 

 10! 

 3! 7! 

#### = 120 

 i. Numerator: there are 10! ways to order the 10 students 

 ii. Denominator: but this double-counts (3!7! times) orderings which shuffle the 

 first three and last seven, but donâ€™t change the identity of the chosen three 

 d. Number of ways to assign 10 students into bins of 3, 5, 2: 

 10! 

 3! 5! 2! 

#### =2, 520 

8. Applications: discrimination lawsuit after 9 workers (3 immigrants, 6 natives) assigned to 4 

 dangerous jobs + 5 safe jobs 

 a. All 3 immigrants assigned to dangerous jobs 

 i. ğ‘ƒğ‘ƒ(ğ¸ğ¸)= 

 ğ‘›ğ‘› ğ¸ğ¸ 

 ğ‘›ğ‘› ğ‘†ğ‘† 

#### = 

 ğ¶ğ¶ 

 3 

 3 

 Ã—ğ¶ğ¶ 

 1 

 6 

 ğ¶ğ¶ 

 4 

 9 

#### = 

 ï¿½ 

 3! 

 3! 0! 

 ï¿½ï¿½ 

 6! 

 1! 5! 

 ï¿½ 

 9! 

 4! 5! 

#### = 

 6 

 ï¿½ 

 9â‹…8â‹…7â‹…6 

 4â‹…3â‹…2â‹…1 

 ï¿½ 

#### = 

 1 

 21 

#### â‰ˆ0. 05 

 ii. Alternatively, can think sequentially: 

 4 

 9 

#### â‹… 

 3 

 8 

#### â‹… 

 2 

 7 

#### = 

 1 

 21 

 iii. Alternatively, can assign workers to safe jobs: ğ‘ƒğ‘ƒ(ğ¸ğ¸)= 

 ğ‘›ğ‘› 

 ğ¸ğ¸ 

 ğ‘›ğ‘› 

 ğ‘†ğ‘† 

#### = 

 ğ¶ğ¶ 

 0 

 3 

 Ã—ğ¶ğ¶ 

 5 

 6 

 ğ¶ğ¶ 

 5 

 9 

#### â‰ˆ0. 05 

 iv. Alternatively, can assign jobs to workers: ğ‘ƒğ‘ƒ(ğ¸ğ¸)= 

 ğ¶ğ¶ 

 3 

 4 

 ğ¶ğ¶ 

 0 

 5 

 ğ¶ğ¶ 

 3 

 9 

#### â‰ˆ0. 05 

 b. 2 out of 3 immigrants assigned to dangerous jobs 

---

 i. ğ‘ƒğ‘ƒ( 2 )= 

 ğ‘›ğ‘› ğ¸ğ¸ 

 ğ‘›ğ‘› ğ‘†ğ‘† 

#### = 

 ï¿½ 

 ğ¶ğ¶ 

 2 

 3 

 Ã—ğ¶ğ¶ 

 2 

 6 

 ï¿½ 

 ğ¶ğ¶ 

 4 

 9 

#### = 

 ï¿½ 

 3! 

 2! 1! 

 ï¿½ï¿½ 

 6! 

 2! 4! 

 ï¿½ 

 9! 

 4! 5! 

#### = 

 3ï¿½ 

 6â‹…5 

 2â‹…1 

 ï¿½ 

 ï¿½ 

 9â‹…8â‹…7â‹…6 

 4â‹…3â‹…2â‹…1 

 ï¿½ 

#### = 

 5 

 14 

#### â‰ˆ0. 36 

 ii. ğ‘ƒğ‘ƒ(ğ¸ğ¸)=ğ‘ƒğ‘ƒ( 2 )+ğ‘ƒğ‘ƒ( 3 )â‰ˆ0. 36+0. 05=0. 41 

 c. 24 workers assigned to 10 safe and 14 dangerous jobs, lawsuit because 6 immigrants all 

 assigned dangerous job 

 i. ğ‘ƒğ‘ƒ(ğ¸ğ¸ 

 1 

#### )= 

 ğ‘›ğ‘› 

 ğ¸ğ¸ 

 ğ‘›ğ‘› 

 ğ‘†ğ‘† 

#### = 

 ğ¶ğ¶ 

 8 

 18 

 ğ¶ğ¶ 

 6 

 6 

 ğ¶ğ¶ 

 14 

 24 

#### = 

 18! 

 8! 10! 

 6! 

 6! 0! 

 24! 

 14! 10! 

#### = 

 18! 14! 

 8! 24! 

#### â‰ˆ.022 

 ii. 

 ğ¶ğ¶ 

 6 

 14 

 ğ¶ğ¶ 

 0 

 10 

 ğ¶ğ¶ 

 6 

 24 

#### = 

 14! 

 6! 8! 

 10! 

 0! 10! 

 24! 

 6! 18! 

#### = 

 14! 18! 

 8! 24! 

#### â‰ˆ.022 

 iii. If 5 out of 6 assigned dangerous job: ğ‘ƒğ‘ƒ(ğ¸ğ¸ 

 2 

#### )= 

 ğ¶ğ¶ 

 9 

 18 

 ğ¶ğ¶ 

 5 

 6 

 ğ¶ğ¶ 

 14 

 24 

#### = 

 18! 

 9! 9! 

 6! 

 5! 1! 

 24! 

 14! 10! 

#### = 

 18! 14 !10 âˆ™6 

 24! 9! 

#### â‰ˆ.149, 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ¸ğ¸ 

#### ) 

#### â‰ˆ.022+.149=.171 

## L8 Conditional Probability (WMS 2.7-10) 

1. If possible, be prepared next lecture with idea for research project 

2. Typically, donâ€™t count to determine Pr 

#### ( 

#### ğ¸ğ¸ 

#### ) 

 ; estimate from sample 

Conditional probability 

1. Definition: Pr(ğ´ğ´|ğµğµ)= 

 Pr(ğ´ğ´âˆ©ğµğµ) 

 Pr(ğµğµ) 

2. This is how online stores (e.g. Ebay, Amazon, Google) figure out what to advertise: given that 

 you purchased a textbook, how likely are you to want a Lego set or motorcycle helmet? 

3. Story problem keywords: â€œgivenâ€, â€œconditional onâ€, â€œamongâ€, or â€œout ofâ€ 

4. Example 1: Among set ğ‘†ğ‘† of workers in particular industry, unemployment rate ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆ)=.10, 

 women ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š)=.25, intersection ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆâˆ©ğ‘Šğ‘Š)=.05 

 d. Rectangular Venn diagram 

 e. Unemployment rate among women ğ‘ƒğ‘ƒ(ğ‘ˆğ‘ˆ|ğ‘Šğ‘Š)= 

. 05 

. 25 

#### =.20 

 f. Fraction of unemployed who are women ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š 

#### | 

#### ğ‘ˆğ‘ˆ)= 

. 05 

. 10 

#### =.50 

 g. Practice: 

 i. Unemployment rate among men ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ˆğ‘ˆ 

#### | 

#### ğ‘Šğ‘Š 

#### ï¿½ 

#### ) 

#### = 

. 05 

. 75 

#### = 

 1 

 15 

#### â‰ˆ.07 

 ii. Fraction of unemployed who are men ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š 

#### ï¿½ 

#### |ğ‘ˆğ‘ˆ)= 

. 05 

. 10 

#### =.50= 1 âˆ’ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š|ğ‘ˆğ‘ˆ) 

---

Independence 

1. Definition: ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### | 

#### ğµğµ 

#### ) 

#### =ğ‘ƒğ‘ƒ(ğ´ğ´), ğ‘ƒğ‘ƒ 

#### ( 

#### ğµğµ 

#### | 

#### ğ´ğ´ 

#### ) 

 =ğ‘ƒğ‘ƒ(ğµğµ) (equivalent to ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´âˆ©ğµğµ 

#### ) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğµğµ 

#### ) 

#### ) 

2. What is the probability of a person being unemployed? ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### ) 

 =.10; what if itâ€™s raining outside? 

 Then the probability of being unemployed is ğ‘ƒğ‘ƒ(ğ´ğ´ 

#### | 

#### ğµğµ)=.10. 

3. Surgeon joke (failing to account for independence): the bad news is that this type of surgery is 

 successful only 25% of the time. The good news is that the last three patients all died. 

Event decomposition: 

1. If ğ¸ğ¸ 

 1 

#### , ...,ğ¸ğ¸ 

 ğ‘˜ğ‘˜ 

 are mutually exclusive and collectively exhaustive then ğ‘ƒğ‘ƒ(ğ´ğ´)=âˆ‘ ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğ¸ğ¸ 

 ğ‘˜ğ‘˜ 

#### ) 

 ğ‘›ğ‘› 

 ğ‘˜ğ‘˜=1 

2. Example 1: 30% of web traffic comes from a Google add (ğºğº), 30% from online newspaper (ğ‘ğ‘), 

 and 40% from a product reviewerâ€™s blog (ğ‘…ğ‘…). 40% of Google traffic, 20% of newspaper traffic, 

 and 30% of reviewer traffic end in a sale (ğ‘†ğ‘†). What fraction of overall traffic ends in a purchase? 

 a. Step 1: draw event tree (first web source, then purchase decision) 

 b. Step 2: translate question into notation. ğ‘ƒğ‘ƒ(ğºğº)=.3, ğ‘ƒğ‘ƒ(ğ‘ğ‘)=.3, ğ‘ƒğ‘ƒ(ğ‘‡ğ‘‡)=.4, ğ‘ƒğ‘ƒ(ğ‘†ğ‘†|ğºğº)= 

 .4, ğ‘ƒğ‘ƒ(ğ‘†ğ‘†|ğ‘ğ‘)=.2, ğ‘ƒğ‘ƒ(ğ‘†ğ‘†|ğ‘…ğ‘…)=.3, wish to find ğ‘ƒğ‘ƒ(ğ‘†ğ‘†) 

 c. ğ‘ƒğ‘ƒ(ğ‘†ğ‘†)=ğ‘ƒğ‘ƒ(ğ‘†ğ‘†âˆ©ğºğº)+ğ‘ƒğ‘ƒ(ğ‘†ğ‘†âˆ©ğ‘ğ‘)+ğ‘ƒğ‘ƒ(ğ‘†ğ‘†âˆ©ğ‘…ğ‘…) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğºğº 

#### ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘†ğ‘† 

#### | 

#### ğºğº 

#### ) 

#### +ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ğ‘ 

#### ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘†ğ‘† 

#### | 

#### ğ‘ğ‘ 

#### ) 

#### +ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘…ğ‘… 

#### ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘†ğ‘† 

#### | 

#### ğ‘…ğ‘… 

#### ) 

#### =.3 Ã—.4 +.3 Ã—.2 +.4 Ã—.3 =.12+ 

#### .06+.12=.3 

 d. ğ‘†ğ‘† and ğ‘…ğ‘… are independent, since ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘†ğ‘† 

#### | 

#### ğ‘…ğ‘… 

#### ) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘†ğ‘† 

#### ) 

 =.3. Is ğ‘†ğ‘† independent of ğºğº? Of ğ‘ğ‘? 

3. Bayesâ€™ Rule 

 a. ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğµğµ)=ï¿½ 

#### ğ‘ƒğ‘ƒ(ğ´ğ´|ğµğµ)ğ‘ƒğ‘ƒ(ğµğµ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğµğµ 

#### | 

#### ğ´ğ´ 

#### ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### ) 

 b. Therefore, can derive ğ‘ƒğ‘ƒ 

#### ( 

#### ğ´ğ´ 

#### | 

#### ğµğµ 

#### ) 

 from ğ‘ƒğ‘ƒ 

#### ( 

#### ğµğµ 

#### | 

#### ğ´ğ´ 

#### ) 

 , or vice versa. 

 c. ğ‘ƒğ‘ƒ(ğ´ğ´|ğµğµ)= 

 ğ‘ƒğ‘ƒ ï¿½ 

#### ğµğµ 

 ï¿½ 

#### ğ´ğ´ 

 ï¿½ 

 ğ‘ƒğ‘ƒ(ğ´ğ´) 

 ğ‘ƒğ‘ƒ 

 ( ğµğµ 

 ) 

#### = 

 ğ‘ƒğ‘ƒ ï¿½ 

#### ğµğµ 

 ï¿½ 

#### ğ´ğ´ 

 ï¿½ 

 ğ‘ƒğ‘ƒ(ğ´ğ´) 

#### ğ‘ƒğ‘ƒï¿½ğµğµï¿½ğ´ğ´ï¿½ğ‘ƒğ‘ƒ(ğ´ğ´)+ğ‘ƒğ‘ƒï¿½ğµğµï¿½ğ´ğ´ 

#### Ì… 

 ï¿½ğ‘ƒğ‘ƒ(ğ´ğ´ 

 Ì… ) 

 d. Practice: find and interpret ğ‘ƒğ‘ƒ(ğºğº|ğ‘†ğ‘†), ğ‘ƒğ‘ƒ(ğ‘…ğ‘…|ğ‘†ğ‘†), ğ‘ƒğ‘ƒ(ğ‘ğ‘|ğ‘†ğ‘†) = 

 ğ‘ƒğ‘ƒ 

 ( ğ‘ğ‘âˆ©ğ‘†ğ‘† 

 ) 

 ğ‘ƒğ‘ƒ 

 ( ğ‘†ğ‘† 

 ) 

#### = 

. 06 

. 3 

 =.2 (mere 

 coincidence that ğ‘ƒğ‘ƒ(ğ‘ğ‘|ğ‘†ğ‘†)=ğ‘ƒğ‘ƒ(ğ‘†ğ‘†|ğ‘ğ‘)) 

4. Warning: think carefully about difference between ğ‘ƒğ‘ƒ(ğ´ğ´|ğµğµ), ğ‘ƒğ‘ƒ(ğ´ğ´), and ğ‘ƒğ‘ƒ(ğµğµ|ğ´ğ´). Be sure you 

 know which you really want. 

5. Note: Itâ€™s possible for composite probabilities and conditional probabilities to tell rather 

 opposite stories 

---

a. Charig et al. (1986): Kidney stone treatment B looked more effective, but A was more 

 actually effective more effective both with small stones and large stones (but stone size 

 matters, and treatments A and B had been used disproportionately on large and small 

 stones, respectively) 

 Kidney stone size Treatment A Treatment B 

 Small 81/87= 93% 234/270=87% 

 Large 192/263= 73% 55/80=69% 

 Both 273/350=78% 289/350= 83% 

b. MLB batting averages: David Justice was better in 1995 and 1996 but Derek Jeter was 

 better in 1995-96. Who is better batter? 

 Batter 1995 1996 1995 96 

 Derek Jeter 12/48=.250 183/582=.314 195/630= .310 

 David Justice 104/411= .253 45/140= .321 149/551=.270 

 Either could be. Likely depends on which is more predictive of 1997 (depends on other 

 assumptions) 

c. Israel covid data: August 2021 (https://www.covid-datascience.com/post/israeli-data

 how-can-efficacy-vs -severe-disease-be-strong-when-60-of -hospitalized-are-vaccinated) 

 i. When covid Delta variant hit, Israeli hospitals filled up with covid cases: 214 that 

 were unvaccinated and 301 that were vaccinated. Since 60% were vaccinated, 

 superficial conclusion is that vaccines make covid worse, not better! 

 ii. But 60%=P(vax|cv). We really want to know P(cv|vax) (actually, want to 

 compare P(cv|vax) and P(cv|no vax)) 

 iii. ğ‘ƒğ‘ƒ(ğ‘ğ‘ğ‘ğ‘|ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥) = 301 /5, 634,634=5. 3âˆ— 10 

 âˆ’5 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ğ‘ğ‘ğ‘ 

#### | 

#### ğ‘›ğ‘›ğ‘›ğ‘› ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ 

#### ) 

#### = 

 214 

 1 , 302 , 912 

#### = 16 .4âˆ— 10 

 âˆ’5 

 Vaccinated only catch covid 

 5. 3 

 16. 4 

 =32% as often (i.e. vaccine 68% effective) 

 iv. Nearly 80% of Israelis over age 12 were vaccinated against covid, so if it were 

 unrelated random draw, 80% of covid patients should have been vaccinated; 

 lower rate than 80% supports hypothesis that treatment helped. 

---

 v. Put differently, so many Israelis were vaccinated that even though those 

 vaccinated only got covid 68% as often, there were more vaccinated covid cases 

 than unvaccinated covid cases. 

## L9 Probability Distributions (WMS 3.1-3) 

1. Events are useful for describing binary/categorical information. Next, weâ€™ll consider random 

 variables, which are useful for describing quantitative information. 

2. Random variables 

 a. Distribution Function: Number of cars ğ‘‹ğ‘‹ owned by a family ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)=ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹=ğ‘¥ğ‘¥)= 

#### â© 

#### âª 

#### â¨ 

#### âª 

#### â§ 

#### . 10 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 0 

#### . 30 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 1 

#### . 40 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 2 

#### . 20 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 3 

#### 0 ğ‘’ğ‘’ğ‘™ğ‘™ğ‘’ğ‘’ğ‘’ğ‘’ 

 b. Mean (i.e. average) â€œmuâ€ ğœ‡ğœ‡ 

 i. We donâ€™t know total population size. If we knew there were 100 families, ğœ‡ğœ‡= 

 1 

 100 

 ( 0 â‹… 10 + 1 â‹… 30 + 2 â‹… 40 + 3 â‹… 20 )=1. 7. If population size ğ‘›ğ‘› is unknown 

 then ğœ‡ğœ‡= 

 1 

 ğ‘›ğ‘› 

#### [ 

#### 0 

#### ( 

#### . 10ğ‘›ğ‘› 

#### ) 

#### + 1 

#### ( 

#### . 30ğ‘›ğ‘› 

#### ) 

#### + 2 

#### ( 

#### . 40ğ‘›ğ‘› 

#### ) 

#### + 3 

#### ( 

#### . 20ğ‘›ğ‘› 

#### )] 

 but this reduces to ... 

 ii. Expected value (or â€œexpectationâ€) ğœ‡ğœ‡=ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### = 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥) 

 ğ‘¥ğ‘¥ 

#### = 0 

#### ( 

#### . 10 

#### ) 

#### + 1 

#### ( 

#### . 30 

#### ) 

#### + 

#### 2 

#### ( 

#### . 40 

#### ) 

#### + 3 

#### ( 

#### . 20 

#### ) 

#### =1. 7 

1. Note: if all realizations of ğ‘‹ğ‘‹ are equally likely then ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### = 

 1 

 ğ‘›ğ‘› 

 for all ğ‘¥ğ‘¥ so 

#### ğ¸ğ¸(ğ‘‹ğ‘‹)=âˆ‘ ğ‘¥ğ‘¥ 

 1 

 ğ‘›ğ‘› 

 ğ‘¥ğ‘¥ 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘¥ğ‘¥ 

 reduces to familiar formula 

 c. Expected value of functions of ğ‘‹ğ‘‹ 

 i. Example: expected utility ğ¸ğ¸ 

#### [ 

#### ğ‘¢ğ‘¢ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### )] 

#### =ğ¸ğ¸ï¿½âˆšğ‘‹ğ‘‹ï¿½) 

 ii. Formula: ğ¸ğ¸[ğ‘“ğ‘“(ğ‘¥ğ‘¥)]=âˆ‘ ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥) 

 ğ‘¥ğ‘¥ 

 iii. Example: ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

 2 

#### ) 

#### = 0 

 2 

#### ( 

#### . 1 

#### ) 

#### + 1 

 2 

#### ( 

#### . 3 

#### ) 

#### + 2 

 2 

#### ( 

#### . 4 

#### ) 

#### + 3 

 2 

#### ( 

#### . 2 

#### ) 

#### =3. 7 

 iv. Algebra shortcuts 

1. Linear functions, e.g. car maintenance cost ğ¶ğ¶= 200 + 100 ğ‘‹ğ‘‹; average 

 maintenance cost ğ¸ğ¸(ğ¶ğ¶)= 200 (. 1)+ 300 (. 3)+ 400 (. 4)+ 500 (. 2)= 

#### 370 

2. Shortcuts: ğ¸ğ¸ 

#### ( 

#### 200 + 100 ğ‘‹ğ‘‹ 

#### ) 

#### =ğ¸ğ¸ 

#### ( 

#### 200 

#### ) 

#### +ğ¸ğ¸ 

#### ( 

#### 100 ğ‘‹ğ‘‹ 

#### ) 

---

#### = 200 + 100 ğ¸ğ¸(ğ‘‹ğ‘‹)= 200 + 100 (1. 7)= 370 

 a. Summations: ğ¸ğ¸ 

#### (âˆ‘ 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ) 

#### = 

#### âˆ‘ 

#### ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 b. Pull out coefficients 

 c. For constant ğ‘ğ‘, ğ¸ğ¸(ğ‘ğ‘)=ğ‘ğ‘ 

 d. Variance, standard deviation 

 i. Variance ğœğœ 

 2 

#### =ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### =ğ¸ğ¸ 

#### [( 

#### ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡ 

#### ) 

 2 

#### ] 

#### = 

#### [( 

#### 0 âˆ’1. 7 

#### ) 

 2 

#### ]( 

#### . 1 

#### ) 

#### + 

#### [( 

#### 1 âˆ’1. 7 

#### ) 

 2 

#### ]( 

#### . 3 

#### ) 

#### + 

#### [ 

#### ( 2 âˆ’1. 7) 

 2 

#### ] 

#### (. 4)+ 

#### [ 

#### ( 3 âˆ’1. 7) 

 2 

#### ] 

#### (. 2)=.81 

 ii. Standard deviation ğœğœ=âˆšğœğœ 

 2 

#### = 

#### âˆš 

#### . 81=.9 

1. Interpretation: by rule of thumb, â€œmostâ€ families own between -.1 and 

 3.5 cars 

2. Note: variance, by itself, is difficult to interpret (e.g. units is â€œcars 

 squaredâ€), but is easier to work with algebraically, because itâ€™s 

 technically and average of something, whereas standard deviation is the 

 square root of an average of something. 

 iii. Simpler formula: ğ‘‰ğ‘‰(ğ‘‹ğ‘‹) =ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

 2 

#### ) 

#### âˆ’ğœ‡ğœ‡ 

 2 

#### =3. 7âˆ’ 

#### ( 

#### 1. 7 

#### ) 

 2 

#### =.81 

1. Show equivalent, as homework problem 

2. Remember this formula, as weâ€™ll use it repeatedly 

 iv. Algebra shortcuts 

#### 1. ğ¸ğ¸(ğ¶ğ¶ 

 2 

#### )= 200 

 2 

#### (. 1)+ 300 

 2 

#### (. 3)+ 400 

 2 

#### (. 4)+ 500 

 2 

#### (. 2)= 145 ,000 

 doesnâ€™t have any easy algebra shortcut; ğ‘‰ğ‘‰(ğ¶ğ¶)=ğ¸ğ¸(ğ¶ğ¶ 

 2 

#### )âˆ’ 370 

 2 

#### =8, 100 

2. Shortcut: ğ‘‰ğ‘‰(ğ¶ğ¶)=ğ‘‰ğ‘‰( 200 + 100 ğ‘‹ğ‘‹)=ğ‘‰ğ‘‰( 100 ğ‘‹ğ‘‹)= 100 

 2 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=8, 100 

 a. 200 gets added and subtracted: ğ‘‰ğ‘‰(ğ¶ğ¶)=ğ¸ğ¸{[( 200 + 100 ğ‘‹ğ‘‹)âˆ’ 

#### ğ¸ğ¸ 

#### ( 

#### 200 + 100 ğ‘‹ğ‘‹ 

#### )] 

 2 

#### } 

 b. For constant ğ‘ğ‘, ğ‘‰ğ‘‰(ğ‘ğ‘)= 0 

 c. Pull out coefficients, ... but square them! (because Variance is a 

 quadratic function of a random variable) 

3. Practice example: number ğ‘Œğ‘Œ of car accidents ğ‘ƒğ‘ƒ(ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)=ï¿½ 

#### . 7 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 0 

#### . 2 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 1 

#### . 1 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 2 

#### 0 ğ‘’ğ‘’ğ‘™ğ‘™ğ‘’ğ‘’ğ‘’ğ‘’ 

 a. ğœ‡ğœ‡= 0 (. 7)+ 1 (. 2)+ 2 (. 1)=.4 

 b. ğ¸ğ¸(ğ‘Œğ‘Œ 

 2 

#### )= 0 

 2 

#### (. 7)+ 1 

 2 

#### (. 2)+ 2 

 2 

#### (. 1)=.6 

 c. ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘Œğ‘Œ 

#### ) 

#### =ğ¸ğ¸ 

#### ( 

#### ğ‘Œğ‘Œ 

 2 

#### ) 

#### âˆ’ğœ‡ğœ‡ 

 2 

#### =.6âˆ’ 

#### ( 

#### . 4 

#### ) 

 2 

#### =.44 

---

 d. ğœğœ=âˆš. 44â‰ˆ.663 

 e. Insurance profit Î =$1200âˆ’$2000â‹…ğ‘Œğ‘Œ 

 i. E(Î )=E( 1200 âˆ’ 2000 â‹…Y)= 1200 âˆ’ 2000 ğ¸ğ¸(ğ‘Œğ‘Œ)= 1200 âˆ’ 2000 (. 4)= 

#### $400 

 ii. ğ‘‰ğ‘‰(Î )=V( 1200 âˆ’2000Y)=0 +(âˆ’ 2000 ) 

 2 

#### ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)=4, 000,000(. 44)= 

#### 1, 760,000 

 iii. ğœğœ 

 Î  

#### = 

#### âˆš 

#### 1, 760,000=$1,326 

 f. Review this one more time before attempting your homework! 

## L10 Correlation (WMS 3.1-8) 

 [Long lecture; use time efficiently] 

1. Correlation coefficient ğœŒğœŒâˆˆ[âˆ’1, 1] 

 a. Positive correlation means variables ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ tend to move in same direction (e.g. 

 temperature ğ‘‹ğ‘‹ and ice cream sales ğ‘Œğ‘Œ) 

 b. Negative correlation means variables ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ tend to move in opposite direction (e.g. 

 frequency of exercise ğ‘‹ğ‘‹ and body mass index ğ‘Œğ‘Œ) 

 c. Magnitude indicates strength of relationship 

 d. Independence implies ğœŒğœŒ= 0 

2. Joint distribution function 

 a. Employee hours per week ğ‘‹ğ‘‹ and hourly wage ğ‘Œğ‘Œ 

#### ğ‘Œğ‘Œ= 10 ğ‘Œğ‘Œ= 15 

#### ğ‘‹ğ‘‹= 20 .2 .1 

#### ğ‘‹ğ‘‹= 30 .1 .2 

#### ğ‘‹ğ‘‹= 40 .1 .3 

 Illustrate: ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹=ğ‘¥ğ‘¥âˆ©ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)= 

#### â© 

#### âª 

#### âª 

#### â¨ 

#### âª 

#### âª 

#### â§ 

#### . 2 ğ‘–ğ‘–ğ‘“ğ‘“ (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=( 20 ,10) 

#### . 1 ğ‘–ğ‘–ğ‘“ğ‘“ (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=( 20 ,15) 

#### . 1 ğ‘–ğ‘–ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### = 

#### ( 

#### 30 ,10 

#### ) 

#### . 2 ğ‘–ğ‘–ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### = 

#### ( 

#### 30 ,15 

#### ) 

#### . 1 ğ‘–ğ‘–ğ‘“ğ‘“ (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=( 40 ,10) 

#### . 3 ğ‘–ğ‘–ğ‘“ğ‘“ (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=( 40 ,15) 

3. Marginal distributions 

---

 a. Sum rows or columns: ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)=ï¿½ 

#### . 3 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 20 

#### . 3 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 30 

#### . 4 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 40 

 and ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)=ï¿½ 

#### . 4 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 10 

#### . 6 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 15 

 b. Summary statistics (quick recap) 

 i. ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### = 20 (. 3)+ 30 (. 3)+ 40 (. 4)= 31 

 ii. ğœğœ 

 ğ‘¥ğ‘¥ 

#### â‰ˆ8. 3 

#### 1. ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )= 20 

 2 

#### (. 3)+ 30 

 2 

#### (. 3)+ 40 

 2 

#### (. 4)=1, 030 

#### 2. ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### =1, 030âˆ’ 31 

 2 

#### = 69 

#### 3. ğœğœ 

 ğ‘¥ğ‘¥ 

#### =âˆš 69 â‰ˆ8. 3 

 iii. ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### = 10 (. 4)+ 15 (. 6)= 13 

 iv. ğœğœ 

 ğ‘¦ğ‘¦ 

#### â‰ˆ2. 4 

#### 1. ğ¸ğ¸ 

#### ( 

#### ğ‘Œğ‘Œ 

 2 

#### ) 

#### = 10 

 2 

#### ( 

#### . 4 

#### ) 

#### + 15 

 2 

#### ( 

#### . 6 

#### ) 

#### = 175 

#### 2. ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### = 175 âˆ’ 13 

 2 

#### = 6 

#### 3. ğœğœ 

 ğ‘¦ğ‘¦ 

#### =âˆš 6 â‰ˆ2. 4 

 c. Independence 

 i. Definition: ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### =ğ‘ƒğ‘ƒ 

 ğ‘¥ğ‘¥ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

 (ğ‘¥ğ‘¥) for every 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

 pair 

 ii. Not independent here, since ğ‘ƒğ‘ƒ 

#### ( 

#### 20 ,10 

#### ) 

#### =.20â‰ ğ‘ƒğ‘ƒ 

#### ( 

#### 20 

#### ) 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### 10 

#### ) 

#### =.3 Ã—.4 =.12 

4. Expectations of functions of ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ 

 a. Average weekly payment ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ğ‘Œğ‘Œ 

#### ) 

#### = 

#### ( 

#### 20 â‹… 10 

#### )( 

#### . 20 

#### ) 

#### + 

#### ( 

#### 20 â‹… 15 

#### )( 

#### . 10 

#### ) 

#### + 

#### ( 30 â‹… 10 )(. 10)+( 30 â‹… 15 )(. 20)+( 40 â‹… 10 )(. 10)+( 40 â‹… 15 )(. 30)= 40 + 30 + 30 + 

#### 90 + 40 + 180 = 410 

 b. Can do any function ğ¸ğ¸ 

#### [ 

#### ğ‘“ğ‘“ 

#### ( 

#### ğ‘‹ğ‘‹,ğ‘Œğ‘Œ 

#### )] 

#### = 

#### âˆ‘ 

#### ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥) 

 (ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) 

5. Correlation 

 a. Covariance 

#### ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### =ğ¸ğ¸ï¿½(ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### )ï¿½ğ‘Œğ‘Œâˆ’ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### ï¿½ï¿½ 

#### = 

#### [( 

#### 20 âˆ’ 31 

#### )( 

#### 10 âˆ’ 13 

#### )]( 

#### . 20 

#### ) 

#### +[( 20 âˆ’ 31 )( 15 âˆ’ 13 )](. 10) 

#### +[( 30 âˆ’ 31 )( 10 âˆ’ 13 )](. 10) 

#### + 

#### [( 

#### 30 âˆ’ 31 

#### )( 

#### 10 âˆ’ 15 

#### )]( 

#### . 20 

#### ) 

#### +[( 40 âˆ’ 31 )( 10 âˆ’ 13 )](. 10) 

#### +[( 40 âˆ’ 31 )( 15 âˆ’ 13 )](. 30)= 7 

 b. Simpler formula: ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### =ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ğ‘Œğ‘Œ 

#### ) 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### = 410 âˆ’ 

#### ( 

#### 31 

#### )( 

#### 13 

#### ) 

#### = 7 

---

 c. Correlation ğœŒğœŒ= 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### = 

 7 

 ( 8. 3 

 )( 2. 4 

 ) 

#### â‰ˆ0. 35 

 i. Positive, but fairly weak (again, not independent) 

 ii. Later: ğœŒğœŒ 

 2 

 â‰ˆ0. 12 measures % of variation in ğ‘Œğ‘Œ that covaries with ğ‘‹ğ‘‹ 

6. Algebra shortcuts 

 a. Covariance of a sum 

#### ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ 

#### ( 

#### ğ‘‹ğ‘‹, 1200âˆ’ 2000 ğ‘Œğ‘Œ 

#### ) 

#### =ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ 

#### ( 

#### ğ‘‹ğ‘‹, 1200 

#### ) 

#### +ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ 

#### ( 

#### ğ‘‹ğ‘‹,âˆ’ 2000 ğ‘Œğ‘Œ 

#### ) 

#### =0 + 

#### ( 

#### âˆ’ 2000 

#### ) 

#### ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

 b. Correlation of a sum 

#### ğ¶ğ¶ğ‘›ğ‘›ğ¶ğ¶ğ¶ğ¶(ğ‘‹ğ‘‹, 1200âˆ’ 2000 ğ‘Œğ‘Œ)=âˆ’ğœŒğœŒ 

 c. Variance of a sum 

#### ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘‹ğ‘‹+ğ‘Œğ‘Œ 

#### ) 

#### =ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### +ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘Œğ‘Œ 

#### ) 

#### + 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ) 

 d. Variance of a larger sum 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹+ğ‘Œğ‘Œ+ğ‘ğ‘)=ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)+ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)+ğ‘‰ğ‘‰(ğ‘ğ‘)+ 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ)+ 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘ğ‘)+ 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘(ğ‘Œğ‘Œ,ğ‘ğ‘) 

 (with 100 variables, ğ¶ğ¶ 

 2 

 100 

 â‰ˆ 5000 ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ terms) 

 i. Importance of independent samples 

7. Application: financial diversification 

 a. Assume two stocks have same average return ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### =ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

 =ğœ‡ğœ‡ and same standard 

 deviation ğœğœ 

 ğ‘¥ğ‘¥ 

#### =ğœğœ 

 ğ‘¦ğ‘¦ 

#### =ğœğœ. 

 b. You could buy two shares of ğ‘‹ğ‘‹, or one share of ğ‘‹ğ‘‹ and one share of ğ‘Œğ‘Œ. Since you are 

 indifferent between ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ, it might seem that you should be indifferent between 

 these two stock portfolios. 

 c. However, on your homework, you will prove that ğ¸ğ¸( 2 ğ‘‹ğ‘‹)=ğ¸ğ¸(ğ‘‹ğ‘‹+ğ‘Œğ‘Œ) but ğ‘‰ğ‘‰( 2 ğ‘‹ğ‘‹)< 

 ğ‘‰ğ‘‰(ğ‘‹ğ‘‹+ğ‘Œğ‘Œ), as long as ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ are not perfectly correlated (i.e. ğœŒğœŒ< 1 ). In fact, if they are 

 perfectly negatively correlated then ğ‘‰ğ‘‰(ğ‘‹ğ‘‹+ğ‘Œğ‘Œ)= 0! 

 d. Thus, the common financial advice to â€œDiversify your portfolioâ€. 

8. Practice [if time]: Cell phone use ğ‘‹ğ‘‹ and number ğ‘Œğ‘Œ of car accidents 

#### ğ‘Œğ‘Œ= 0 ğ‘Œğ‘Œ= 1 ğ‘Œğ‘Œ= 2 

#### ğ‘‹ğ‘‹= 0 .60 .08 .02 

#### ğ‘‹ğ‘‹= 1 .10 .12 .08 

 a. Note: numerical values can be assigned to binary categorical variables, so that notion of 

 correlation is still meaningful. 

---

 b. Marginal distribution ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘‹ğ‘‹=ğ‘¥ğ‘¥ 

#### ) 

#### =ï¿½ 

#### . 70 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 0 

#### . 30 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 1 

 , mean ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

 =.3, variance ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

#### ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 2 

#### =.3âˆ’. 3 

 2 

 =.21, standard deviation ğœğœ 

 ğ‘¥ğ‘¥ 

#### = 

#### âˆš 

#### . 21â‰ˆ0. 458 

 c. Marginal distribution ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘Œğ‘Œ=ğ‘¥ğ‘¥ 

#### ) 

#### =ï¿½ 

#### . 70 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 0 

#### . 20 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 1 

#### . 10 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 2 

 , mean ğ¸ğ¸ 

#### ( 

#### ğ‘Œğ‘Œ 

#### ) 

 =.4, variance ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### =.44, 

 standard deviation ğœğœ 

 ğ‘¦ğ‘¦ 

#### =âˆš. 44â‰ˆ0. 663 

 d. Not independent since ğ‘ƒğ‘ƒ 

#### ( 

#### 0, 0 

#### ) 

#### =.6â‰ ğ‘ƒğ‘ƒ 

 ğ‘¥ğ‘¥ 

#### ( 

#### 0 

#### ) 

#### ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 

#### 0 

#### ) 

#### = 

#### ( 

#### . 7 

#### )( 

#### . 7 

#### ) 

#### =.49 

 e. ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ğ‘Œğ‘Œ 

#### ) 

#### = 

#### ( 

#### 0 

#### )( 

#### 0 

#### )( 

#### . 60 

#### ) 

#### + 

#### ( 

#### 0 

#### )( 

#### 1 

#### )( 

#### . 08 

#### ) 

#### + 

#### ( 

#### 0 

#### )( 

#### 2 

#### )( 

#### . 02 

#### ) 

#### + 

#### ( 

#### 1 

#### )( 

#### 0 

#### )( 

#### . 10 

#### ) 

#### + 

#### ( 

#### 1 

#### )( 

#### 1 

#### )( 

#### . 12 

#### ) 

#### + 

#### ( 1 )( 2 )(. 08)=.28 

 f. Covariance ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### =ğ¸ğ¸(ğ‘‹ğ‘‹ğ‘Œğ‘Œ)âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =.28âˆ’(. 3)(. 4)=.16 

 g. Correlation ğœŒğœŒ= 

 ğœğœ ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### = 

. 16 

 (. 458 )(. 663 ) 

 â‰ˆ0. 527 positive and moderately strong 

## L11 Continuous Distributions (WMS 4.1-3) 

1. Continuous random variables 

 a. Infinite domain, e.g. sleep hours ğ‘¥ğ‘¥ âˆˆ[6, 9] 

 b. Philosophical view: continuous functions conveniently approximate discrete world, or 

 world is truly infinite but measurement is imprecise 

2. Probability density function (pdf) ğ‘“ğ‘“(ğ‘¥ğ‘¥) 

 a. Measures relative likelihood of individual ğ‘¥ğ‘¥ values 

 b. Individual ğ‘¥ğ‘¥ values occur with zero probability (and ğ‘“ğ‘“(ğ‘¥ğ‘¥)> 1 is possible); to find 

 probabilities, must take definite integral ğ‘ƒğ‘ƒ(7 <ğ‘‹ğ‘‹< 8 )= âˆ« 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 8 

 7 

 c. Density must be non-negative and integrate to one over domain (just like probabilities 

 sum to one) 

 d. Example ğ‘“ğ‘“(ğ‘¥ğ‘¥)=ğ‘˜ğ‘˜(âˆ’ğ‘¥ğ‘¥ 

 2 

#### + 16 ğ‘¥ğ‘¥âˆ’ 60 ); 6 â‰¤ğ‘¥ğ‘¥ â‰¤ 9 

 i. Not directly from (finite) data; maybe from calibrated theory 

 ii. Find ğ‘˜ğ‘˜ 

#### 1. 1 =âˆ« ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 9 

 6 

#### =ğ‘˜ğ‘˜ï¿½âˆ’ 

 1 

 3 

#### ğ‘¥ğ‘¥ 

 3 

#### + 8 ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 60 ğ‘¥ğ‘¥ï¿½ 

 6 

 9 

#### =ğ‘˜ğ‘˜ 

#### [ 

#### (âˆ’ 243 + 648 âˆ’ 

#### 540 )âˆ’ 

#### ( 

#### âˆ’ 72 + 288 âˆ’ 360 

#### )] 

 = 9 ğ‘˜ğ‘˜ requires that ğ‘˜ğ‘˜= 

 1 

 9 

2. That is, ğ‘“ğ‘“(ğ‘¥ğ‘¥)=âˆ’ 

 1 

 9 

#### ğ‘¥ğ‘¥ 

 2 

#### + 

 16 

 9 

#### ğ‘¥ğ‘¥âˆ’ 

 60 

 9 

#### ; 6 â‰¤ğ‘¥ğ‘¥ â‰¤ 9 

---

 iii. Mode solves ğ‘“ğ‘“ 

 â€² 

#### (ğ‘¥ğ‘¥)=âˆ’ 

 2 

 9 

#### ğ‘˜ğ‘˜ğ‘¥ğ‘¥+ 

 16 

 9 

 ğ‘˜ğ‘˜= 0 ; solution at ğ‘¥ğ‘¥= 8 

1. Note: if ğ‘“ğ‘“ 

 â€² 

 (ğ‘¥ğ‘¥) everywhere positive/negative then maximum is at 

 highest/lowest ğ‘¥ğ‘¥ in range 

2. Note: second-order condition ğ‘“ğ‘“ 

 â€²â€² 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### =âˆ’ 

 2 

 9 

 ğ‘˜ğ‘˜ â‰¤ 0 satisfied as long as 

#### ğ‘˜ğ‘˜ â‰¥ 0 

 iv. Probabilities: ğ‘ƒğ‘ƒ 

#### ( 

#### 7 â‰¤ğ‘¥ğ‘¥ â‰¤ 8 

#### ) 

#### =âˆ« 

 1 

 9 

#### (âˆ’ğ‘¥ğ‘¥ 

 2 

#### + 16 ğ‘¥ğ‘¥âˆ’ 60 )ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 8 

 7 

#### =â‹¯= 

 11 

 27 

#### â‰ˆ0. 4 

3. Cumulative distribution function (cdf) ğ¹ğ¹(ğ‘¥ğ‘¥) 

 a. ğ¹ğ¹(ğ‘¥ğ‘¥)=ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹ â‰¤ğ‘¥ğ‘¥)= âˆ« 

 1 

 9 

#### (âˆ’ğ‘¥ğ‘¥ï¿½ 

 2 

#### + 16 ğ‘¥ğ‘¥ï¿½âˆ’ 60 )ğ‘‘ğ‘‘ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 6 

#### =ï¿½âˆ’ 

 1 

 27 

#### ğ‘¥ğ‘¥ï¿½ 

 3 

#### + 

 8 

 9 

#### ğ‘¥ğ‘¥ï¿½ 

 2 

#### âˆ’ 

 20 

 3 

#### ğ‘¥ğ‘¥ï¿½ï¿½ 

 ğ‘¥ğ‘¥ï¿½=6 

 ğ‘¥ğ‘¥ï¿½=ğ‘¥ğ‘¥ 

#### =âˆ’ 

 1 

 27 

#### ğ‘¥ğ‘¥ 

 3 

#### + 

 8 

 9 

#### ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 

 20 

 3 

#### ğ‘¥ğ‘¥+ 16 

 (This assumes 6 â‰¤ğ‘¥ğ‘¥ â‰¤ 9 ; if ğ‘¥ğ‘¥< 6 then ğ¹ğ¹ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

 = 0 and if ğ‘¥ğ‘¥> 9 then ğ¹ğ¹ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### = 1 ) 

 b. Percentiles 

 Median ğ¹ğ¹ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### =âˆ’ 

 1 

 27 

#### ğ‘¥ğ‘¥ 

 3 

#### + 

 8 

 9 

#### ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 

 20 

 3 

#### ğ‘¥ğ‘¥+ 16 = 

 1 

 2 

 ; solving by computer, ğ‘¥ğ‘¥ â‰ˆ7. 8 

#### 75 

 th 

 percentile ğ¹ğ¹(ğ‘¥ğ‘¥)=âˆ’ 

 1 

 27 

#### ğ‘¥ğ‘¥ 

 3 

#### + 

 8 

 9 

#### ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 

 20 

 3 

#### ğ‘¥ğ‘¥+ 16 =.75â‡’ğ‘¥ğ‘¥ â‰ˆ8. 4 

#### 90 

 th 

 percentile ğ¹ğ¹ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### =âˆ’ 

 1 

 27 

#### ğ‘¥ğ‘¥ 

 3 

#### + 

 8 

 9 

#### ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 

 20 

 3 

#### ğ‘¥ğ‘¥+ 16 =.90â‡’ğ‘¥ğ‘¥ â‰ˆ8. 7 

 c. Easier probabilities ğ‘ƒğ‘ƒ( 7 â‰¤ğ‘‹ğ‘‹ â‰¤ 8 )=ğ¹ğ¹( 8 )âˆ’ğ¹ğ¹( 7 ) 

#### =ï¿½âˆ’ 

 1 

 27 

#### 8 

 3 

#### + 

 8 

 9 

#### 8 

 2 

#### âˆ’ 

 20 

 3 

#### 8 + 16 ï¿½âˆ’ï¿½âˆ’ 

 1 

 27 

#### 7 

 3 

#### + 

 8 

 9 

#### 7 

 2 

#### âˆ’ 

 20 

 3 

#### 7 + 16 ï¿½= 

 11 

 27 

#### â‰ˆ0. 4 

 d. From cdf, get pdf ğ‘“ğ‘“(ğ‘¥ğ‘¥)=ğ¹ğ¹ 

 â€² 

#### (ğ‘¥ğ‘¥)=âˆ’ 

 1 

 9 

#### ğ‘¥ğ‘¥ 

 2 

#### + 

 16 

 9 

#### ğ‘¥ğ‘¥âˆ’ 

 60 

 9 

 ; 6 â‰¤ğ‘¥ğ‘¥ â‰¤ 9 , else ğ‘“ğ‘“(ğ‘¥ğ‘¥)= 0 

4. Moments 

 a. Mean 

#### ğœ‡ğœ‡=ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### =âˆ«ğ‘¥ğ‘¥ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

 ğ‘‘ğ‘‘ğ‘¥ğ‘¥ (just like ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### =âˆ‘ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### ) 

#### =âˆ« ğ‘¥ğ‘¥ 

 1 

 9 

#### ( 

#### âˆ’ğ‘¥ğ‘¥ 

 2 

#### + 16 ğ‘¥ğ‘¥âˆ’ 60 

#### ) 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 9 

 6 

#### = 

#### âˆ« 

 1 

 9 

#### (âˆ’ğ‘¥ğ‘¥ 

 3 

#### + 16 ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 60 ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 9 

 6 

#### =â‹¯= 

 31 

 4 

#### â‰ˆ7. 75 

 b. Standard deviation 

 i. ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

 2 

#### ) 

#### =âˆ« ğ‘¥ğ‘¥ 

 2 

#### ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 9 

 6 

#### =âˆ« ğ‘¥ğ‘¥ 

 2 

 1 

 9 

#### (âˆ’ğ‘¥ğ‘¥ 

 2 

#### + 16 ğ‘¥ğ‘¥âˆ’ 60 )ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 9 

 6 

#### =âˆ« 

 1 

 9 

#### (âˆ’ğ‘¥ğ‘¥ 

 4 

#### + 16 ğ‘¥ğ‘¥ 

 3 

#### âˆ’ 60 ğ‘¥ğ‘¥ 

 2 

#### )ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 9 

 6 

#### =â‹¯= 

 303 

 5 

 ii. ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )âˆ’ğœ‡ğœ‡ 

 2 

#### = 

 303 

 5 

#### âˆ’ï¿½ 

 31 

 4 

#### ï¿½ 

 2 

#### = 

 43 

 80 

 iii. ğœğœ 

 ğ‘‹ğ‘‹ 

#### =ï¿½ 

 43 

 80 

#### â‰ˆ0. 73 

---

 c. Note: algebra tricks still work (e.g. lost wages while sleeping) 

 i. ğ¸ğ¸ 

#### ( 

#### $20ğ‘‹ğ‘‹ 

#### ) 

#### =$20ğ¸ğ¸(ğ‘‹ğ‘‹) =$20â‹…7. 75=$155 

 ii. ğ‘‰ğ‘‰( 20 ğ‘‹ğ‘‹)= 20 

 2 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹) 

5. Practice describing steps to classmate: Warehouse stock (as fraction of capacity) ğ‘“ğ‘“(ğ‘¥ğ‘¥)= 

#### âˆ’ 2 ğ‘¥ğ‘¥ 

 2 

#### +ğ‘˜ğ‘˜ğ‘¥ğ‘¥+ 

 1 

 6 

#### ; 0â‰¤ğ‘¥ğ‘¥ â‰¤ 1 

 a. Find ğ‘˜ğ‘˜= 3 

 b. mode = 

 3 

 4 

 c. Draw and interpret pdf (upside-down parabola; warehouse full more often than empty) 

 d. Find cdf ğ¹ğ¹(ğ‘¥ğ‘¥)=âˆ’ 

 2 

 3 

#### ğ‘¥ğ‘¥ 

 3 

#### + 

 3 

 2 

#### ğ‘¥ğ‘¥ 

 2 

#### + 

 1 

 6 

#### ğ‘¥ğ‘¥; 0â‰¤ğ‘¥ğ‘¥ â‰¤ 1 

 e. Find ğ‘“ğ‘“(ğ‘¥ğ‘¥) from ğ¹ğ¹(ğ‘¥ğ‘¥) 

 f. ğ‘ƒğ‘ƒï¿½ 

 1 

 2 

#### â‰¤ğ‘‹ğ‘‹ â‰¤ 

 3 

 4 

#### ï¿½= 

 5 

 16 

 g. median â‰ˆ.6, 7 5 

 th 

 percentile â‰ˆ.8 

 h. mean ğœ‡ğœ‡â‰ˆ0. 58 

 i. standard deviation ğœğœâ‰ˆ0. 26 

 j. Insurance payout ğœ‹ğœ‹=$1,000,000ğ‘‹ğ‘‹+$100,000 

 i. ğ¸ğ¸(ğœ‹ğœ‹)=$1,000,000ğœ‡ğœ‡+$100,000=$680,000 ğœğœ 

 ğœ‹ğœ‹ 

#### = 

#### ï¿½ğ‘‰ğ‘‰ 

#### ( 

#### $1,000,000ğ‘‹ğ‘‹+$100,000 

#### ) 

#### =$1,000,000ğœğœ 

 ğ‘¥ğ‘¥ 

#### =$260,000 

## L12 Continuous Joint Distributions (WMS 5.1-8) 

1. Joint Density 

 a. Compare discrete/continuous pdf and joint pdf 

 b. Warehouse stocks up to two pallets of cereal ğ‘‹ğ‘‹ and one pallet of cereal ğ‘Œğ‘Œ, with density 

#### ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### =ğ‘ğ‘ 

#### ( 

#### ğ‘¥ğ‘¥+ 2 ğ‘¥ğ‘¥ 

#### ) 

#### ;ğ‘¥ğ‘¥ âˆˆ 

#### [ 

#### 0, 2 

#### ] 

#### ,ğ‘¥ğ‘¥âˆˆ[0, 1]. 

 c. Height of joint pdf represents likelihood of particular (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥) pairs. Must integrate to one 

 (double integral). 

 i. 1 = âˆ« âˆ« 

#### ğ‘ğ‘(ğ‘¥ğ‘¥+ 2 ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### = 

#### âˆ« 

#### (ğ‘ğ‘ğ‘¥ğ‘¥+ğ‘ğ‘)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

 = 4 ğ‘ğ‘ requires ğ‘ğ‘= 

 1 

 4 

#### , 

 or ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### = 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 2 

#### ğ‘¥ğ‘¥;ğ‘¥ğ‘¥ âˆˆ 

#### [ 

#### 0, 2 

#### ] 

#### ,ğ‘¥ğ‘¥âˆˆ[0, 1]. 

 d. Mode: since upward sloping in both dimensions, mode at 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### =(2, 1) 

2. Marginal densities 

---

 a. Analogous to margins of table in discrete joint distribution: total probability of particular 

 realization of ğ‘¥ğ‘¥ is the sum of all joint probabilities of (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥) pairs, with that particular ğ‘¥ğ‘¥ 

 value, but any ğ‘¥ğ‘¥ value in domain. 

 b. ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### =âˆ« 

 1 

 4 

#### ( 

#### ğ‘¥ğ‘¥+ 2 ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### = 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 4 

#### ;ğ‘¥ğ‘¥ âˆˆ[0, 2] 

 c. ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### (ğ‘¥ğ‘¥)= 

#### âˆ« 

 1 

 4 

#### (ğ‘¥ğ‘¥+ 2 ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### = 

 1 

 2 

#### +ğ‘¥ğ‘¥;ğ‘¥ğ‘¥âˆˆ[0, 1] 

 d. Subscript simply distinguishes ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

 (.5) from ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### (.5) 

 e. Moments: means, standard deviations 

 i. ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### =ğ¸ğ¸(ğ‘‹ğ‘‹)= 

#### âˆ« 

#### ğ‘¥ğ‘¥ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### =âˆ« ğ‘¥ğ‘¥ï¿½ 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 4 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### = 

 2 

 3 

#### + 

 1 

 2 

#### = 

 7 

 6 

 ii. ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )= 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### = 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 4 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### =1 + 

 2 

 3 

#### = 

 5 

 3 

 iii. ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

 5 

 3 

#### âˆ’ï¿½ 

 7 

 6 

#### ï¿½ 

 2 

#### = 

 11 

 36 

 iv. ğœğœ 

 ğ‘¥ğ‘¥ 

#### =ï¿½ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ï¿½ 

 11 

 36 

#### â‰ˆ.55 

 v. ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =ğ¸ğ¸(ğ‘Œğ‘Œ)= 

#### âˆ« 

#### ğ‘¥ğ‘¥ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### (ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### = 

#### âˆ« 

#### ğ‘¥ğ‘¥ï¿½ 

 1 

 2 

#### +ğ‘¥ğ‘¥ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### = 

 1 

 4 

#### + 

 1 

 3 

#### = 

 7 

 12 

 vi. ğ¸ğ¸(ğ‘Œğ‘Œ 

 2 

#### )= 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### (ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### =âˆ« ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 1 

 2 

#### +ğ‘¥ğ‘¥ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### = 

 1 

 6 

#### + 

 1 

 4 

#### = 

 5 

 12 

 vii. ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘Œğ‘Œ 

#### ) 

#### =ğ¸ğ¸ 

#### ( 

#### ğ‘Œğ‘Œ 

 2 

#### ) 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

 2 

#### = 

 5 

 12 

#### âˆ’ï¿½ 

 7 

 12 

#### ï¿½ 

 2 

#### = 

 11 

 144 

 viii. ğœğœ 

 ğ‘¦ğ‘¦ 

#### = 

#### ï¿½ 

#### ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)=ï¿½ 

 11 

 144 

#### â‰ˆ0. 28 

 ix. Could also derive mode, median, cdf, percentiles of ğ‘‹ğ‘‹ or ğ‘Œğ‘Œ 

 f. Independence requires ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### =ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

 for all (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥) pairs. 

 i. ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ not independent here, since ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)= 

 1 

 4 

#### (ğ‘¥ğ‘¥+ 2 ğ‘¥ğ‘¥)â‰ ï¿½ 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 4 

#### ï¿½ï¿½ 

 1 

 2 

#### +ğ‘¥ğ‘¥ï¿½ 

 (e.g. when (ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=(0, 0)) 

3. Correlation 

 a. ğ¸ğ¸(ğ‘‹ğ‘‹ğ‘Œğ‘Œ)= âˆ« âˆ« 

#### ğ‘¥ğ‘¥ğ‘¥ğ‘¥ ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### =âˆ« âˆ« ğ‘¥ğ‘¥ğ‘¥ğ‘¥ï¿½ 

 1 

 4 

#### ( 

#### ğ‘¥ğ‘¥+ 2 ğ‘¥ğ‘¥ 

#### ) 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### =âˆ« ï¿½ 

 1 

 8 

#### ğ‘¥ğ‘¥ 

 2 

#### + 

 1 

 6 

#### ğ‘¥ğ‘¥ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 ğ‘¥ğ‘¥=0 

#### = 

 1 

 3 

#### + 

 1 

 3 

#### = 

 2 

 3 

---

 b. ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### =ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ)=ğ¸ğ¸(ğ‘‹ğ‘‹ğ‘Œğ‘Œ)âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### = 

 2 

 3 

#### âˆ’ï¿½ 

 7 

 6 

#### ï¿½ï¿½ 

 7 

 12 

#### ï¿½=âˆ’ 

 1 

 72 

 c. ğœŒğœŒ= 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

#### = 

 âˆ’ 

 1 

 72 

 (. 55 )(. 28 ) 

#### â‰ˆâˆ’.09 

4. Practice example: ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

 =ğ‘ğ‘(1âˆ’ğ‘¥ğ‘¥ğ‘¥ğ‘¥) for ğ‘¥ğ‘¥,ğ‘¥ğ‘¥âˆˆ 

#### [ 

#### 0, 1 

#### ] 

 a. Find ğ‘ğ‘: âˆ« âˆ« ğ‘ğ‘ 

#### ( 

#### 1 âˆ’ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¥ğ‘¥=0 

#### = 

 3 

 4 

 ğ‘ğ‘ implies ğ‘ğ‘= 

 4 

 3 

 b. Find marginal densities ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### , ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### : ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)= 

#### âˆ« 

 4 

 3 

#### ( 1 âˆ’ğ‘¥ğ‘¥ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### =â‹¯= 

 4 

 3 

#### âˆ’ 

 2 

 3 

 ğ‘¥ğ‘¥ for ğ‘¥ğ‘¥ âˆˆ 

#### [ 

#### 0, 1 

#### ] 

#### ; 

 symmetrically, ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### = 

 4 

 3 

#### âˆ’ 

 2 

 3 

 ğ‘¥ğ‘¥ for ğ‘¥ğ‘¥âˆˆ 

#### [ 

#### 0, 1 

#### ] 

 c. Find means ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 and ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

 and standard deviations ğœğœ 

 ğ‘¥ğ‘¥ 

 and ğœğœ 

 ğ‘¦ğ‘¦ 

#### : 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### =ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### =âˆ« ğ‘¥ğ‘¥ï¿½ 

 4 

 3 

#### âˆ’ 

 2 

 3 

#### ğ‘¥ğ‘¥ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¥ğ‘¥=0 

#### =â‹¯= 

 4 

 9 

#### ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )= 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 4 

 3 

#### âˆ’ 

 2 

 3 

#### ğ‘¥ğ‘¥ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¥ğ‘¥=0 

#### =â‹¯= 

 5 

 18 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

 5 

 18 

#### âˆ’ï¿½ 

 4 

 9 

#### ï¿½ 

 2 

#### = 

 13 

 162 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

#### =ï¿½ 

 13 

 162 

#### â‰ˆ.283 

 Symmetrically, ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### = 

 4 

 9 

#### , ğœğœ 

 ğ‘¦ğ‘¦ 

#### â‰ˆ.283 

 d. Correlation ğœŒğœŒ: 

#### ğ¸ğ¸(ğ‘‹ğ‘‹ğ‘Œğ‘Œ)= 

#### âˆ« âˆ« 

#### ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 4 

 3 

#### ( 1 âˆ’ğ‘¥ğ‘¥ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¦ğ‘¦=0 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 1 

 ğ‘¥ğ‘¥=0 

#### = 

 5 

 27 

#### ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### =ğ¸ğ¸(ğ‘‹ğ‘‹ğ‘Œğ‘Œ)âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### â‰ˆ 

 5 

 27 

#### âˆ’ï¿½ 

 4 

 9 

#### ï¿½ 

 2 

#### =âˆ’ 

 1 

 81 

#### â‰ˆâˆ’.012 

#### ğœŒğœŒ= 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### = 

 âˆ’ 

 1 

 81 

 ï¿½ 

 13 

 162 

 ï¿½ 

 13 

 162 

#### =âˆ’ 

 2 

 13 

#### â‰ˆâˆ’0. 154 

## L13 Conditional Distributions (WMS 5.3, 11) 

1. Recall distribution of cell phone use and car accidents: 

#### ğ‘Œğ‘Œ= 0 ğ‘Œğ‘Œ= 1 ğ‘Œğ‘Œ= 2 

#### ğ‘‹ğ‘‹= 0 .60 .08 .02 

#### ğ‘‹ğ‘‹= 1 .10 .12 .08 

 a. Recall ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### =.3,ğœğœ 

 ğ‘¥ğ‘¥ 

#### â‰ˆ.458,ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =.4,ğœğœ 

 ğ‘¦ğ‘¦ 

#### â‰ˆ.663,ğœŒğœŒâ‰ˆ.527 

2. Conditional probability 

---

 a. Cell phone use among those with two accidents ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹= 1 |ğ‘Œğ‘Œ= 2 )= 

. 08 

. 10 

 =.80 versus 

 those with no accidents ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘‹ğ‘‹= 1 

#### | 

#### ğ‘Œğ‘Œ= 0 

#### ) 

#### = 

. 10 

. 70 

#### â‰ˆ0. 14 

 b. Practice: find ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 0 |ğ‘‹ğ‘‹= 0 )= 

. 60 

. 7 

#### â‰ˆ.86, ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 1 |ğ‘‹ğ‘‹= 0 )= 

. 08 

. 7 

#### =.11, ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 2 |ğ‘‹ğ‘‹= 0 )= 

. 02 

. 7 

#### â‰ˆ 

#### .03, ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 0 |ğ‘‹ğ‘‹= 1 )= 

. 10 

. 3 

#### â‰ˆ.33, ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 1 |ğ‘‹ğ‘‹= 1 )= 

. 12 

. 3 

#### =.40, ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### ( 2 |ğ‘‹ğ‘‹= 1 )= 

. 08 

. 3 

#### â‰ˆ.27 

3. Conditional distribution 

 a. ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥|ğ‘‹ğ‘‹=0) and ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥|ğ‘‹ğ‘‹=1) are legitimate distribution functions (numerators sum to 

 denominator) 

 b. Plot, and compare with ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥) 

 c. Conditional mean 

 i. ğ¸ğ¸(ğ‘Œğ‘Œ 

#### | 

#### ğ‘‹ğ‘‹= 1 )= 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ(ğ‘Œğ‘Œ=ğ‘¥ğ‘¥ 

#### | 

#### ğ‘‹ğ‘‹= 1 ) â‰ˆ 0 (. 33)+ 1 (. 40)+ 2 (. 27)=.94 

 ii. Practice ğ¸ğ¸(ğ‘Œğ‘Œ|ğ‘‹ğ‘‹= 0 )â‰ˆ 0 (. 86)+ 1 (. 11)+ 2 (. 03)=.17 

 d. Average number of car accidents is higher for cell phone users than for non-users. This 

 still doesnâ€™t imply causation! 

 e. Conditional standard deviation 

 i. Just like ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)=ğ¸ğ¸(ğ‘Œğ‘Œ 

 2 

#### )âˆ’ 

#### [ 

#### ğ¸ğ¸(ğ‘Œğ‘Œ) 

#### ] 

 2 

#### , 

#### ğ‘‰ğ‘‰(ğ‘Œğ‘Œ|ğ‘‹ğ‘‹=ğ‘¥ğ‘¥)=ğ¸ğ¸(ğ‘Œğ‘Œ 

 2 

#### |ğ‘‹ğ‘‹=ğ‘¥ğ‘¥)âˆ’[ğ¸ğ¸(ğ‘Œğ‘Œ|ğ‘‹ğ‘‹=ğ‘¥ğ‘¥)] 

 2 

 ii. (If time) Example : ğ¸ğ¸(ğ‘Œğ‘Œ 

 2 

#### | 

#### ğ‘‹ğ‘‹= 1 ) â‰ˆ 0 

 2 

#### (. 33)+ 1 

 2 

#### (. 40)+ 2 

 2 

 (. 27)=1. 21, so 

 In this case, ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘Œğ‘Œ 

#### | 

#### ğ‘‹ğ‘‹= 1 

#### ) 

#### â‰ˆ1. 21âˆ’. 94 

 2 

 â‰ˆ0. 326, and ğœğœ 

 ğ‘¦ğ‘¦|ğ‘‹ğ‘‹=1 

#### â‰ˆâˆš. 326â‰ˆ0. 57 

 By a similar derivation, ğœğœ 

 ğ‘¦ğ‘¦|ğ‘‹ğ‘‹=0 

 â‰ˆ0. 41; cell phone use increases variance. 

4. In an effort to establish causation, could find ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥|ğ‘ğ‘=ğ‘§ğ‘§)= 

 ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦,ğ‘§ğ‘§) 

 ğ‘ƒğ‘ƒ 

 ğ‘§ğ‘§ 

 (ğ‘§ğ‘§) 

 or ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥|ğ‘ğ‘=ğ‘§ğ‘§)= 

 ğ‘‘ğ‘‘ 

 ( ğ‘¥ğ‘¥,ğ‘¦ğ‘¦,ğ‘§ğ‘§ 

 ) 

 ğ‘‘ğ‘‘ 

 ğ‘§ğ‘§ 

 ( ğ‘§ğ‘§ 

 ) 

 , and then ğœŒğœŒ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦|ğ‘§ğ‘§ 

 =ğ¶ğ¶ğ‘›ğ‘›ğ¶ğ¶ğ¶ğ¶(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ|ğ‘ğ‘=ğ‘§ğ‘§). For example, find correlation between cell phone 

 use and car accidents among teenagers. 

5. Continuous densities 

 a. Recall joint density of cereal inventory, ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### = 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 2 

#### ğ‘¥ğ‘¥;ğ‘¥ğ‘¥ âˆˆ 

#### [ 

#### 0, 2 

#### ] 

#### ,ğ‘¥ğ‘¥âˆˆ 

#### [ 

#### 0, 1 

#### ] 

 b. Recall marginal densities ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)= 

 1 

 4 

#### ğ‘¥ğ‘¥+ 

 1 

 4 

 ;ğ‘¥ğ‘¥ âˆˆ[0, 2] and ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### (ğ‘¥ğ‘¥)= 

 1 

 2 

#### +ğ‘¥ğ‘¥;ğ‘¥ğ‘¥âˆˆ[0, 1], 

 means ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### = 

 7 

 6 

#### , ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### = 

 7 

 12 

 , standard deviations ğœğœ 

 ğ‘¥ğ‘¥ 

#### â‰ˆ.55, ğœğœ 

 ğ‘¦ğ‘¦ 

#### â‰ˆ0. 28 

 c. Conditional density ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥|ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)= 

 ğ‘‘ğ‘‘ 

 ( ğ‘¥ğ‘¥,ğ‘¦ğ‘¦ 

 ) 

 ğ‘‘ğ‘‘ ğ‘¥ğ‘¥ 

 ( ğ‘¦ğ‘¦ 

 ) 

#### = 

 1 

 4 

 ğ‘¥ğ‘¥+ 

 1 

 2 

 ğ‘¦ğ‘¦ 

 1 

 2 

 +ğ‘¦ğ‘¦ 

#### = 

 ğ‘¥ğ‘¥+2ğ‘¦ğ‘¦ 

 2+4ğ‘¦ğ‘¦ 

 ;ğ‘¥ğ‘¥ âˆˆ[0, 2]. For example, 

#### ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥|ğ‘Œğ‘Œ= 0 )= 

 ğ‘¥ğ‘¥ 

 2 

#### ;ğ‘¥ğ‘¥ âˆˆ[0, 2] 

---

 d. Conditional mean and standard deviation 

#### ğ¸ğ¸(ğ‘‹ğ‘‹|ğ‘Œğ‘Œ= 0 )= 

#### âˆ« 

#### ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 2 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 0 

#### = 

#### ï¿½ 

 1 

 6 

#### ğ‘¥ğ‘¥ 

 3 

#### ï¿½ 

 0 

 2 

#### = 

 8 

 6 

#### = 

 4 

 3 

#### ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### | 

#### ğ‘Œğ‘Œ= 0 )= 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥ 

#### | 

#### 0 )ğ‘‘ğ‘‘ğ‘¥ğ‘¥= 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 ğ‘¥ğ‘¥ 

 2 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 0 

#### =ï¿½ 

 1 

 8 

#### ğ‘¥ğ‘¥ 

 4 

#### ï¿½ 

 0 

 2 

#### = 2 

#### ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### | 

#### ğ‘Œğ‘Œ= 0 

#### ) 

#### =ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

 2 

#### | 

#### ğ‘Œğ‘Œ= 0 

#### ) 

#### âˆ’ 

#### [ 

#### ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### | 

#### ğ‘Œğ‘Œ= 0 

#### )] 

 2 

#### = 2 âˆ’ï¿½ 

 4 

 3 

#### ï¿½ 

 2 

#### = 

 2 

 9 

#### . 

#### ğœğœ 

 ğ‘¥ğ‘¥|ğ‘Œğ‘Œ=0 

#### =ï¿½ 

 2 

 9 

 Thus, when ğ‘Œğ‘Œ= 0 : density of ğ‘‹ğ‘‹ is steeper, mean of ğ‘‹ğ‘‹ is higher, variance is lower. 

 e. More generically, for arbitrary ğ‘¥ğ‘¥ value, 

#### ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### | 

#### ğ‘Œğ‘Œ=ğ‘¥ğ‘¥ 

#### ) 

#### =âˆ«ğ‘¥ğ‘¥ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### | 

#### ğ‘¥ğ‘¥ 

#### ) 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥=âˆ« ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥+2ğ‘¦ğ‘¦ 

 2+4ğ‘¦ğ‘¦ 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 0 

#### = 

 1 

 2+4ğ‘¦ğ‘¦ 

#### âˆ« 

#### (ğ‘¥ğ‘¥ 

 2 

#### + 2 ğ‘¥ğ‘¥ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 0 

#### = 

 1 

 2+4ğ‘¦ğ‘¦ 

#### ï¿½ 

 1 

 3 

#### ğ‘¥ğ‘¥ 

 3 

#### +ğ‘¥ğ‘¥ 

 2 

#### ğ‘¥ğ‘¥ 

#### ï¿½ 

 0 

 2 

#### = 

 1 

 2+4ğ‘¦ğ‘¦ 

#### ï¿½ 

 8 

 3 

#### + 4 ğ‘¥ğ‘¥ ï¿½= 

 4+6ğ‘¦ğ‘¦ 

 3+6ğ‘¦ğ‘¦ 

 For example, ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### | 

#### ğ‘¥ğ‘¥= 0 

#### ) 

#### = 

 4 

 3 

 as before 

 Practice: ğ¸ğ¸(ğ‘‹ğ‘‹|ğ‘¥ğ‘¥= 1 )= 

 10 

 9 

#### ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### |ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)= 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥|ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

#### = 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ï¿½ 

 ğ‘¥ğ‘¥+2ğ‘¦ğ‘¦ 

 2+4ğ‘¦ğ‘¦ 

#### ï¿½ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 2 

 0 

#### =â‹¯= 

 6+8ğ‘¦ğ‘¦ 

 3+6ğ‘¦ğ‘¦ 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹|ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### |ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)âˆ’[ğ¸ğ¸(ğ‘‹ğ‘‹|ğ‘Œğ‘Œ=ğ‘¥ğ‘¥)] 

 2 

#### =ï¿½ 

 6+8ğ‘¦ğ‘¦ 

 3+6ğ‘¦ğ‘¦ 

#### ï¿½âˆ’ï¿½ 

 4+6ğ‘¦ğ‘¦ 

 3+6ğ‘¦ğ‘¦ 

#### ï¿½ 

 2 

#### ğœğœ 

 ğ‘¥ğ‘¥|ğ‘Œğ‘Œ=ğ‘¦ğ‘¦ 

#### = 

#### ï¿½ 

#### ï¿½ 

 6+8ğ‘¦ğ‘¦ 

 3+6ğ‘¦ğ‘¦ 

#### ï¿½âˆ’ï¿½ 

 4+6ğ‘¦ğ‘¦ 

 3+3ğ‘¦ğ‘¦ 

#### ï¿½ 

 2 

. For example, ğœğœ 

 ğ‘¥ğ‘¥|ğ‘Œğ‘Œ=0 

#### = 

#### ï¿½ 

 6 

 3 

#### âˆ’ï¿½ 

 4 

 3 

#### ï¿½ 

 2 

#### =ï¿½ 

 2 

 9 

 as before, 

#### ğœğœ 

 ğ‘¥ğ‘¥|ğ‘Œğ‘Œ=1 

#### = 

#### ï¿½ 

#### ï¿½ 

 14 

 9 

#### ï¿½âˆ’ï¿½ 

 10 

 9 

#### ï¿½ 

 2 

#### =ï¿½ 

 26 

 81 

#### â‰ˆ 

 5 

 9 

 Thus, when ğ‘Œğ‘Œ= 1 , density of ğ‘‹ğ‘‹ is less steep, mean is lower, variance is higher. 

6. With three variables, can derive joint distribution of ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ conditional on ğ‘ğ‘ 

 a. Israel covid data 

 i. When covid Delta variant hit, Israeli hospitals filled up with covid patients. 60% 

 of these patients had already been vaccinated. 

 ii. Natural (but erroneous) conclusion: vaccines make things worse, not better! 

---

 iii. Nearly 80% of Israelis over age 12 were vaccinated against covid, so if â€œrandom 

 draw,â€ 80% of hospital patients should have been vaccinated; lower rate means 

 treatment helped. 

 iv. 

#### 7. 

## L14 Regressions (WMS 5.3, 11) 

1. Regressions 

 a. Sir Francis Galton 1886 (cousin of Darwin) 

 b. Use data to determine (average) linear relationship ğ‘Œğ‘Œ=ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

 ğ‘‹ğ‘‹. Once relationship is 

 known, we can predict ğ‘Œğ‘Œ for any ğ‘‹ğ‘‹ value (even out of sample), as if through a crystal 

 ball! 

 c. Examples: 

 i. Income ğ‘Œğ‘Œ as function of education ğ‘‹ğ‘‹ 

 ii. Unemployment ğ‘Œğ‘Œ next year as function of (e.g. fiscal or monetary) policy ğ‘‹ğ‘‹ 

 iii. Stock price tomorrow ğ‘Œğ‘Œ as function of todayâ€™s earnings/price ğ‘‹ğ‘‹ 

 iv. Consultantâ€™s â€œsecret formulaâ€ predicting sales, etc. 

 d. Puts units on correlation (â€œeducation and income are strongly correlatedâ€ vs. â€œeach year 

 of education is associated with an additional $4k of incomeâ€) 

 e. Working example: education ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 = 15 years; ğœğœ 

 ğ‘¥ğ‘¥ 

 = 3 years; income ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =$70,000; ğœğœ 

 ğ‘¦ğ‘¦ 

#### = 

 $20,000; correlation ğœŒğœŒ=.6 

 f. Any ğ›½ğ›½ 

 0 

 and ğ›½ğ›½ 

 1 

 determine line ğ‘Œğ‘Œ=ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

 ğ‘‹ğ‘‹, implying an error term ğœ€ğœ€=ğ‘Œğ‘Œâˆ’ğ›½ğ›½ 

 0 

#### âˆ’ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹ 

 such that the data satisfies ğ‘Œğ‘Œ=ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

 ğ‘‹ğ‘‹+ğœ€ğœ€. We can choose ğ›½ğ›½ 

 0 

 and ğ›½ğ›½ 

 1 

 so that the 

 resulting line is as useful as possible. 

 g. â€œLeast squaresâ€ regression: choose ğ›½ğ›½ 

 0 

 and ğ›½ğ›½ 

 1 

 to minimize ğ¸ğ¸(ğœ€ğœ€ 

 2 

#### ) 

 i. Equivalently, choose ğ›½ğ›½ 

 0 

 so that ğ¸ğ¸(ğœ€ğœ€)= 0 and ğ›½ğ›½ 

 1 

 to minimize ğ‘‰ğ‘‰(ğœ€ğœ€) 

 ii. Can use other criteria (e.g. least absolute deviation ğ¸ğ¸(|ğœ€ğœ€|)), but less common 

2. Intercept 

 a. If ğ›½ğ›½ 

 0 

 is high, most ğœ€ğœ€ 

 ğ‘–ğ‘– 

 will be negative; if ğ›½ğ›½ 

 0 

 is low, most ğœ€ğœ€ 

 ğ‘–ğ‘– 

 will be positive 

 b. ğ¸ğ¸(ğœ€ğœ€)=ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### âˆ’ğ›½ğ›½ 

 0 

#### âˆ’ğ›½ğ›½ 

 1 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 = 0 implies that ğ›½ğ›½ 

 0 

#### =ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### âˆ’ğ›½ğ›½ 

 1 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### . 

 Easier: ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 , so regression line passes through ï¿½ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ,ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### ï¿½ 

---

 c. Example: ğ›½ğ›½ 

 0 

#### =$70,000âˆ’$4,000â‹… 15 =$10,000 

3. Slope 

 a. ğ‘‰ğ‘‰(ğœ€ğœ€)=ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)+ğ‘‰ğ‘‰(âˆ’ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹)+ 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘(ğ‘Œğ‘Œ,âˆ’ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹)=ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### +ğ›½ğ›½ 

 1 

 2 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 2 ğ›½ğ›½ 

 1 

#### ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

 b. To minimize, 0 = 

 ğ‘‘ğ‘‘ğ‘‘ğ‘‘(ğœ€ğœ€) 

 ğ‘‘ğ‘‘ğ›½ğ›½ 

 1 

#### = 2 ğ›½ğ›½ 

 1 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 2 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

 c. Solution ğ›½ğ›½ 

 1 

#### = 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### =ğœŒğœŒ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 d. Slope is simply (normalized) correlation coefficient 

 e. Example: ğ›½ğ›½ 

 1 

#### =.6 

 $20, 000 

 3ğ‘¦ğ‘¦ğ‘¦ğ‘¦. 

 =$4,000/yr. (e.g. four-year degree provides extra $16,000/yr) 

 f. Equivalently, can derive same value by choosing ğ›½ğ›½ 

 1 

 such that ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ 

#### ( 

#### ğ‘‹ğ‘‹,ğœ€ğœ€ 

#### ) 

#### = 0 (HW) 

4. Predictions 

 a. High school grad (ğ‘‹ğ‘‹ 

 âˆ— 

 = 12 ) expects ğ‘Œğ‘Œ 

 âˆ— 

#### =$10ğ‘˜ğ‘˜+$4ğ‘˜ğ‘˜(12) =$58ğ‘˜ğ‘˜ 

 b. College grad (ğ‘‹ğ‘‹ 

 âˆ— 

 = 16 ) expects ğ‘Œğ‘Œ 

 âˆ— 

#### =$10ğ‘˜ğ‘˜+$4ğ‘˜ğ‘˜(16) =$74ğ‘˜ğ‘˜ 

 c. Three PhDs (ğ‘‹ğ‘‹ 

 âˆ— 

 = 31 ) expects ğ‘Œğ‘Œ 

 âˆ— 

#### =$10ğ‘˜ğ‘˜+$4ğ‘˜ğ‘˜(31) =$134ğ‘˜ğ‘˜ 

 i. This assumes linear trend holds up, constant returns to scale (which may not be 

 reasonable); in econometrics (Econ 388), learn nonlinear regressions 

 d. Standardized 

 i. 

 ğ‘Œğ‘Œ 

 âˆ— 

 âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### =ğœŒğœŒ 

 ğ‘‹ğ‘‹ 

 âˆ— 

 âˆ’ğœ‡ğœ‡ ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 (since ğ›½ğ›½ 

 1 

#### =ğœŒğœŒ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### , ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 , and ğ‘Œğ‘Œ 

 âˆ— 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹ 

 âˆ— 

#### ). 

 ii. If ğ‘‹ğ‘‹ 

 âˆ— 

 is 1 or 2 or ğ‘˜ğ‘˜ standard deviation above ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 then ğ‘Œğ‘Œ 

 âˆ— 

 is ğœŒğœŒ or 2 ğœŒğœŒ or ğ‘˜ğ‘˜ğœŒğœŒ 

 standard deviations above ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

 e. Stay in school to get rich? 

 i. Maybe. Correlation might reflect causation; if so, staying in school boosts 

 income. 

 ii. Regressions still just express correlation, not causation (now in meaningful 

 units) 

 iii. Maybe not. Maybe spurious (rich kids enjoy school, but would be rich either 

 way) or maybe helps those who already attend (e.g. engineers) but those who 

 choose not to attend (e.g. mechanics, artists) wouldnâ€™t benefit. 

 iv. Either way, predict higher incomes for those who do stay in school 

 v. But maybe not for those who choose not, but compelled/advised to go to school 

 f. Reverse predictions 

 i. What if worker makes $100k income and asks for you to guess education? 

---

 ii. Could read regression backward, but it was designed to minimize errors in 

 income not errors in education 

 iii. Better to start over, with income as ğ‘‹ğ‘‹ and education as ğ‘Œğ‘Œ 

5. Errors 

 a. ğœ€ğœ€ 

 ğ‘–ğ‘– 

#### =ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’(ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ) 

 b. De-trend data (e.g. â€œskillâ€ or â€œluckâ€, above and beyond education) 

 c. Example: who is more genius (or luckier): 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

 =(12, $80ğ‘˜ğ‘˜) or 

#### ( 

#### ğ‘¥ğ‘¥,ğ‘¥ğ‘¥ 

#### ) 

#### = 

#### ( 

#### 16 , $100ğ‘˜ğ‘˜ 

#### ) 

#### ? 

 i. $80ğ‘˜ğ‘˜âˆ’ 

#### ( 

#### 10 + 4 â‹… 12 

#### ) 

#### =$22ğ‘˜ğ‘˜ 

 ii. $100ğ‘˜ğ‘˜âˆ’( 10 + 4 â‹… 16 )=$26ğ‘˜ğ‘˜ 

 d. Variance ğœğœ 

 ğœ€ğœ€ 

 2 

 of error distribution tells us how far people are off the regression line 

#### ğœğœ 

 ğœ€ğœ€ 

 2 

#### =ğ‘‰ğ‘‰(ğ‘Œğ‘Œâˆ’ğ›½ğ›½ 

 0 

#### âˆ’ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹)=ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### +ğ›½ğ›½ 

 1 

 2 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### âˆ’ 2 ğ›½ğ›½ 

 1 

#### ğ‘ğ‘ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ) 

#### = 20 ğ‘˜ğ‘˜ 

 2 

#### + 4 ğ‘˜ğ‘˜ 

 2 

#### 3 

 2 

#### âˆ’ 2 ( 4 ğ‘˜ğ‘˜)(. 6 Ã— 20 ğ‘˜ğ‘˜Ã— 3 )=($16ğ‘˜ğ‘˜) 

 2 

6. Explanatory power 

 a. Partition ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)=ğ›½ğ›½ 

 1 

 2 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)+ğ‘‰ğ‘‰(ğœ€ğœ€)= 144 + 256 

 i. Note: 2 ğ›½ğ›½ 

 1 

 ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğœ€ğœ€)= 0 (see homework) because optimal slope extracts all 

 correlation 

 ii. This decomposes ğ‘‰ğ‘‰(ğ‘Œğ‘Œ) into â€œexplainedâ€ and â€œunexplainedâ€ (e.g. talent, luck, or 

 some other mystery). More accurately, variation that is â€œrelated to educationâ€ 

 and variation that is â€œunrelated to educationâ€ 

 b. â€œExplainedâ€ portion is ğœŒğœŒ 

 2 

 fraction of ğ‘‰ğ‘‰(ğ‘Œğ‘Œ) 

 i. ğ›½ğ›½ 

 1 

 2 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### =ï¿½ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

#### ğœŒğœŒï¿½ 

 2 

#### ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### =ğœŒğœŒ 

 2 

#### ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

 ii. In this case,. 6 

 2 

 =36% (â€œcoefficient of determinationâ€) 

 c. â€œUnexplainedâ€ portion is 1 âˆ’ğœŒğœŒ 

 2 

 i. In this case, 1 âˆ’. 6 

 2 

#### =64% 

 ii. Shortcut ğœğœ 

 ğœ€ğœ€ 

 2 

#### =.64 

#### ( 

#### 400 

#### ) 

#### = 256 = 

#### ( 

#### $16ğ‘˜ğ‘˜ 

#### ) 

 2 

 d. Implicit linearity of ğœŒğœŒ 

 i. Fundamentally, what does ğœŒğœŒ measure? 

 ii. ğ‘‹ğ‘‹ 

 2 

 is perfectly predictable from ğ‘‹ğ‘‹, but linear regression produces ğœŒğœŒ 

 2 

#### < 1 

 iii. Thus, ğ‘ğ‘ğ‘›ğ‘›ğ¶ğ¶ğ¶ğ¶ 

#### ( 

#### ğ‘‹ğ‘‹,ğ‘‹ğ‘‹ 

 2 

#### ) 

#### â‰  1 

 iv. ğœŒğœŒ fundamentally measures linear relationship (see homework) 

---

## Exam 1 Review 

1. Spiritual thought: prayer through lifeâ€™s trials, faith without works is dead, obedience gives 

 confidence 

2. Exam info 

 a. Any calculator 

 b. No time limit, predict 2-3 hours 

 c. Handout provided 

 d. Hard: typically C average 

3. Study tips 

 a. Take it seriously: equal weight with final exam 

 b. Start with study guide 

 c. Practice exams (first without solutions, then with) 

 d. Extra homework problems (or repeat homework problems) 

 e. Time saver: talk through steps, donâ€™t work out algebra 

4. Exam strategies 

 a. Donâ€™t forget to pray! 

 b. Extend familiar material to unfamiliar settings (good practice for real world) 

 c. Difficulty is similar to homework, but no TAs or books, so fewer Aâ€™s than homework 

 d. Average score is typically C, which averaged with Ahomework gives Boverall. 

 e. Show work and list what you know for partial credit (e.g. ğœŒğœŒ= 

 ğœğœ ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 , even if you canâ€™t 

 figure out ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### ) 

5. Key formulas 

 a. Binary events 

 i. ğ‘ƒğ‘ƒ 

#### ( 

#### ğ¸ğ¸ 

#### ) 

#### = 

 #ğ¸ğ¸ 

 #ğ‘†ğ‘† 

 ii. ğ¶ğ¶ 

 ğ‘˜ğ‘˜ 

 ğ‘›ğ‘› 

#### = 

 ğ‘›ğ‘›! 

 ğ‘˜ğ‘˜! 

 ( ğ‘›ğ‘›âˆ’ğ‘˜ğ‘˜ 

 ) ! 

 iii. ğ‘ƒğ‘ƒ(ğ´ğ´âˆªğµğµ)=ğ‘ƒğ‘ƒ(ğ´ğ´)+ğ‘ƒğ‘ƒ(ğµğµ)âˆ’ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğµğµ) 

 iv. Independent events: ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğµğµ)=ğ‘ƒğ‘ƒ(ğ´ğ´)ğ‘ƒğ‘ƒ(ğµğµ) (or ğ‘ƒğ‘ƒ(ğ´ğ´|ğµğµ)=ğ‘ƒğ‘ƒ(ğ´ğ´)) 

 v. ğ‘ƒğ‘ƒ(ğ´ğ´|ğµğµ)= 

 ğ‘ƒğ‘ƒ 

 ( ğ´ğ´âˆ©ğµğµ 

 ) 

 ğ‘ƒğ‘ƒ 

 ( ğµğµ 

 ) 

 vi. ğ‘ƒğ‘ƒ(ğ´ğ´âˆ©ğµğµ)=ğ‘ƒğ‘ƒ(ğµğµ|ğ´ğ´)ğ‘ƒğ‘ƒ(ğ´ğ´) 

 b. Random variables 

---

 i. Legitimate distribution? âˆ‘ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)=âˆ«ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥= 1 

 ii. Mode maximizes ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥) or ğ‘“ğ‘“(ğ‘¥ğ‘¥) (i.e. ğ‘“ğ‘“ 

 â€² 

 (ğ‘¥ğ‘¥)= 0 and ğ‘“ğ‘“ 

 â€²â€² 

#### (ğ‘¥ğ‘¥)< 0 ) 

 iii. ğœ‡ğœ‡=ğ¸ğ¸(ğ‘‹ğ‘‹)=âˆ‘ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)=âˆ«ğ‘¥ğ‘¥ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 iv. ğ¸ğ¸(ğ‘‹ğ‘‹ 

 3 

#### )=âˆ‘ğ‘¥ğ‘¥ 

 3 

#### ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)=âˆ«ğ‘¥ğ‘¥ 

 3 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 v. ğœğœ 

 2 

#### =ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ğ¸ğ¸[(ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡) 

 2 

#### ]=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )âˆ’ğœ‡ğœ‡ 

 2 

#### ; ğœğœ=ï¿½ğ‘‰ğ‘‰(ğ‘‹ğ‘‹) 

 vi. ğ¹ğ¹(ğ‘¥ğ‘¥)= âˆ« 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥ï¿½)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 âˆ’âˆ 

#### , ğ‘“ğ‘“(ğ‘¥ğ‘¥)=ğ¹ğ¹â€²(ğ‘¥ğ‘¥) 

 vii. ğ‘ƒğ‘ƒ(ğ‘ğ‘<ğ‘‹ğ‘‹<ğ‘ğ‘)=ğ¹ğ¹(ğ‘ğ‘)âˆ’ğ¹ğ¹(ğ‘ğ‘) 

 viii. Percentile: solve ğ¹ğ¹(ğœ™ğœ™ 

. 5 

#### )=.5 

 ix. ğ‘“ğ‘“(ğ‘¥ğ‘¥)=ğ¹ğ¹â€²(ğ‘¥ğ‘¥) 

c. Joint distributions 

 i. Legitimate joint distribution? âˆ‘ âˆ‘ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)= âˆ«âˆ« 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ğ‘‘ğ‘‘ğ‘¥ğ‘¥= 1 

 ii. Marginal distribution 

#### ğ‘ƒğ‘ƒ 

 ğ‘¥ğ‘¥ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### = 

#### âˆ‘ 

#### ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥) 

 ğ‘¦ğ‘¦ 

#### ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)=âˆ«ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 iii. Independent variables 

#### ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=ğ‘ƒğ‘ƒ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)ğ‘ƒğ‘ƒ 

 ğ‘¦ğ‘¦ 

#### (ğ‘¥ğ‘¥) 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)=ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### (ğ‘¥ğ‘¥)ğ‘“ğ‘“ 

 ğ‘¦ğ‘¦ 

#### (ğ‘¥ğ‘¥) 

 iv. ğ¸ğ¸ï¿½ 

 ğ‘‹ğ‘‹ 

 ğ‘Œğ‘Œ 

#### ï¿½=âˆ‘ âˆ‘ï¿½ 

 ğ‘¥ğ‘¥ 

 ğ‘¦ğ‘¦ 

#### ï¿½ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)= 

#### âˆ«âˆ« 

#### ï¿½ 

 ğ‘¥ğ‘¥ 

 ğ‘¦ğ‘¦ 

#### ï¿½ğ‘“ğ‘“(ğ‘¥ğ‘¥,ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 v. ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ)=ğ¸ğ¸ï¿½(ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### )ï¿½ğ‘Œğ‘Œâˆ’ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### ï¿½ï¿½=ğ¸ğ¸(ğ‘‹ğ‘‹ğ‘Œğ‘Œ)âˆ’ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

 vi. ğœŒğœŒ= 

 ğ¶ğ¶ğ‘œğ‘œğ¶ğ¶(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ) 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 vii. Conditional distribution 

#### ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘‹ğ‘‹=ğ‘¥ğ‘¥ 

#### | 

#### ğ‘Œğ‘Œ= 3 

#### ) 

#### = 

 ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥, 3 ) 

 ğ‘ƒğ‘ƒ 

 ğ‘¥ğ‘¥ 

 ( 3 ) 

#### ğ‘“ğ‘“ 

 ğ‘¥ğ‘¥ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### | 

#### ğ‘Œğ‘Œ= 3 

#### ) 

#### = 

 ğ‘‘ğ‘‘(ğ‘¥ğ‘¥, 3 ) 

 ğ‘‘ğ‘‘ 

 ğ‘¥ğ‘¥ 

 ( 3 ) 

 viii. ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### | 

#### ğ‘Œğ‘Œ= 3 )=âˆ‘ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥|ğ‘Œğ‘Œ= 3 )=âˆ«ğ‘¥ğ‘¥ğ‘“ğ‘“(ğ‘¥ğ‘¥|ğ‘Œğ‘Œ= 3 )ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 ix. ğ‘‰ğ‘‰(ğ‘‹ğ‘‹|ğ‘Œğ‘Œ= 3 )=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### |ğ‘Œğ‘Œ= 3 )âˆ’[ğ¸ğ¸(ğ‘‹ğ‘‹|ğ‘Œğ‘Œ= 3 )] 

 2 

d. Regressions 

 i. ğ›½ğ›½ 

 1 

#### = 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### =ğœŒğœŒ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 ii. ğ›½ğ›½ 

 0 

#### =ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### âˆ’ğ‘ğ‘ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 iii. 

 ğ‘‘ğ‘‘(ğ‘ğ‘+ğ‘ğ‘ğ‘‹ğ‘‹) 

 ğ‘‘ğ‘‘(ğ‘Œğ‘Œ) 

#### =ğœŒğœŒ 

 2 

---

 iv. ğœ€ğœ€ 

 ğ‘–ğ‘– 

#### =ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### âˆ’(ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

6. Algebra tricks 

 a. ğ¸ğ¸($100âˆ’$5ğ‘‹ğ‘‹)=$100âˆ’$5ğ¸ğ¸(ğ‘‹ğ‘‹) 

 b. ğ‘‰ğ‘‰($100âˆ’$5ğ‘‹ğ‘‹+$3ğ‘Œğ‘Œ)=ğ‘‰ğ‘‰($100)+ğ‘‰ğ‘‰(âˆ’$5ğ‘‹ğ‘‹)+ğ‘‰ğ‘‰($3ğ‘Œğ‘Œ)+ 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘(âˆ’$5ğ‘‹ğ‘‹, $3ğ‘Œğ‘Œ)=0 + 

#### ($5) 

 2 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)+($3) 

 2 

#### ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)âˆ’$30ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ) 

 c. ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘($100âˆ’$5ğ‘‹ğ‘‹,ğ‘Œğ‘Œ)=ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘($100,ğ‘Œğ‘Œ)+ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘(âˆ’$5,ğ‘Œğ‘Œ)= 0 âˆ’$5ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ) 

 d. ğ¶ğ¶ğ‘›ğ‘›ğ¶ğ¶ ğ¶ğ¶ 

#### ( 

#### $100âˆ’$5ğ‘‹ğ‘‹,ğ‘Œğ‘Œ 

#### ) 

#### =ğ¶ğ¶ğ‘›ğ‘›ğ¶ğ¶ğ¶ğ¶ 

#### ( 

#### âˆ’ğ‘‹ğ‘‹,ğ‘Œğ‘Œ 

#### ) 

#### =âˆ’ğ¶ğ¶ğ‘›ğ‘›ğ¶ğ¶ğ¶ğ¶ 

#### ( 

#### ğ‘‹ğ‘‹,ğ‘Œğ‘Œ 

#### ) 

7. Distributional relationships 

 a. If ğ‘‹ğ‘‹ âˆ¼ğ‘ğ‘ then 3 ğ‘‹ğ‘‹ âˆ¼ğ‘ğ‘ and ğ‘‹ğ‘‹+ 7 âˆ¼ğ‘ğ‘ 

 b. If ğ‘‹ğ‘‹ 

 1 

#### ,ğ‘‹ğ‘‹ 

 2 

 âˆ¼ğ‘ğ‘ then ğ‘‹ğ‘‹ 

 1 

#### +ğ‘‹ğ‘‹ 

 2 

#### âˆ¼ğ‘ğ‘ 

 c. If ğ‘ğ‘âˆ¼ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 then ğ‘ğ‘ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### ( 

#### 1 

#### ) 

 d. If ğ‘Šğ‘Š 

 1 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (3),ğ‘Šğ‘Š 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

 (5) independent then ğ‘Šğ‘Š 

 1 

#### +ğ‘Šğ‘Š 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

 (8) and 

 ğ‘Šğ‘Š 1 

 / 3 

 ğ‘Šğ‘Š2/ 5 

#### âˆ¼ğ¹ğ¹(3, 5) 

 e. If ğ‘ğ‘âˆ¼ğ‘ğ‘(0, 1) and ğ‘Šğ‘Š âˆ¼ğœ’ğœ’ 

 2 

 (ğœˆğœˆ) independent then 

 ğ‘ğ‘ 

 ï¿½ 

 ğ‘Šğ‘Š 

 ğœˆğœˆ 

#### âˆ¼ğ‘¡ğ‘¡(ğœˆğœˆ) 

8. Rejoice in how much weâ€™ve learned! 

## L15 Bernoulli, Uniform, Standard Normal (WMS 4.4-4.5) 

Spiritual thought 

1. Dealing with disappointment 

 a. In grad school, we took two years of courses, then two qualifying exams. If pass, four 

 years of research; if fail, retake or exit with Masters degree. I prepped hard, but on day 

 of exam, got hung up on one really hard question, lost track of time, didnâ€™t finish, and 

 failed! 

 b. I benefitted from a friendâ€™s experience, who had previously been preparing for 

 graduation (robes, parents in town, etc.), when checked grades: E! Couldnâ€™t graduate. 

 i. First reaction: denial. Must be a mistake! 

 ii. Second reaction: blame. Grading is unfair! 

 iii. Third reaction: dejection. Iâ€™m a failure. 

 iv. Fourth reaction: hope. Iâ€™m not a failure, I just failed at this thing. I can move 

 forward productively to the next step. Retook class, found a summer internship 

 that turned out to be career altering. 

---

 c. Scriptures 

 i. Joseph Smith in Liberty jail: â€œMy son, peace be unto they soul; thine adversity 

 and thine afflictions shall be but a small moment; and then, if thou endure it 

 well, God shall exalt thee on high; thou shalt triumph over all thy foesâ€ (D&C 

#### 121:7-8). 

 ii. â€œSearch diligently, pray always, and be believing, and all things shall work 

 together for your good, if ye walk uprightly and remember the covenant 

 wherewith ye have covenanted one with anotherâ€ (D&C 90:24). 

 iii. â€œ...All things work together for good for them that love God...â€ (Romans 8:28). 

 d. Midterm exam: If you performed less well than you hoped, press forward with a perfect 

 brightness of hope! Help the Lord make it work toward your good. 

 i. Learn what went wrong (like spelling bee mistakes, may always remember). 

 Final exam not cumulative per se, but does repeat concepts. 

 ii. Reassess study habits (e.g. understand every step of every question; donâ€™t just 

 trust TA or study group). 

2. ğ‘‹ğ‘‹ âˆ¼ Bernoulli(ğ‘ğ‘) (after Swiss mathematician Jacob Bernoulli, 1713) 

 a. Recall cell phone use ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹=ğ‘¥ğ‘¥)=ï¿½ 

#### . 7 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 0 

#### . 3 ğ‘–ğ‘–ğ‘“ğ‘“ ğ‘¥ğ‘¥= 1 

 b. Mean ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### = 0 

#### ( 

#### . 7 

#### ) 

#### + 1 

#### ( 

#### . 3 

#### ) 

#### =.3 

 c. Variance ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 2 

#### )âˆ’ğœ‡ğœ‡ 

 2 

#### =[ 0 

 2 

#### (. 7)+ 1 

 2 

#### (. 3)]âˆ’. 3 

 2 

#### =.21=(. 3)(.7) 

 d. Pattern: ğ¸ğ¸(ğ‘‹ğ‘‹)=ğ‘ğ‘, ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ğ‘ğ‘(1âˆ’ğ‘ğ‘) for â€œsuccessâ€ parameter ğ‘ğ‘ 

3. ğ‘‹ğ‘‹ âˆ¼ Uniform(ğ‘ğ‘,ğ‘ğ‘) 

 a. ğ‘“ğ‘“ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### =ğ‘˜ğ‘˜;ğ‘ğ‘ â‰¤ğ‘¥ğ‘¥ â‰¤ğ‘ğ‘ 

 b. ğ¹ğ¹ 

#### ( 

#### ğ‘¥ğ‘¥ 

#### ) 

#### =âˆ« ğ‘˜ğ‘˜ğ‘‘ğ‘‘ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 ğ‘ğ‘ 

#### =â‹¯= 

 ğ‘¥ğ‘¥âˆ’ğ‘ğ‘ 

 ğ‘ğ‘âˆ’ğ‘ğ‘ 

#### ;ğ‘ğ‘â‰¤ğ‘¥ğ‘¥ â‰¤ğ‘ğ‘ 

 c. ğœ‡ğœ‡= âˆ« 

#### ğ‘¥ğ‘¥ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 ğ‘ğ‘ 

 ğ‘ğ‘ 

#### =â‹¯= 

 ğ‘ğ‘+ğ‘ğ‘ 

 2 

 d. ğœğœ 

 2 

#### = 

#### âˆ« 

#### ğ‘¥ğ‘¥ 

 2 

#### ğ‘“ğ‘“(ğ‘¥ğ‘¥)ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 ğ‘ğ‘ 

 ğ‘ğ‘ 

#### âˆ’ğœ‡ğœ‡ 

 2 

#### =â‹¯= 

 ( ğ‘ğ‘âˆ’ğ‘ğ‘ 

 ) 

 2 

 12 

 e. Example: 90 second stop light 

 i. Average wait time ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### = 45 

 ii. Standard deviation ğœğœ=ï¿½ 

 ( 90 ) 

 2 

 12 

#### â‰ˆ 26 

 iii. Wait less than 30 seconds with probability ğ¹ğ¹( 30 )= 

 30 âˆ’0 

 90âˆ’0 

#### = 

 1 

 3 

1. Standard normal ğ‘ğ‘(0, 1) 

---

 a. ğ‘“ğ‘“(ğ‘§ğ‘§)= 

 1 

 âˆš 

 2ğœ‹ğœ‹ 

#### ğ‘’ğ‘’ 

 âˆ’ 

 1 

 2 

 ğ‘§ğ‘§ 

 2 

 (integrate using polar coordinates or trig substitutions) 

 b. ğ¸ğ¸(ğ‘ğ‘)= âˆ« 

#### ğ‘§ğ‘§ 

 1 

 âˆš 2 ğœ‹ğœ‹ 

#### ğ‘’ğ‘’ 

 âˆ’ 

 1 

 2 

 ğ‘§ğ‘§ 

 2 

#### ğ‘‘ğ‘‘ğ‘§ğ‘§ 

 âˆ 

 âˆ’âˆ 

 =â‹¯= 0 (u substitution) 

 c. ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘ğ‘ 

#### ) 

#### =âˆ« ğ‘§ğ‘§ 

 2 

 1 

 âˆš 

 2 ğœ‹ğœ‹ 

#### ğ‘’ğ‘’ 

 âˆ’ 

 1 

 2 

 ğ‘§ğ‘§ 

 2 

#### ğ‘‘ğ‘‘ğ‘§ğ‘§ 

 âˆ 

 âˆ’âˆ 

#### âˆ’ 0 

 2 

 =â‹¯= 1 (integration by parts) 

 d. Practice reading Table A 

 i. Excel: NORM.S.DIST(x, cdf?) or NORM.S.INV(percentile) 

 ii. ğ‘ƒğ‘ƒ(âˆ’1 <ğ‘‹ğ‘‹< 1 )â‰ˆ.68 

 iii. ğ‘ƒğ‘ƒ(âˆ’2 <ğ‘‹ğ‘‹< 2 )â‰ˆ.95 

 iv. ğ‘ƒğ‘ƒ(âˆ’3 <ğ‘‹ğ‘‹< 3 )â‰ˆ.997 

 e. Symmetric: ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘‹ğ‘‹<âˆ’ 3 

#### ) 

#### =ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹>3) 

## L16 Normal, Chi Square, t Distributions (WMS 4.5-4. 6) 

1. Standardization (for later reference) 

 a. If ğ¸ğ¸(ğ‘‹ğ‘‹)=ğœ‡ğœ‡ and ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)=ğœğœ 

 2 

 then you can always change units to create a new random 

 variable ğ‘ğ‘= 

 ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡ 

 ğœğœ 

 such that ğ¸ğ¸ 

#### ( 

#### ğ‘ğ‘ 

#### ) 

 = 0 and ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘ğ‘ 

#### ) 

#### = 1 

 i. ğ¸ğ¸(ğ‘ğ‘)=ğ¸ğ¸ï¿½ 

 1 

 ğœğœ 

#### (ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡)ï¿½= 

 1 

 ğœğœ 

#### [ 

#### ğ¸ğ¸(ğ‘‹ğ‘‹)âˆ’ğœ‡ğœ‡ 

#### ] 

#### = 0 

 ii. ğ‘‰ğ‘‰(ğ‘ğ‘)=ğ‘‰ğ‘‰ï¿½ 

 1 

 ğœğœ 

#### ğ‘‹ğ‘‹âˆ’ 

 1 

 ğœğœ 

#### ğœ‡ğœ‡ï¿½=ğ‘‰ğ‘‰ï¿½ 

 1 

 ğœğœ 

#### ğ‘‹ğ‘‹ï¿½= 

 1 

 ğœğœ 

 2 

#### ğ‘‰ğ‘‰(ğ‘‹ğ‘‹)= 1 

2. Normal (or Gaussian, after German mathematician Carl Friedrich Gauss, 1809) ğ‘ğ‘(ğœ‡ğœ‡,ğœğœ 

 2 

#### ) 

 a. ğ‘“ğ‘“(ğ‘¥ğ‘¥)= 

 1 

 ğœğœâˆš2ğœ‹ğœ‹ 

#### ğ‘’ğ‘’ 

 âˆ’ 

 1 

 2 

 ï¿½ 

 ğ‘¥ğ‘¥âˆ’ğœ‡ğœ‡ 

 ğœğœ 

 ï¿½ 

 2 

 (integrate using polar coordinates or trig substitutions) 

 b. ğ¸ğ¸(ğ‘‹ğ‘‹)= âˆ« 

#### ğ‘¥ğ‘¥ 

 1 

 ğœğœ âˆš 

 2 ğœ‹ğœ‹ 

#### ğ‘’ğ‘’ 

 âˆ’ 

 1 

 2 

 ï¿½ 

 ğ‘¥ğ‘¥âˆ’ğœ‡ğœ‡ 

 ğœğœ 

 ï¿½ 

 2 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 âˆ 

 âˆ’âˆ 

 =â‹¯=ğœ‡ğœ‡ (u substitution) 

 c. ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ) 

#### =âˆ« ğ‘¥ğ‘¥ 

 2 

 1 

 ğœğœ âˆš 

 2ğœ‹ğœ‹ 

#### ğ‘’ğ‘’ 

 âˆ’ 

 1 

 2 

 ï¿½ 

 ğ‘¥ğ‘¥âˆ’ğœ‡ğœ‡ 

 ğœğœ 

 ï¿½ 

 2 

#### ğ‘‘ğ‘‘ğ‘¥ğ‘¥ 

 âˆ 

 âˆ’âˆ 

#### âˆ’ğœ‡ğœ‡ 

 2 

#### =â‹¯=ğœğœ 

 2 

 (integration by parts) 

 d. No analytical cdf; instead, approximate numerically 

 i. Excel: NORM.DIST(x, mu, sd, cdf?) 

 ii. Percentiles: NORM.INV(percentile, mu, sd) 

 e. Special Properties 

 i. ğ‘ğ‘+ 7 âˆ¼ğ‘ğ‘ 

 In other words, adding a constant changes the precise distribution of ğ‘‹ğ‘‹ but 

 keeps it in the normal family 

---

1. Note: this is true of some other families of random variables (e.g. 

 uniform) but not all (e.g. Bernoulli, binomial, exponential) 

 ii. 3 ğ‘ğ‘ âˆ¼ğ‘ğ‘ 

 In other words, multiplying by a constant keeps ğ‘‹ğ‘‹ in the normal family 

1. Note: this is true of some other families of random variables (e.g. 

 uniform, exponential) but not all (e.g. Bernoulli, binomial) 

 iii. ğ‘ğ‘+ğ‘ğ‘âˆ¼ğ‘ğ‘ 

 That is, if ğ‘‹ğ‘‹ âˆ¼ğ‘ğ‘ 

#### ( 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ,ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### ) 

 and ğ‘Œğ‘Œâˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### ,ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### ï¿½ 

 then ğ‘‹ğ‘‹+ğ‘Œğ‘Œâˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### +ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### ,ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### +ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### + 2 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### ï¿½ 

 In other words, the sum of two normally distributed random variables is a 

 normally distributed random variable 

1. Note: this is true of some other families of random variables (e.g. 

 independent binomials), but not all (e.g. Bernoulli, correlated binomials, 

 uniform, exponential) 

3. Standard normal ğ‘ğ‘(0, 1) 

 a. Practice reading Table A 

 i. Excel: NORM.S.DIST(x, cdf?) or NORM.S.INV(percentile) 

 ii. ğ‘ƒğ‘ƒ 

#### ( 

#### âˆ’1 <ğ‘‹ğ‘‹< 1 

#### ) 

#### â‰ˆ.68 

 iii. ğ‘ƒğ‘ƒ(âˆ’2 <ğ‘‹ğ‘‹< 2 )â‰ˆ.95 

 iv. ğ‘ƒğ‘ƒ(âˆ’3 <ğ‘‹ğ‘‹< 3 )â‰ˆ.997 

 b. Symmetric: ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹<âˆ’ 3 )=ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹>3) 

 c. Standardized normal ğ‘ğ‘= 

 ğ‘‹ğ‘‹âˆ’ğœ‡ğœ‡ 

 ğœğœ 

 is standard normal âˆ¼ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 (because of special 

 properties of normal ğ‘‹ğ‘‹) 

 d. Example 1: ğ‘‹ğ‘‹ âˆ¼ğ‘ğ‘( 75 ,25) to find ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹> 80 )=ğ‘ƒğ‘ƒï¿½ğ‘ğ‘> 

 80 âˆ’75 

 âˆš 25 

#### ï¿½ 

#### =ğ‘ƒğ‘ƒ(ğ‘ğ‘> 1 )= 1 âˆ’ğ‘ƒğ‘ƒ(ğ‘ğ‘â‰¤ 1 ) 

#### â‰ˆ 1 âˆ’.8413=.1587 

 e. Example 2: costs ğ¶ğ¶ âˆ¼ğ‘ğ‘(120,100) 

 i. Budget ğ‘ğ‘ so that ğ‘ƒğ‘ƒ(ğ¶ğ¶<ğ‘ğ‘)=.9 

 ii.. 90=ğ‘ƒğ‘ƒ(ğ¶ğ¶<ğ‘ğ‘)=ğ‘ƒğ‘ƒï¿½ğ‘ğ‘< 

 ğ‘ğ‘âˆ’120 

 10 

 ï¿½â‰ˆğ‘ƒğ‘ƒ(ğ‘ğ‘<1. 28) (from Table A) 

 iii. If 

 ğ‘ğ‘âˆ’120 

 10 

 â‰ˆ1. 28 then ğ‘ğ‘â‰ˆ 132 .8 

---

 f. Example 3: costs ğ¶ğ¶ âˆ¼ğ‘ğ‘(120,100)) and revenue ğ‘…ğ‘…âˆ¼ğ‘ğ‘(150,400) are independent; 

 how often are profits ğ‘Œğ‘Œ=ğ‘…ğ‘…âˆ’ğ¶ğ¶ positive? 

 i. Yâˆ¼ğ‘ğ‘ 

 ii. E(Y)=E(R)âˆ’E(C)= 150 âˆ’ 120 = 30 

 iii. ğ‘‰ğ‘‰ 

#### ( 

#### Y 

#### ) 

#### =ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘…ğ‘…âˆ’ğ¶ğ¶ 

#### ) 

#### =ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘…ğ‘… 

#### ) 

#### + 

#### ( 

#### âˆ’ 1 

#### ) 

 2 

#### ğ‘‰ğ‘‰ 

#### ( 

#### ğ¶ğ¶ 

#### ) 

#### + 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘ 

#### ( 

#### ğ‘…ğ‘…,ğ¶ğ¶ 

#### ) 

#### = 400 + 100 = 500 

 iv. So ğ‘Œğ‘Œâˆ¼ğ‘ğ‘( 30 ,500) 

 v. ğ‘ƒğ‘ƒ 

#### ( 

#### Y > 0 

#### ) 

#### =ğ‘ƒğ‘ƒï¿½ğ‘ğ‘> 

 0âˆ’30 

 âˆš 

 500 

#### ï¿½â‰ˆğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ğ‘>âˆ’1. 34 

#### ) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘ğ‘<1. 34 

#### ) 

#### â‰ˆ.9099 

#### 4. ğ‘Šğ‘Š âˆ¼ğœ’ğœ’ 

 2 

 (ğœˆğœˆ) (German statistican Friedrich Robert Helmert, 1875) 

 a. Domain is [0,âˆ), roughly bell-shaped (but asymmetric, unlike Normal distribution) 

 b. ğœˆğœˆ is often called â€œdegrees of freedomâ€, because in the most common application, it 

 corresponds to how many 

 c. ğ¸ğ¸ 

#### ( 

#### ğ‘Šğ‘Š 

#### ) 

 =ğœˆğœˆ and ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘Šğ‘Š 

#### ) 

#### = 2 ğœˆğœˆ 

 d. ğ‘“ğ‘“ 

#### ( 

#### ğ‘¤ğ‘¤ 

#### ) 

 =ğ‘¢ğ‘¢ğ‘”ğ‘”ğ‘™ğ‘™ğ‘¥ğ‘¥ (I wonâ€™t expect you to know or use) 

 e. CDF ğ¹ğ¹ 

#### ( 

#### ğ‘¤ğ‘¤ 

#### ) 

 approximated on Table 6 

 i. ğœ’ğœ’ 

 ğ›¼ğ›¼ 

 2 

 represents a realization of the random variable, where ğ›¼ğ›¼ is the probability to 

 the right of that value (i.e., 1 âˆ’ğ¹ğ¹(ğ‘¤ğ‘¤)) 

 ii. Example: suppose sales follow Chi-square distribution, with average of 30 units 

1. Degrees of freedom ğœˆğœˆ= 30 

#### 2. 10 

 th 

 percentile is ğœ’ğœ’ 

. 90 

 2 

#### â‰ˆ 20 .6, 90 

 th 

 percentile is ğœ’ğœ’ 

. 10 

 2 

#### â‰ˆ 40 .3 

3. Putting these together, ğ‘ƒğ‘ƒ( 20 .6 <ğ‘Šğ‘Š< 40 .3)â‰ˆ.8 

 iii. Note: Table 6 only gives 10 points on the cdf. With a computer, you can get the 

 rest. Excel: CHISQ.DIST(x,df, cdf?), CHISQ.INV(percentile, df) 

 f. Facts 

 i. If ğ‘ğ‘âˆ¼ğ‘ğ‘(0, 1) then ğ‘ğ‘ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (1) 

 ii. If ğ‘Šğ‘Š 

 1 

#### âˆ¼ğœ’ğœ’ 

 2 

 (4) and ğ‘Šğ‘Š 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

 ( 7 ) independent then ğ‘Šğ‘Š 

 1 

#### +ğ‘Šğ‘Š 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (11) 

 iii. Variance is a quadratic function of a random variable, so when we estimate the 

 variance of a random variable that has a normal distribution (in lecture L19), our 

 estimates will follow a ğœ’ğœ’ 

 2 

 distribution. 

5. ğ‘¡ğ‘¡ distribution (Friedrich Robert Helmert 1876, Karl Pearson 1900) 

 a. ğ‘‡ğ‘‡âˆ¼ğ‘¡ğ‘¡(ğœˆğœˆ); as in Chi-square distribution, ğœˆğœˆ is called â€œdegrees of freedomâ€ 

 b. Similar to standard normal, but with higher variance (i.e. thicker tails) 

 c. Approaches ğ‘ğ‘(0, 1) as ğœˆğœˆâ†’âˆ 

---

 d. ğ‘“ğ‘“(ğ‘¡ğ‘¡)=ğ‘¢ğ‘¢ğ‘”ğ‘”ğ‘™ğ‘™ğ‘¥ğ‘¥ (I wonâ€™t expect you to know or use) 

 e. ğ¸ğ¸(ğ‘‡ğ‘‡)= 0 , ğ‘‰ğ‘‰(ğ‘‡ğ‘‡)= 

 ğœˆğœˆ 

 ğœˆğœˆâˆ’2 

#### â†’ 1 

 f. CDF ğ¹ğ¹(ğ‘¡ğ‘¡) approximated on Table C 

 i. Table is oriented so that probability ğ¶ğ¶ lies between âˆ’ğ‘¡ğ‘¡ 

 âˆ— 

 and ğ‘¡ğ‘¡ 

 âˆ— 

#### . 

 ii. Example: if ğ‘‡ğ‘‡âˆ¼ğ‘¡ğ‘¡( 20 ) find 90 

 th 

 percentile 

1. Following ğ¶ğ¶=80% (fifth column) for ğ‘‘ğ‘‘ğ‘“ğ‘“= 20 leads to ğ‘¡ğ‘¡ 

 âˆ— 

#### =1. 325. 

2. In other words, 10% of the distribution is left of âˆ’1. 325, 80% is 

 between âˆ’1. 325 and 1. 325, and 10% is above 1. 325. 

3. Since 10%+80%=90% of the distribution is below 1. 325 and 10% 

 is above, 1. 325 is the 90 

 th 

 percentile of the distribution. 

4. Alternatively, can come up from a one-sided p-value of. 10 or a two- 

 sided p-value of. 20 (bottom of the table) to reach the same conclusion. 

 iii. For degrees of freedom greater than 1000 , can read ğ‘§ğ‘§ 

 âˆ— 

 row of the table, which 

 corresponds to a standard normal distribution (i.e., âˆ degrees of freedom). 

 iv. Note: Table C only gives 12 points on CDF. With a computer, you can get the 

 rest. Excel: T.DIST(x, df, cdf?) and T.INV(percentile, df) 

 g. Fact 

 i. If ğ‘ğ‘âˆ¼ğ‘ğ‘(0, 1) and ğ‘Šğ‘Š âˆ¼ğœ’ğœ’ 

 2 

 (ğœˆğœˆ) independent then 

 ğ‘ğ‘ 

 ï¿½ 

 ğ‘Šğ‘Š 

 ğœˆğœˆ 

#### âˆ¼ğ‘¡ğ‘¡(ğœˆğœˆ) 

 ii. If we knew the population variance, then estimates of the mean would follow a 

 normal distribution. Since we have to estimate the population variance, and 

 estimates follow a ğœ’ğœ’ 

 2 

 distribution, our estimates of the mean follow a ğ‘¡ğ‘¡ 

 distribution 

6. Other distributions 

 a. The distributions weâ€™ve gone over are some of the most common; there are many 

 others, with various shapes, properties, and uses. 

 b. Illustrated: https://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm 

 c. Discrete 

 i. Uniform 

 ii. Binomial 

 iii. Geometric 

 iv. Poisson 

---

 v. Hypergeometric 

 d. Continuous 

 i. Exponential 

 ii. F 

 iii. Beta 

 iv. Gamma 

 v. Log-normal 

 vi. Pareto 

 vii. Weibull 

## L17 Confidence Intervals (WMS 7.2-3,8.5-9) 

Project note 

1. If you didnâ€™t turn the project in on time, get it in ASAP! Items from part 2 of the project will 

 show up on homework; if you do them with your homework, your project will be finished by the 

 end of the semester. 

2. From now on, must start on project as part of homework 

3. Keep results, to submit as project 

4. Note: If you have a population instead of a sample from a population (e.g. all 50 states), just 

 pretend this is a sample from a larger population (i.e. 50 draws from a population of thousands 

 of U.S. states). 

Samples 

1. Population vs. sample 

 a. So far, our discussion of distributions has presumed an entire population. Often, 

 information is only available from a sample. 

 i. Surveys are costly, populations are often huge 

 ii. Some of you might have whole populations (e.g. all 50 states, all teams, every 

 week of a companyâ€™s sales data); for projects, pretend sample even if you 

 actually have population. But be careful: 

1. Sometimes population of interest includes future generations (e.g. NBA 

 rookies, stock returns). 

---

2. Similarly, population of things that actually happened can in some cases 

 be viewed as a sample from the larger set of things that potentially 

 could have occurred instead. 

 b. Unless entire population is observed, canâ€™t know what is true, only what is probably true 

2. Random sample 

 a. i.i.d. (Independently and Identically Distributed): survey answers are independent from 

 each other, and identical to population of interest 

 b. Convenience survey (e.g. urban survey of wages): expand sample or limit scope of 

 inference 

 c. Selection bias (e.g. survey participation, program participation): administrative records, 

 measurements before participation decided, interpret results narrowly (e.g. benefit of 

 college for those who chose to attend) 

 d. Time trends (e.g. daily/weekly sales) â€“ rare â€œspot checkâ€ observations, econometric 

 corrections 

3. Estimation 

 a. Example: Suppose we wish to estimate the average family size ğœ‡ğœ‡ of BYU students, along 

 with the standard deviation ğœğœ and the correlation ğœŒğœŒ between family size and GPA. What 

 pieces of data should be used, and how should they be combined? 

 b. Population parameter ğœƒğœƒ (i.e. generic proxy for ğœ‡ğœ‡,ğœğœ,ğ‘ğ‘,ğ‘ğ‘,ğœŒğœŒ,ğ›½ğ›½,ğ‘ğ‘, etc.), seek â€œestimatorâ€ 

 function ğœƒğœƒ 

#### ï¿½ 

#### (ğ‘¥ğ‘¥ 

 1 

#### ,ğ‘¥ğ‘¥ 

 2 

#### , ...,ğ‘¥ğ‘¥ 

 ğ‘›ğ‘› 

 ) (commonly denoted by â€œhatâ€ variable) 

 i. Evaluating this â€œestimatorâ€ function with our data provides point estimates ; 

 next two lectures weâ€™ll talk about interval estimates, or margin of error 

 c. An estimator is a tool for producing estimates. Weâ€™ll spend most of the semester talking 

 about a variety of such tools (i.e. estimators for different parameters) but first we need 

 some tool-building tools (i.e. techniques for developing estimators in new settings of 

 interest). 

Estimators vs. estimates 

1. Example: suppose distribution of income among last yearâ€™s 8, 500 BYU graduates has mean 

#### ğ¸ğ¸ 

#### ( 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

 =ğœ‡ğœ‡=$48ğ‘˜ğ‘˜ and standard deviation ï¿½ğ‘‰ğ‘‰ 

#### ( 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

#### =ğœğœ=$13ğ‘˜ğ‘˜ 

---

 But we canâ€™t observe this, so we survey ğ‘›ğ‘›= 25 graduates and estimate ğœ‡ğœ‡Ì‚= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 =ğ‘¥ğ‘¥Ì… and 

#### ğœğœ 

#### ï¿½ 2 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ« 

#### ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

2. Sampling distribution 

 a. Every survey of 25 students yields different estimates ï¿½ğœ‡ğœ‡Ì‚,ğœğœ 

 2 ï¿½ 

 ï¿½. Sampling with 

 replacement, there are 8, 500 

 25 

#### â‰ˆ 10 

 98 

 such samples. 

 (Sampling without replacement is more common in practice, and violates i.i.d. but only 

 slightly, as long as population size is large.) 

 b. Before we conduct interviews, survey responses ğ‘‹ğ‘‹ 

 1 

#### ,ğ‘‹ğ‘‹ 

 2 

#### , ...,ğ‘‹ğ‘‹ 

 ğ‘›ğ‘› 

 can be viewed as random 

 variables, each drawn from the population of BYU grads 

3. Estimates and estimators 

 a. Once we conduct survey, ğœƒğœƒ 

#### ï¿½ 

#### ( 

#### ğ‘¥ğ‘¥ 

 1 

#### ,ğ‘¥ğ‘¥ 

 2 

#### , ...,ğ‘¥ğ‘¥ 

 ğ‘›ğ‘› 

#### ) 

 provides estimate of parameter ğœƒğœƒ. Before we 

 conduct survey, estimator ğœƒğœƒ 

#### ï¿½ 

#### ( 

#### ğ‘‹ğ‘‹ 

 1 

#### ,ğ‘‹ğ‘‹ 

 2 

#### , ...,ğ‘‹ğ‘‹ 

 ğ‘›ğ‘› 

#### ) 

 is random. 

 b. To evaluate estimation procedure, we must think about entire distribution of estimates 

 (in other words, evaluate estimator), not individual estimate. 

 c. Therefore, ğœ‡ğœ‡Ì‚= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =ğ‘‹ğ‘‹ 

#### ï¿½ 

 is random variable with mean ğœ‡ğœ‡ 

 ğœ‡ğœ‡ï¿½ 

 and variance ğœğœ 

 ğœ‡ğœ‡ï¿½ 

 2 

 d. Similarly, ğœğœ 

#### ï¿½ 2 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ« 

#### (ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### ) 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 is random variable with mean ğœ‡ğœ‡ 

 ğœğœ 

 ï¿½ 2 

 and variance ğœğœ 

 ğœğœ 

 ï¿½ 2 

 2 

Margin of error 

1. Recall that 

#### ğœ‡ğœ‡ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### =ğ¸ğ¸ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½=â‹¯=ğœ‡ğœ‡ 

#### ğœğœ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

 2 

#### =ğ‘‰ğ‘‰ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½=â‹¯= 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

2. Previous estimates are _point_ estimates; _margin of error_ (e.g. Â±$20ğ‘˜ğ‘˜) measures precision, gives 

 interval estimate 

3. Example 

 a. Income ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 of 8,500 BYU graduates has unknown mean ğœ‡ğœ‡ and known standard deviation 

#### ğœğœ=$13ğ‘˜ğ‘˜. 

 b. If ğ‘›ğ‘›= 25 then ğ‘‹ğ‘‹ 

#### ï¿½ 

 has same mean ğœ‡ğœ‡, and standard error ğœğœ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### = 

#### ï¿½ 

 ($13 ğ‘˜ğ‘˜) 

 2 

 25 

#### =$2.6ğ‘˜ğ‘˜ 

 c. Rule of thumb: ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 typically within ğœ‡ğœ‡Â± 2 ğœğœ, ğ‘‹ğ‘‹ 

#### ï¿½ 

 typically within ğœ‡ğœ‡Â± 2 ğœğœ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

 =ğœ‡ğœ‡Â±$5.2ğ‘˜ğ‘˜; thus, 

 $5.2ğ‘˜ğ‘˜ is â€œmargin of errorâ€ 

---

 d. Dog and leash principle: 3 ft. leash keeps dog within 3 ft. of owner; symmetrically keeps 

 owner within 3 ft. of dog 

 e. Observe ğ‘¥ğ‘¥Ì…=$47.1ğ‘˜ğ‘˜ 

 Maybe ğœ‡ğœ‡ as low as $41.9ğ‘˜ğ‘˜ and we overestimated 

 Maybe ğœ‡ğœ‡ as high as $52.3ğ‘˜ğ‘˜ and we underestimated. 

4. If ğœğœ unknown, can estimate margin of error using 

#### ï¿½ 

 ğ‘œğ‘œ 

 2 

 100 

Confidence Intervals 

1. How often is ğ‘‹ğ‘‹ 

#### ï¿½ 

 in interval ğœ‡ğœ‡Â± 2 ğœğœ? To compute probability, we need the cdf of ğ‘‹ğ‘‹ 

#### ï¿½ 

#### . 

2. Normality of ğ‘‹ğ‘‹ 

#### ï¿½ 

 a. If population distribution of ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 is normal then ğ‘‹ğ‘‹ 

#### ï¿½ 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 is normal too. Specifically, 

#### ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡, 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### ï¿½. 

 b. Standardizing, 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘ğ‘(0, 1). 

Confidence interval for ğœ‡ğœ‡ 

1. Construction 

 a. We want Pr 

#### ( 

#### #<ğœ‡ğœ‡<# 

#### ) 

 =.90 and from Table A we know ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡, 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### ï¿½= 

#### ğ‘ğ‘(ğœ‡ğœ‡, $2.6ğ‘˜ğ‘˜ 

 2 

 ) (still assuming ğœğœ=$13ğ‘˜ğ‘˜ and ğ‘›ğ‘›= 25 ) 

#### . 90=ğ‘ƒğ‘ƒï¿½âˆ’1. 64< 

 ğ‘‹ğ‘‹ 

 ï¿½ 

 âˆ’ğœ‡ğœ‡ 

 $2.6ğ‘˜ğ‘˜ 

#### <1. 64ï¿½ 

#### =ğ‘ƒğ‘ƒ(âˆ’1. 64â‹…$2.6ğ‘˜ğ‘˜<ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ’ğœ‡ğœ‡<1. 64â‹…$2.6ğ‘˜ğ‘˜) 

#### â‰ˆğ‘ƒğ‘ƒ(âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ’$4.3ğ‘˜ğ‘˜<âˆ’ğœ‡ğœ‡<âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### +$4.3ğ‘˜ğ‘˜) 

#### =ğ‘ƒğ‘ƒ 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ï¿½ 

#### +$4.3ğ‘˜ğ‘˜>ğœ‡ğœ‡>ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ’$4.3ğ‘˜ğ‘˜ 

#### ) 

 b. (Can also construct one-sided confidence intervals) 

 c. Example: ğ‘¥ğ‘¥Ì…=$47.1ğ‘˜ğ‘˜ (still assume ğœğœ=$13ğ‘˜ğ‘˜; later weâ€™ll estimate) 

 i. 90% confidence interval ğ‘¥ğ‘¥Ì…Â±1. 64ğœğœ 

 ğ‘¥ğ‘¥Ì… 

#### =$47.1ğ‘˜ğ‘˜Â±$4.3ğ‘˜ğ‘˜ 

 ii. 95% confidence interval ğ‘¥ğ‘¥Ì…Â±1. 96ğœğœ 

 ğ‘¥ğ‘¥Ì… 

#### =$47.1ğ‘˜ğ‘˜Â±$5.1ğ‘˜ğ‘˜ 

 iii. 99% confidence interval ğ‘¥ğ‘¥Ì…Â±2. 58ğœğœ 

 ğ‘¥ğ‘¥Ì… 

#### =$47.1ğ‘˜ğ‘˜Â±$6.7ğ‘˜ğ‘˜ 

2. Distribution of ğ‘†ğ‘† 

 2 

 a. If ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ¼ğ‘ğ‘ 

#### ( 

#### ğœ‡ğœ‡,ğœğœ 

 2 

#### ) 

 and ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡, 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 ï¿½ then 

#### ( 

#### ğ‘›ğ‘›âˆ’ 1 

#### ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### ( 

#### ğ‘›ğ‘›âˆ’ 1 

#### ) 

#### . 

---

 b. Intuitively, expectation of ğœ’ğœ’ 

 2 

 (ğ‘›ğ‘›âˆ’ 1 ) is ğ‘›ğ‘›âˆ’ 1 , expectation of 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

 is 1. 

3. Confidence interval for ğœ‡ğœ‡ when ğœğœ unknown 

 a. If we replace ğœğœ 

 2 

 with ğ‘’ğ‘’ 

 2 

 then 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğ‘†ğ‘† 

 2 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’1). 

 i. This is because 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğ‘†ğ‘† 

 2 

 ğ‘›ğ‘› 

#### = 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### â‹… 

 1 

 ï¿½ 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

 , which is ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 divided by 

 ğœ’ğœ’ 

 2 

 (ğ‘›ğ‘›âˆ’1) 

 ğ‘›ğ‘›âˆ’1 

 ii. Note: i f ğ‘›ğ‘› large then ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’ 1 )â‰ˆğ‘ğ‘(0, 1). 

 b. Example: average weekly income ğ‘›ğ‘›= 25 , ğ‘¥ğ‘¥Ì…=$47.1ğ‘˜ğ‘˜, ğ‘’ğ‘’=$13ğ‘˜ğ‘˜, ğœğœï¿½ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### = 

#### ï¿½ 

 ğ‘œğ‘œ 

 2 

 ğ‘›ğ‘› 

#### =$2.6ğ‘˜ğ‘˜ 

 i. 90% confidence interval ğ‘¥ğ‘¥Ì…Â±1. 726ğœğœï¿½ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### =$47.1ğ‘˜ğ‘˜Â±$4.5ğ‘˜ğ‘˜ 

 ii. 95% confidence interval ğ‘¥ğ‘¥Ì…Â±2. 093ğœğœï¿½ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### =$47.1ğ‘˜ğ‘˜Â±$5.4ğ‘˜ğ‘˜ 

 iii. 99% confidence interval ğ‘¥ğ‘¥Ì…Â±2. 861ğœğœï¿½ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### =$47.1ğ‘˜ğ‘˜Â±$7.4ğ‘˜ğ‘˜ 

Central Limit Theorem (de Moivre 1733, Laplace 1812, Lyapunov 1901) 

#### 4. 

#### âˆ‘ 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 â†’ğ‘ğ‘ (and therefore ğ‘‹ğ‘‹ 

#### ï¿½ 

 â†’ğ‘ğ‘) no matter what the distribution of ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

5. Dice example 

 a. Distribution of ğ‘‹ğ‘‹ 

 1 

#### +ğ‘‹ğ‘‹ 

 2 

 is bell-shaped, even though ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 is (discrete) uniform 

 b. Intuition: centrist values frequent (e.g. moderate ğ‘‹ğ‘‹ 

 1 

 and ğ‘‹ğ‘‹ 

 2 

 , or ğ‘‹ğ‘‹ 

 1 

 low ğ‘‹ğ‘‹ 

 2 

 high, or vice 

 versa), but extreme values rare (e.g. ğ‘‹ğ‘‹ 

 1 

 and ğ‘‹ğ‘‹ 

 2 

 both low) 

 c. ğ‘ƒğ‘ƒ(ğ‘‹ğ‘‹ 

#### ï¿½ 

 100 

#### = 1 )=ï¿½ 

 1 

 6 

#### ï¿½ 

 100 

#### â‰ˆ 10 

 âˆ’78 

 ; tails become exponentially less likely (key feature of 

 normal distribution) as ğ‘›ğ‘› increases 

6. Skewed example 

 a. Bernoulli unemployment ğ‘ƒğ‘ƒ 

#### ( 

#### 0, 1 

#### ) 

#### = 

#### ( 

#### . 7,.3 

#### ) 

 b. Average of two: ğ‘ƒğ‘ƒ 

#### ( 

#### 0,.5, 1 

#### ) 

#### â‰ˆ 

#### ( 

#### . 5,.4,.1 

#### ) 

 c. Average of four: ğ‘ƒğ‘ƒ(0,.25, .5,.75,1)â‰ˆ(.25,. 4,.25, .1, 0) 

7. CLT explains why normal distribution is so prevalent in nature: one attribute is sum total of 

 many, smaller, independent attributes 

## L18 Hypothesis Tests (WMS 10.2-8) 

1. Hypothesis test: old profit ğ‘‹ğ‘‹ âˆ¼ğ‘ğ‘($400, $100 

 2 

 ), new management; keep or fire? 

 a. Null hypothesis (benefit of doubt) ğ»ğ» 

 0 

#### :ğœ‡ğœ‡= 400 

---

 b. Alternative hypothesis (burden of proof) ğ»ğ» 

 ğ‘ğ‘ 

#### :ğœ‡ğœ‡< 400 

 c. Level ğ›¼ğ›¼=Pr 

#### ( 

#### ğ¶ğ¶ğ‘’ğ‘’ğ‘Ÿğ‘Ÿ ğ‘’ğ‘’ğ‘ğ‘ğ‘¡ğ‘¡ ğ»ğ» 

 0 

#### | 

#### ğ»ğ» 

 0 

#### ğ‘¡ğ‘¡ğ¶ğ¶ğ‘¢ğ‘¢ğ‘’ğ‘’ 

#### ) 

#### =.10 

 d. Test statistic 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘ğ‘(0, 1) 

 e. Critical value âˆ’1. 28, rejection region to left 

 f. Data: ğ‘¥ğ‘¥Ì…=$350 over 8 weeks 

 g. If ğ»ğ» 

 0 

 true, 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### = 

 350 âˆ’400 

 ï¿½100 

 2 

 / 8 

 â‰ˆâˆ’1. 41âˆˆğ‘…ğ‘…ğ‘…ğ‘…; reject ğ»ğ» 

 0 

 h. â€œSignificantly less than 400â€ (statistical vs. practical significance) 

 i. Type 1 error: convict innocent (probability ğ›¼ğ›¼) 

 j. Type 2 error: acquit guilty (probability ğ›½ğ›½) 

 k. Repeat for ğ›¼ğ›¼=.01; critical value âˆ’2. 33, fail to reject 

 l. P-value = smallest ğ›¼ğ›¼ such that (for this data) reject ğ»ğ» 

 0 

 ; 0. 0793 in this case 

 m. Practice: Reject if ğ›¼ğ›¼=.05? 

2. Variations 

 a. ğ»ğ» 

 ğ‘ğ‘ 

 :ğœ‡ğœ‡< 380 (expect and tolerate adjustment cost $20 for new); test statistic increases 

 to âˆ’.85, p-value increases to 0. 20. (At ğ›¼ğ›¼=.10 level, $350 is significantly less than 

 $400, but not significantly less than $380) 

 b. ğ»ğ» 

 ğ‘ğ‘ 

 :ğœ‡ğœ‡> 450 ; if still ğ›¼ğ›¼=.10 then critical value +1. 28; test statistic negative, so (really) 

 fail to reject 

 c. What if ğœğœ 

 2 

 unknown, and ğ‘’ğ‘’ 

 2 

#### = 100 

 2 

 instead? Use t-distribution with 7 degrees of 

 freedom; critical value if ğ›¼ğ›¼=.10 is 1. 415; reject null hypothesis. (p-value not on chart, 

 but by computer is 0. 1007) 

 d. ğ»ğ» 

 ğ‘ğ‘ 

 :ğœ‡ğœ‡â‰  400 ; critical values at Â±1. 645, now fail to reject; p-value 2 (. 079)=0. 158 

3. Relationship to confidence intervals 

 a. In two-sided ğ›¼ğ›¼=.05 level hypothesis test, reject if ï¿½ 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’400 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ï¿½ 

#### ï¿½ 

 >1. 645. In other words, if 

#### ğ‘‹ğ‘‹ 

#### ï¿½ 

 more than 1. 645 ğœğœ 

 ğ‘¥ğ‘¥Ì… 

 units from 400. 

 b. Two-sided 95% confidence interval consists of ğ‘‹ğ‘‹ 

#### ï¿½ 

#### Â±1. 645ğœğœ 

 ğ‘¥ğ‘¥Ì… 

 c. In other words,. 05 level hypothesis test merely asks whether 400 lies inside the 95% 

 confidence interval. 

---

## L19 Bias and Consistency (WMS 7.2-7.4, 9.1-9.3) 

[What if you used median to estimate mean, say in income distribution? Biased.] 

Properties of estimators 

1. Evaluating ğœƒğœƒ 

#### ï¿½ 

 amounts to evaluating distribution of ğœƒğœƒ 

#### ï¿½ 

#### (ğ‘‹ğ‘‹ 

 1 

#### ,ğ‘‹ğ‘‹ 

 2 

#### , ...,ğ‘‹ğ‘‹ 

 ğ‘›ğ‘› 

 ) relative to true unknown 

 value ğœƒğœƒ 

2. Though ğœƒğœƒ is unknown, we know how ğœƒğœƒ 

#### ï¿½ 

 relates to ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 and how ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 relates to ğœƒğœƒ, so can know 

 (probabilistically) how ğœƒğœƒ 

#### ï¿½ 

 relates to ğœƒğœƒ 

3. Weâ€™ll use this to evaluate estimator goodness and to define _margin of error_ / _interval estimates_ , 

 and do hypothesis test 

4. Moments of ğœ‡ğœ‡Ì‚=ğ‘‹ğ‘‹ 

#### ï¿½ 

 a. ğœ‡ğœ‡ 

 ğœ‡ğœ‡ï¿½ 

#### =ğ¸ğ¸(ğœ‡ğœ‡Ì‚)=ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

#### )=ğ¸ğ¸ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ¸ğ¸(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

 1 

 ğ‘›ğ‘› 

#### ğ‘›ğ‘›ğ¸ğ¸(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### )=ğ¸ğ¸(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### )=ğœ‡ğœ‡ 

 Thus, thought we donâ€™t know what ğœ‡ğœ‡ is, we know that average realization of ğ‘‹ğ‘‹ 

#### ï¿½ 

 and 

 average realization of ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 are same 

 b. ğœğœ 

 ğœ‡ğœ‡ï¿½ 

 2 

#### =ğ‘‰ğ‘‰(ğœ‡ğœ‡Ì‚)=ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

#### ï¿½ 

#### )=ğ‘‰ğ‘‰ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 1 

 ğ‘›ğ‘› 

 2 

#### âˆ‘ ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

 1 

 ğ‘›ğ‘› 

 2 

#### ğ‘›ğ‘›ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### )= 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 Variance of ğ‘‹ğ‘‹ 

#### ï¿½ 

 is much smaller than variance of ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 c. Standard error (i.e. standard deviation of estimator) ğœğœ 

 ğ‘‹ğ‘‹ 

 ï¿½ 

#### = 

#### ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### = 

 ğœğœ 

 âˆš 

 ğ‘›ğ‘› 

 i. In population (ğ‘›ğ‘›= 1 ), incomes typically between $48ğ‘˜ğ‘˜Â±$26ğ‘˜ğ‘˜ [$22ğ‘˜ğ‘˜, $74ğ‘˜ğ‘˜] 

 ii. For ğ‘›ğ‘›= 25 , sample average ğ‘‹ğ‘‹ 

#### ï¿½ 

 typically between $48ğ‘˜ğ‘˜Â±$5.2ğ‘˜ğ‘˜ [$43ğ‘˜ğ‘˜, $53ğ‘˜ğ‘˜] 

 iii. For ğ‘›ğ‘›= 100 , sample average ğ‘‹ğ‘‹ 

#### ï¿½ 

 typically between $48ğ‘˜ğ‘˜Â±$2.6ğ‘˜ğ‘˜ [$44ğ‘˜ğ‘˜, $51ğ‘˜ğ‘˜] 

 iv. For ğ‘›ğ‘›= 10 ,000, ğ‘‹ğ‘‹ 

#### ï¿½ 

 typically between $48ğ‘˜ğ‘˜Â±$0.26ğ‘˜ğ‘˜ [$47.7,$48.3ğ‘˜ğ‘˜] 

Consistency 

1. Best imaginable case: ğœƒğœƒ 

#### ï¿½ 

 degenerate with ğ¸ğ¸ ï¿½ 

#### ğœƒğœƒ 

#### ï¿½ 

#### ï¿½ 

 =ğœƒğœƒ and ğ‘‰ğ‘‰ ï¿½ 

#### ğœƒğœƒ 

#### ï¿½ 

#### ï¿½= 0 

2. As ğ‘›ğ‘›â†’âˆ, ğœƒğœƒ 

#### ï¿½ 

 approaches ideal distribution 

 a. That is, ğ¸ğ¸ï¿½ğœƒğœƒ 

#### ï¿½ 

 ï¿½â†’ğœƒğœƒ and ğ‘‰ğ‘‰ï¿½ğœƒğœƒ 

#### ï¿½ 

#### ï¿½â†’ 0 

 Put differently, ğœƒğœƒ 

#### ï¿½ 

 ğ‘›ğ‘› 

 â†’ğœƒğœƒ (â€œin probabilityâ€) 

3. Example: ğœ‡ğœ‡Ì‚=ğ‘‹ğ‘‹ 

#### ï¿½ 

 is consistent estimator of ğœ‡ğœ‡ 

 a. ğ¸ğ¸(ğœ‡ğœ‡Ì‚)=ğœ‡ğœ‡ for all ğ‘›ğ‘› 

 b. ğ‘‰ğ‘‰ 

#### ( 

#### ğœ‡ğœ‡Ì‚ 

#### ) 

#### = 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### â†’ 0 

---

4. Law of large numbers (Jacob Bernoulli, 1713) 

 a. Sample means converge to population means 

 b. Higher order moments 

 i. ğ¸ğ¸ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 3 

 ğ‘–ğ‘–=1 

#### ï¿½=ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 3 

#### ï¿½ 

 ii. ğ‘‰ğ‘‰ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 3 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 ğ‘‘ğ‘‘ï¿½ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 3 

 ï¿½ 

 ğ‘›ğ‘› 

#### â†’ 0 

 iii. Sample moments converge to population moments (justification for MOM) 

5. Fact: continuous functions of consistent estimators are consistent 

6. Fact: MLE are always consistent 

Bias 

1. Bias ğµğµï¿½ğœƒğœƒ 

#### ï¿½ 

#### ï¿½=ğ¸ğ¸ï¿½ğœƒğœƒ 

#### ï¿½ 

#### âˆ’ğœƒğœƒï¿½=ğ¸ğ¸ï¿½ğœƒğœƒ 

#### ï¿½ 

#### ï¿½âˆ’ğœƒğœƒ 

2. On average, does ğœƒğœƒ 

#### ï¿½ 

 produces estimates that are higher or lower than ğœƒğœƒ? 

3. Unbiased estimator: ğ¸ğ¸ï¿½ğœƒğœƒ 

#### ï¿½ 

#### ï¿½=ğœƒğœƒ 

#### 4. ğ‘‹ğ‘‹ 

#### ï¿½ 

 is unbiased estimator of ğœ‡ğœ‡ because ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

#### )=ğœ‡ğœ‡ 

5. Example of biased estimation procedure: sample max from uniform distribution 

6. When bias can be measured, can sometimes correct (target analogy) 

(Relative) Efficiency 

5. Given two estimators, the one with lower variance is more efficient. 

6. An estimator cannot be efficient, per se, but only more efficient than another estimator. In 

 some cases in Econ 388, however, it is possible to prove categorically that a particular unbiased 

 estimator is more efficient than any other unbiased estimator. 

7. Example: consider throwing out one observation, computing sample average of ğ‘›ğ‘›âˆ’ 1 

 observations 

 a. ğ¸ğ¸ 

#### ( 

#### ğœ‡ğœ‡ï¿½ 

#### ) 

 =ğœ‡ğœ‡ still 

 b. ğ‘‰ğ‘‰(ğœ‡ğœ‡ï¿½)=â‹¯= 

 ğœğœ 

 2 

 ğ‘›ğ‘›âˆ’1 

 c. Still unbiased, still consistent, but less efficient than using all available data 

Sample Variance 

#### 1. ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

 is biased 

 a. ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ (ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### ) 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

#### =â‹¯= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

 2 

---

 b. ğ¸ğ¸(ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### )= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 2 

#### ï¿½ 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

 2 

#### ) 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ 

#### (ğœ‡ğœ‡ 

 2 

#### +ğœğœ 

 2 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ï¿½ğœ‡ğœ‡ 

 2 

#### + 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### ï¿½ 

 (since ğœğœ 

 2 

#### =ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### )=ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 2 

#### ï¿½âˆ’ğœ‡ğœ‡ 

 2 

 and 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### =ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

#### ï¿½ 

#### )=ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

 2 

#### )âˆ’ğœ‡ğœ‡ 

 2 

#### ) 

#### =ğœ‡ğœ‡ 

 2 

#### +ğœğœ 

 2 

#### âˆ’ğœ‡ğœ‡ 

 2 

#### âˆ’ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### =ğœğœ 

 2 

#### âˆ’ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### = 

 ğ‘›ğ‘›âˆ’1 

 ğ‘›ğ‘› 

#### ğœğœ 

 2 

 c. ğµğµ 

#### ( 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### ) 

#### = 

 ğ‘›ğ‘›âˆ’1 

 ğ‘›ğ‘› 

#### ğœğœ 

 2 

#### âˆ’ğœğœ 

 2 

#### =âˆ’ 

 1 

 ğ‘›ğ‘› 

#### ğœğœ 

 2 

 d. Still consistent: ğµğµ 

#### ( 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### ) 

 â†’ 0 (and can show that ğ‘‰ğ‘‰ 

#### ( 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### ) 

#### â†’ 0 ) 

2. â€œSample varianceâ€ ğ‘†ğ‘† 

 2 

#### = 

 1 

 ğ‘›ğ‘›âˆ’1 

#### âˆ‘ (ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### ) 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

 a. To eliminates bias: ğ¸ğ¸ï¿½ 

 ğ‘›ğ‘› 

 ğ‘›ğ‘›âˆ’1 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### ï¿½= 

 ğ‘›ğ‘› 

 ğ‘›ğ‘›âˆ’1 

#### ğ¸ğ¸ 

#### ( 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### ) 

#### = 

 ğ‘›ğ‘› 

 ğ‘›ğ‘›âˆ’1 

 ğ‘›ğ‘›âˆ’1 

 ğ‘›ğ‘› 

#### ğœğœ 

 2 

#### =ğœğœ 

 2 

 b. So if ğ‘†ğ‘† 

 2 

#### = 

 ğ‘›ğ‘› 

 ğ‘›ğ‘›âˆ’1 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### = 

 1 

 ğ‘›ğ‘›âˆ’1 

#### âˆ‘ (ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### ) 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

 then ğµğµ(ğ‘†ğ‘† 

 2 

#### )= 0 

 i. Example: sample of ğ‘›ğ‘›= 4 student wages, ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### =$11, $10, $14, $15, ğ‘¥ğ‘¥Ì…=$13.50, 

#### ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

#### =ï¿½ 

 1 

 4 

#### (1. 5 

 2 

#### +2. 5 

 2 

#### +3. 5 

 2 

#### +. 5 

 2 

#### )=ï¿½ 

 21 

 4 

#### â‰ˆ$2.29 

#### ğ‘’ğ‘’=ï¿½ 

 1 

 3 

#### (1. 5 

 2 

#### +2. 5 

 2 

#### +3. 5 

 2 

#### +. 5 

 2 

#### )=ï¿½ 

 21 

 3 

#### â‰ˆ$2.65 

 ii. Excel: use VAR.S or STDEV.S, not =VAR.P or =STDEV.P 

 c. Correcting bias actually sacrifices some efficiency 

## L18 Difference in Means, Proportions (WMS 8.3-8,10.3) 

1. [Long lecture; use time efficiently.] 

2. Difference in means 

 a. Relating quantitative and binary variables: conditional distributions, conditional means 

#### ğ¸ğ¸(ğ‘Œğ‘Œ|ğ‘‹ğ‘‹= 0 ), ğ¸ğ¸(ğ‘Œğ‘Œ|ğ‘‹ğ‘‹= 1 ) 

 b. Wages gap between men and women: 

#### ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

#### = 40 , ğ‘¥ğ‘¥Ì… 

 ğ‘¤ğ‘¤ 

#### =$32,ğœğœ 

 ğ‘¤ğ‘¤ 

#### =$13,ğ‘›ğ‘› 

 ğ‘šğ‘š 

#### = 45 , ğ‘¥ğ‘¥Ì… 

 ğ‘šğ‘š 

#### =$35, ğœğœ 

 ğ‘šğ‘š 

#### =$15. 

 c. 95% confidence intervals for men 

#### [ 

#### $30.62, $39.38 

#### ] 

 and women 

#### [ 

#### $27.97, $36.03 

#### ] 

 overlap, making it difficult to tell true size of wage gap (if any) 

 d. Trick (used a lot in more advanced settings): combine into single parameter 

#### ğœƒğœƒ= 

#### ( 

#### ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

#### ) 

 ; MOM estimator ğœƒğœƒ 

#### ï¿½ 

#### = 

#### ( 

#### ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### ) 

 i. ğ¸ğ¸ï¿½ğœƒğœƒ 

#### ï¿½ 

#### ï¿½=ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### )=ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### )âˆ’ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### )=ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 =ğœƒğœƒ; unbiased! 

---

 ii. ğ‘‰ğ‘‰ï¿½ğœƒğœƒ 

#### ï¿½ 

#### ï¿½=ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### )= 

 ğœğœ ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› ğ‘šğ‘š 

#### +(âˆ’ 1 ) 

 2 

 ğœğœ ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› ğ‘¤ğ‘¤ 

 â†’ 0 ; consistent (as long as both sample 

 sizes grow large)! 

 iii. ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### , 

 ğœğœ 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› 

 ğ‘šğ‘š 

 ï¿½ and ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

#### , 

 ğœğœ 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

 ï¿½, so... 

 iv. ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

#### , 

 ğœğœ 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› 

 ğ‘šğ‘š 

#### + 

 ğœğœ 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

#### ï¿½ 

 Standardizing, 

 ğœƒğœƒ 

 ï¿½ 

 âˆ’ğœ‡ğœ‡ 

 ğœƒğœƒ 

 ï¿½ 

 ğœğœ 

 ğœƒğœƒ 

 ï¿½ 

#### = 

 (ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘šğ‘š 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘¤ğ‘¤ 

 )âˆ’(ğœ‡ğœ‡ 

 ğ‘šğ‘š 

 âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 ) 

 ï¿½ 

 ğœğœ 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› ğ‘šğ‘š 

 + 

 ğœğœ 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› ğ‘¤ğ‘¤ 

#### âˆ¼ğ‘ğ‘(0, 1) 

 v. Note: if estimate ğ‘’ğ‘’ 

 ğ´ğ´ 

 2 

 and ğ‘’ğ‘’ 

 ğµğµ 

 2 

 then 

 (ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘šğ‘š 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘¤ğ‘¤ 

 )âˆ’(ğœ‡ğœ‡ 

 ğ‘šğ‘š 

 âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 ) 

 ï¿½ 

 ğ‘†ğ‘† 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› ğ‘šğ‘š 

 + 

 ğ‘†ğ‘† 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› ğ‘¤ğ‘¤ 

#### âˆ¼ğ‘¡ğ‘¡ 

#### â 

#### âœ 

#### â› 

#### ğ‘‘ğ‘‘ğ‘“ğ‘“= 

 ï¿½ 

 ğ‘ ğ‘  

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› ğ‘šğ‘š 

 + 

 ğ‘ ğ‘  

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› ğ‘¤ğ‘¤ 

 ï¿½ 

 2 

 ï¿½ 

 ğ‘ ğ‘  

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› 

 ğ‘šğ‘š 

 ï¿½ 

 2 

 ğ‘›ğ‘› ğ‘šğ‘š 

 âˆ’1 

 + 

 ï¿½ 

 ğ‘ ğ‘  ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

 ï¿½ 

 2 

 ğ‘›ğ‘› ğ‘¤ğ‘¤ 

 âˆ’1 

 â  

#### âŸ 

#### â 

1. (e.g. If ğ‘’ğ‘’ 

 ğ‘šğ‘š 

 =$12 and ğ‘’ğ‘’ 

 ğ‘¤ğ‘¤ 

 =$10 then ğ‘‘ğ‘‘ğ‘“ğ‘“ â‰ˆ 83 ) 

2. For this class, just use ğ‘¡ğ‘¡(ğ‘‘ğ‘‘ğ‘“ğ‘“)â‰ˆğ‘ğ‘(0, 1), which is appropriate when ğ‘›ğ‘› 

 ğ‘šğ‘š 

 and ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

 are both large 

3. (df between minimum and sum of 

#### ( 

#### ğ‘›ğ‘› 

 ğ‘šğ‘š 

#### âˆ’ 1 

#### ) 

 and 

#### ( 

#### ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

#### âˆ’ 1 

#### ) 

#### ) 

 e. Margin of error: Â± 2ï¿½ 

 ğœğœ 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› 

 ğ‘šğ‘š 

#### + 

 ğœğœ 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

#### = 2 

#### ( 

#### $1.98 

#### ) 

#### =$3.96 

 f. 95% confidence interval for (ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

#### ): (ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘šğ‘š 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

 ğ‘¤ğ‘¤ 

#### )Â±1. 96ï¿½ 

 ğœğœ 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› 

 ğ‘šğ‘š 

#### + 

 ğœğœ 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

#### =[$0.11, $7.89] 

 g. Test ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 > 0 : test statistic 

 (ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘šğ‘š 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘¤ğ‘¤ 

 )âˆ’(ğœ‡ğœ‡ 

 ğ‘šğ‘š 

 âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 ) 

 ï¿½ 

 ğ‘†ğ‘† 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› 

 ğ‘šğ‘š 

 + 

 ğ‘†ğ‘† 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› 

 ğ‘¤ğ‘¤ 

#### = 

 4âˆ’0 

 ï¿½ 

 144 

 100 

 + 

 100 

 40 

 =2. 02; p-value 0.0 217 

 h. Test ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 â‰  0 : p-value 2 â‹…0. 0217=0. 0434 

 i. Test ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 >$2: test statistic 

 ( ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘šğ‘š 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ 

 ğ‘¤ğ‘¤ 

 ) âˆ’ 

 ( ğœ‡ğœ‡ ğ‘šğ‘š 

 âˆ’ğœ‡ğœ‡ ğ‘¤ğ‘¤ 

 ) 

 ï¿½ 

 ğ‘†ğ‘† 

 ğ‘šğ‘š 

 2 

 ğ‘›ğ‘› ğ‘šğ‘š 

 + 

 ğ‘†ğ‘† 

 ğ‘¤ğ‘¤ 

 2 

 ğ‘›ğ‘› ğ‘¤ğ‘¤ 

#### = 

 4âˆ’2 

 ï¿½ 

 144 

 100 

 + 

 100 

 40 

 =1. 01; p-value 0. 1562 

 j. Note: if we estimated ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘šğ‘š 

 instead of ğœ‡ğœ‡ 

 ğ‘šğ‘š 

#### âˆ’ğœ‡ğœ‡ 

 ğ‘¤ğ‘¤ 

 , rejection region would be on left 

 instead of right, and test statistics would be negative instead of positive, but produce 

 same p-values 

3. Binary data (i.e. Bernoulli(ğ‘ğ‘)) 

 a. Intuitive estimator: proportion ğ‘ğ‘Ì‚= 

 ğ‘Œğ‘Œ 

 ğ‘›ğ‘› 

 , where ğ‘Œğ‘Œ = # of 1â€™s in data 

 i. Example: election survey, ğ‘›ğ‘›= 100 , ğ‘ğ‘Ì‚= 

 52 

 100 

#### =.52 

---

 b. MOM estimator: ğ‘ğ‘Ì‚ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

#### =ğ‘‹ğ‘‹ 

#### ï¿½ 

 ; actually same, since ğ‘Œğ‘Œ= 

#### âˆ‘ 

#### ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 (for zeros and ones, 

 adding is the same as counting) 

 c. Since ğ‘Œğ‘Œâˆ¼ğµğµğ‘–ğ‘–ğ‘›ğ‘›(ğ‘›ğ‘›,ğ‘ğ‘), 

#### ğ¸ğ¸ 

#### ( 

#### ğ‘ğ‘Ì‚ 

#### ) 

#### =ğ¸ğ¸ï¿½ 

 ğ‘Œğ‘Œ 

 ğ‘›ğ‘› 

#### ï¿½= 

 1 

 ğ‘›ğ‘› 

#### ğ¸ğ¸ 

#### ( 

#### ğ‘Œğ‘Œ 

#### ) 

#### = 

 ğ‘›ğ‘›ğ‘›ğ‘› 

 ğ‘›ğ‘› 

 =ğ‘ğ‘; unbiased! 

#### ğ‘‰ğ‘‰(ğ‘ğ‘Ì‚)= 

 1 

 ğ‘›ğ‘› 

 2 

#### ğ‘‰ğ‘‰(ğ‘Œğ‘Œ)= 

 ğ‘›ğ‘›ğ‘›ğ‘› 

 ( 1âˆ’ğ‘›ğ‘› 

 ) 

 ğ‘›ğ‘› 

 2 

#### = 

 ğ‘›ğ‘› 

 ( 1âˆ’ğ‘›ğ‘› 

 ) 

 ğ‘›ğ‘› 

 â†’ 0 ; consistent! 

 d. By Central Limit Theorem, ğ‘ğ‘Ì‚=ğ‘¥ğ‘¥Ì… â†’ğ‘ğ‘ï¿½ğ‘ğ‘, 

 ğ‘›ğ‘›(1âˆ’ğ‘›ğ‘›) 

 ğ‘›ğ‘› 

#### ï¿½â‡’ 

 ğ‘›ğ‘›ï¿½âˆ’ğ‘›ğ‘› 

 ï¿½ 

 ğ‘ğ‘ 

 ( 1âˆ’ğ‘ğ‘ 

 ) 

 ğ‘›ğ‘› 

#### â†’ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 Note: this is not actually different from 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 ; just special case with ğ‘¥ğ‘¥Ì…=ğ‘ğ‘Ì‚, ğœ‡ğœ‡=ğ‘ğ‘, and 

#### ğœğœ 

 2 

#### =ğ‘ğ‘(1âˆ’ğ‘ğ‘) 

 Note: does not follow ğ‘¡ğ‘¡ distribution for small ğ‘›ğ‘›, because ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 not normal 

 e. Example: election survey, ğ‘›ğ‘›= 100 , ğ‘ğ‘Ì‚= 

 52 

 100 

#### =.52 

 i. Margin of error: 2 

#### ï¿½ 

 ğ‘›ğ‘› 

 ( 1âˆ’ğ‘›ğ‘› 

 ) 

 ğ‘›ğ‘› 

#### â‰ˆ 2 

#### ï¿½ 

 ğ‘›ğ‘› 

 ï¿½( 1âˆ’ğ‘›ğ‘› 

 ï¿½) 

 ğ‘›ğ‘› 

#### = 2 ï¿½ 

 .52 â‹…. 48 

 100 

#### â‰ˆ 2 (. 05)=0. 1 

 ii. 95% Confidence interval â‰ˆğ‘ğ‘Ì‚Â±1. 96 

#### ï¿½ 

 ğ‘›ğ‘› 

 ï¿½( 1âˆ’ğ‘›ğ‘› 

 ï¿½) 

 ğ‘›ğ‘› 

#### =.52Â±1. 96(. 05)=[.422, .618] 

 iii. Test ğ»ğ» 

 0 

 :ğ‘ğ‘= 5 against ğ»ğ» 

 ğ‘ğ‘ 

 :ğ‘ğ‘> 5 : test statistic 

 ğ‘›ğ‘›ï¿½âˆ’ğ‘›ğ‘› 

 ï¿½ 

 ğ‘ğ‘ 

 ( 1âˆ’ğ‘ğ‘ 

 ) 

 ğ‘›ğ‘› 

#### = 

 .52 âˆ’. 5 

 ï¿½ 

 .5â‹…. 5 

 100 

 =0. 40; p-value 

#### 0.3446 

4. Difference in proportions: unemployment in U.S. and France (2% difference?) 

 a. ğ‘›ğ‘› 

 ğ¹ğ¹ 

#### = 1000 , ğ‘ğ‘Ì‚= 

 109 

 1000 

#### =.109; ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### = 500 , ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### = 

 38 

 500 

#### =.076 

 b. 95% confidence intervals 

#### [ 

#### . 090, .128 

#### ] 

#### [.053, .099] 

 c. Estimate (ğ‘ğ‘ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ) with MOM estimator (ğ‘ğ‘Ì‚ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### ) 

 i. ğ¸ğ¸(ğ‘ğ‘Ì‚ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### )=ğ¸ğ¸(ğ‘ğ‘Ì‚ 

 ğ¹ğ¹ 

#### )âˆ’ğ¸ğ¸(ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### )=ğ‘ğ‘ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ; unbiased! 

 ii. ğ‘‰ğ‘‰(ğ‘ğ‘Ì‚ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### )=ğ‘‰ğ‘‰(ğ‘ğ‘Ì‚ 

 ğ¹ğ¹ 

#### )+ğ‘‰ğ‘‰(ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### )= 

 ğ‘›ğ‘› 

 ğ¹ğ¹ 

 (1âˆ’ğ‘›ğ‘› 

 ğ¹ğ¹ 

 ) 

 ğ‘›ğ‘› 

 ğ¹ğ¹ 

#### + 

 ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 (1âˆ’ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ) 

 ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 â†’ 0 ; consistent! 

 iii. 

 (ğ‘›ğ‘›ï¿½ 

 ğ¹ğ¹ 

 âˆ’ğ‘›ğ‘›ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 )âˆ’(ğ‘›ğ‘› 

 ğ¹ğ¹ 

 âˆ’ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ) 

 ï¿½ 

 ğ‘ğ‘ 

 ğ¹ğ¹ 

 ï¿½1âˆ’ğ‘ğ‘ 

 ğ¹ğ¹ 

 ï¿½ 

 ğ‘›ğ‘› 

 ğ¹ğ¹ 

 + 

 ğ‘ğ‘ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½1âˆ’ğ‘ğ‘ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½ 

 ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### âˆ¼ğ‘ğ‘(0, 1) 

 d. 95% Confidence interval (ğ‘ğ‘Ì‚ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘Ì‚ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### )Â±1. 96 

#### ï¿½ 

#### ï¿½ 

 ğ‘›ğ‘›ï¿½ 

 ğ¹ğ¹ 

 (1âˆ’ğ‘›ğ‘›ï¿½ 

 ğ¹ğ¹ 

 ) 

 ğ‘›ğ‘› 

 ğ¹ğ¹ 

#### + 

 ğ‘›ğ‘›ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 (1âˆ’ğ‘›ğ‘›ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ) 

 ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

#### ï¿½=[.003, .063] 

 e. Test (ğ‘ğ‘ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 )> 0 , test statistic 

 ( ğ‘›ğ‘› 

 ï¿½ 

 ğ¹ğ¹ 

 âˆ’ğ‘›ğ‘› 

 ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ) âˆ’0 

 ï¿½ï¿½ 

 ğ‘ğ‘ï¿½ 

 ğ¹ğ¹ 

 ï¿½1âˆ’ğ‘ğ‘ï¿½ 

 ğ¹ğ¹ 

 ï¿½ 

 ğ‘›ğ‘› 

 ğ¹ğ¹ 

 + 

 ğ‘ğ‘ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½1âˆ’ğ‘ğ‘ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½ 

 ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½ 

 â‰ˆ2. 14; p-value .0162 

---

 f. Test (ğ‘ğ‘ 

 ğ¹ğ¹ 

#### âˆ’ğ‘ğ‘ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 )>.02, test statistic 

 ( ğ‘›ğ‘›ï¿½ ğ¹ğ¹ 

 âˆ’ğ‘›ğ‘›ï¿½ ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ) âˆ’. 02 

 ï¿½ï¿½ 

 ğ‘ğ‘ï¿½ 

 ğ¹ğ¹ 

 ï¿½1âˆ’ğ‘ğ‘ï¿½ 

 ğ¹ğ¹ 

 ï¿½ 

 ğ‘›ğ‘› 

 ğ¹ğ¹ 

 + 

 ğ‘ğ‘ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½1âˆ’ğ‘ğ‘ï¿½ 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½ 

 ğ‘›ğ‘› 

 ğ‘ˆğ‘ˆğ‘†ğ‘† 

 ï¿½ 

 â‰ˆ0. 84; p-value .2005 

## L19 Variance Estimation (WMS 8.9,10.9) 

Review 

#### 1. ğœğœï¿½ 

 ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ğ‘€ 

 2 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### 2. ğ‘†ğ‘† 

 2 

#### = 

 1 

 ğ‘›ğ‘›âˆ’1 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

Variance estimation 

1. Applications: inequality/heterogeneity, quality control, estimation error 

#### 2. (ğ‘›ğ‘›âˆ’ 1 ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (ğ‘›ğ‘›âˆ’ 1 ) 

3. Sample variance: (ğ‘›ğ‘›âˆ’1) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (ğ‘›ğ‘›âˆ’1) 

 a. Intuition 1: ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ¼ğ‘ğ‘(ğœ‡ğœ‡,ğœğœ 

 2 

 ), so ï¿½ 

 ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 âˆ’ğœ‡ğœ‡ 

 ğœğœ 

#### ï¿½ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (1); 

#### âˆ‘ 

#### ï¿½ 

 ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 âˆ’ğœ‡ğœ‡ 

 ğœğœ 

#### ï¿½ 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ¼ğœ’ğœ’ 

 2 

 (ğ‘›ğ‘›); we lose one 

 degree of freedom because weâ€™re measuring deviations from ğ‘‹ğ‘‹ 

#### ï¿½ 

 rather than deviations 

 from ğœ‡ğœ‡ 

 b. Intuition 2: a single observation conveys information about ğœ‡ğœ‡ but not ğœğœ 

 2 

 , so if ğ‘›ğ‘›= 100 

 then we have 100 pieces of information about ğœ‡ğœ‡ but only 99 pieces of information about 

#### ğœğœ 

 2 

 c. Intuition 3: ğ¸ğ¸(ğ‘†ğ‘† 

 2 

#### )=ğœğœ 

 2 

 , so ğ¸ğ¸ï¿½(ğ‘›ğ‘›âˆ’ 1 ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### ï¿½=ğ‘›ğ‘›âˆ’ 1 

 d. [Skip] Formal derivation: 

#### âˆ‘ 

#### ï¿½ 

 ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 âˆ’ğœ‡ğœ‡ 

 ğœğœ 

#### ï¿½ 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

#### âˆ‘ 

#### ï¿½ 

 (ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ )+(ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡) 

 ğœğœ 

#### ï¿½ 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =âˆ‘ ï¿½ 

 ğ‘‹ğ‘‹ ğ‘–ğ‘– 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ 

 ğœğœ 

#### ï¿½ 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### +âˆ‘ ï¿½ 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ğœğœ 

#### ï¿½ 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### + 2 âˆ‘ 

 ( ğ‘‹ğ‘‹ ğ‘–ğ‘– 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½)( ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ) 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =(ğ‘›ğ‘›âˆ’ 1 ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### + 

 ğ‘›ğ‘›(ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡) 

 2 

 ğœğœ 

 2 

#### + 2 

 (ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡) 

 ğœğœ 

 2 

#### âˆ‘ (ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘‹ğ‘‹ 

#### ï¿½ 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =(ğ‘›ğ‘›âˆ’ 1 ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### + 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### + 2 

 (ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡) 

 ğœğœ 

 2 

#### (ğ‘›ğ‘›ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ’ğ‘›ğ‘›ğ‘‹ğ‘‹ 

#### ï¿½ 

#### ) 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (ğ‘›ğ‘›âˆ’ 1 )+ğœ’ğœ’ 

 2 

#### ( 1 )+ 0 

---

 e. Recall that, in estimating ğœ‡ğœ‡, using 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğ‘ ğ‘  

 2 

 ğ‘›ğ‘› 

 instead of 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 required the use of a t distribution 

 instead of a normal. This is because it can be shown that ğ‘ğ‘= 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 and ğ‘Šğ‘Š= 

#### ( 

#### ğ‘›ğ‘›âˆ’ 1 

#### ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### ( 

#### ğ‘›ğ‘›âˆ’ 1 

#### ) 

 are independent, implying that 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğ‘†ğ‘† 

 2 

 ğ‘›ğ‘› 

#### = 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 1 

 ï¿½ 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### = 

 ğ‘ğ‘ 

 ï¿½ 

 ğ‘Šğ‘Š 

 ğ‘›ğ‘›âˆ’1 

#### âˆ¼ 

#### ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’ 1 ). 

4. Example: variance among ğ‘›ğ‘›= 71 sales representatives is ğ‘’ğ‘’ 

 2 

#### =5. 3 

 2 

 a. Confidence interval 

 i. Seek. 95=Pr (#<ğœğœ<#) and know (ğ‘›ğ‘›âˆ’ 1 ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

 ( 70 ), so from Table 6, 

 ii. Prï¿½ 48 .76< 

#### ( 

#### ğ‘›ğ‘›âˆ’ 1 

#### ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

 < 95 .02ï¿½=Prï¿½ 

 1 

 48. 76 

#### > 

 ğœğœ 

 2 

 (ğ‘›ğ‘›âˆ’1)ğ‘œğ‘œ 

 2 

#### > 

 1 

 95. 02 

#### ï¿½ 

 =Prï¿½ 

 (ğ‘›ğ‘›âˆ’1)ğ‘œğ‘œ 

 2 

 48. 76 

#### >ğœğœ 

 2 

#### > 

 (ğ‘›ğ‘›âˆ’1)ğ‘œğ‘œ 

 2 

 95. 02 

 ï¿½=Prï¿½ 

#### ï¿½ 

 ( 70 ) 5. 3 

 2 

 48. 76 

#### >ğœğœ> 

#### ï¿½ 

 ( 70 ) 5. 3 

 2 

 95. 02 

#### ï¿½ 

 =Pr 

#### ( 

#### 6. 35>ğœğœ>4. 55 

#### ) 

 b. Hypothesis test 

 i. ğ»ğ» 

 ğ‘ğ‘ 

#### :ğœğœ 

 2 

#### > 4 

 2 

#### , ğ›¼ğ›¼=.05 

 ii. Critical value 90 .53 

 iii. Test statistic (nâˆ’ 1 ) 

 S 

 2 

 Ïƒ 

 2 

#### = 70 ï¿½ 

 5. 3 

 2 

 4 

 2 

 ï¿½= 122 .9, reject ğ»ğ» 

 0 

 (from Excel, p-value is 

#### 10 

 âˆ’5 

#### ) 

## L20 Regression Estimation (WMS 11.1-3) 

1. Recall from Lecture 9 

 a. Relationship between ğ‘‹ğ‘‹ and ğ‘Œğ‘Œ can be represented by ğœŒğœŒ=ğ‘ğ‘ğ‘›ğ‘›ğ¶ğ¶ğ¶ğ¶(ğ‘‹ğ‘‹,ğ‘Œğ‘Œ) or by regression 

 line ğ‘Œğ‘Œ=ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘‹ğ‘‹+ğœ€ğœ€ 

 b. ğ¸ğ¸ 

#### ( 

#### ğœ€ğœ€ 

#### ) 

 = 0 can be guaranteed by choosing intercept to solve ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

 c. Crystal ball: can predict ğ‘Œğ‘Œ 

 âˆ— 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 âˆ— 

 for any ğ‘¥ğ‘¥ 

 âˆ— 

 value (even out of sample) 

 d. ğœğœ 

 ğœ€ğœ€ 

 2 

 can be minimized by choosing slope coefficient ğ›½ğ›½ 

 1 

#### = 

 ğœğœ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### =ğœŒğœŒ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 ğœğœ ğ‘¥ğ‘¥ 

 e. Fraction of variation in ğ‘Œğ‘Œ associated with ğ‘‹ğ‘‹ is 

 ğ‘‘ğ‘‘(ğ›½ğ›½ 

 0 

 +ğ›½ğ›½ 

 1 

 ğ‘‹ğ‘‹) 

 ğ‘‘ğ‘‘(ğ‘Œğ‘Œ) 

#### = 

 ğ›½ğ›½ 

 1 

 2 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

 ï¿½ğœŒğœŒ 

 2 

 ğœğœ ğ‘¥ğ‘¥ 

 2 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

 ï¿½ğœğœ ğ‘¥ğ‘¥ 

 2 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

#### =ğœŒğœŒ 

 2 

2. Estimation 

---

 a. ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

 1 

 ğ‘›ğ‘›âˆ’1 

#### âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

 b. ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### = 

 1 

 ğ‘›ğ‘›âˆ’1 

#### âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)(ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥ï¿½) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 c. ğ¶ğ¶= 

 ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğ‘œğ‘œ ğ‘¥ğ‘¥ 

 ğ‘œğ‘œ ğ‘¥ğ‘¥ 

 d. ğœŒğœŒ 

#### ï¿½ 2 

#### =ğ¶ğ¶ 

 2 

 e. ğ›½ğ›½ 

#### Ì‚ 

 1 

#### = 

 ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ 

 2 

#### =ğ¶ğ¶ 

 ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ 

 ğ‘œğ‘œ ğ‘¥ğ‘¥ 

#### = 

 âˆ‘ ( ğ‘¥ğ‘¥ ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥Ì… 

 )( ğ‘¦ğ‘¦ ğ‘–ğ‘– 

 âˆ’ğ‘¦ğ‘¦ï¿½ 

 ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 âˆ‘ 

 ( ğ‘¥ğ‘¥ ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥Ì… 

 ) 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 f. ğ›½ğ›½ 

#### Ì‚ 

 0 

#### =ğ‘¥ğ‘¥ï¿½âˆ’ğ‘ğ‘ 

 1 

#### ğ‘¥ğ‘¥Ì… 

 g. Income predictions ğ‘¥ğ‘¥ï¿½ 

 ğ‘–ğ‘– 

#### =ğ›½ğ›½ 

#### Ì‚ 

 0 

#### +ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 h. Individual errors ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

#### =ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥ï¿½ 

 ğ‘–ğ‘– 

 i. ğ‘’ğ‘’ 

 ğœ€ğœ€ 

 2 

#### = 

 1 

 ğ‘›ğ‘›âˆ’2 

#### âˆ‘ 

#### ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

3. Can also use â€œsums of squaresâ€, rather than variance (i.e. â€œtotalâ€ not â€œaverageâ€ deviations) 

 a. Let ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### =âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

 b. Let ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### = 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### )( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥ï¿½ 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 c. With this notation, ğ›½ğ›½ 

#### Ì‚ 

 1 

#### = 

 ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ 

 2 

#### = 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 d. Let S 

 ÎµÎµ 

#### =âˆ‘ ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

4. Example : Regress Income ğ‘¥ğ‘¥ (in $ thousands) on Education ğ‘¥ğ‘¥ (in years) 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥ï¿½ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)(ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥ï¿½) 

#### ğ›½ğ›½ 

#### Ì‚ 

 0 

#### +ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

#### 11 âˆ’ 4 40 âˆ’ 30 120 37.2 2.8 

#### 16 1 80 10 10 78.2 1.8 

#### 16 1 70 0 0 78.2 8.2 

#### 14 âˆ’ 1 60 âˆ’ 10 10 61.8 1.8 

#### 18 3 100 30 90 94.6 5.4 

#### ğ‘¥ğ‘¥Ì…= 15 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### = 28 ğ‘¥ğ‘¥ï¿½= 70 ğ‘†ğ‘† 

 ğ‘¦ğ‘¦ğ‘¦ğ‘¦ 

#### = 2000 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### = 230 

#### ğœ€ğœ€Ì‚ 

 ğš¤ğš¤ 

#### ï¿½ 

#### = 0 

#### ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

 2 

#### = 7 ğ‘’ğ‘’ 

 ğ‘¦ğ‘¦ 

 2 

#### = 500 ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### = 57. 5 S 

 ÎµÎµ 

#### = 111 

#### ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

#### â‰ˆ 2. 6 ğ‘’ğ‘’ 

 ğ‘¦ğ‘¦ 

#### â‰ˆ 22. 4 ğ¶ğ¶â‰ˆ 0. 97 ğ‘’ğ‘’ 

 ğœ€ğœ€ 

 2 

#### = 37 

#### ğ¶ğ¶ 

 2 

#### â‰ˆ 0. 94 ğ‘’ğ‘’ 

 ğœ€ğœ€ 

#### = 6. 1 

#### ğ›½ğ›½ 

#### Ì‚ 

 1 

#### â‰ˆ 8. 2 

#### ğ›½ğ›½ 

#### Ì‚ 

 0 

#### â‰ˆâˆ’ 53 

---

 a. Weâ€™ll use this example again in subsequent lecture 

 b. Predictions 

 i. High school graduate ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 âˆ— 

 =12 

 âˆ— 

#### =âˆ’ 53 +8. 2 

#### ( 

#### 12 

#### ) 

#### = 45 .4 

 ii. College graduate ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 âˆ— 

 =16 

 âˆ— 

#### =âˆ’ 53 +8. 2( 16 )= 78 .2 

 iii. PhD graduate ğ‘¥ğ‘¥ï¿½ 

 ğ‘¥ğ‘¥ 

 âˆ— 

 =20 

 âˆ— 

#### =âˆ’ 53 +8. 2( 20 )= 111 

 c. Estimated errors 

 i. Predict income ğ‘¥ğ‘¥ï¿½ 

 ğ‘–ğ‘– 

 for each individual in sample 

 ii. ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

#### =ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥ï¿½ 

 ğ‘–ğ‘– 

1. Individual 

 iii. S 

 ÎµÎµ 

#### = 

#### âˆ‘ 

#### ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

#### = 111 

1. Alternatively, ğœğœ 

 ğœ€ğœ€ 

 2 

#### = 

#### ( 

#### 1 âˆ’ğœŒğœŒ 

 2 

#### ) 

#### ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

 , so ğ‘†ğ‘†ğ‘†ğ‘† ğ¸ğ¸= 

#### ( 

#### 1 âˆ’ğ¶ğ¶ 

 2 

#### ) 

#### ğ‘†ğ‘† 

 ğ‘¦ğ‘¦ğ‘¦ğ‘¦ 

#### â‰ˆ 

#### ( 

#### 1 âˆ’.9446 

#### )( 

#### 2000 

#### ) 

 = 111 (useful if only know summary statistics for ğ‘‹ğ‘‹ 

 and ğ‘Œğ‘Œ). 

 iv. ğ‘’ğ‘’ 

 ğœ€ğœ€ 

 2 

#### = 

 1 

 ğ‘›ğ‘›âˆ’2 

#### ğ‘†ğ‘†ğ‘†ğ‘† ğ¸ğ¸= 37 , ğ‘’ğ‘’ 

 ğœ€ğœ€ 

#### =âˆš 37 â‰ˆ6. 1 

 d. Illustrate with Excel: Data>Data Analysis>Regression, using education & income data 

 above 

5. Preliminaries 

 a. Algebra trick 1: It can be shown that 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 0 

#### = 

#### âˆ‘ 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ 

#### âˆ‘ 

#### ğ‘¥ğ‘¥Ì… 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =ğ‘›ğ‘›ğ‘¥ğ‘¥Ì…âˆ’ğ‘›ğ‘›ğ‘¥ğ‘¥Ì…= 0 

 Similarly, 

#### âˆ‘ ( 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘Œğ‘Œ 

#### ï¿½ 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 0 

 b. Algebra trick 2: It can be shown that 

#### ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### =âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 2 

 ğ‘–ğ‘–=1 

#### =âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)(ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ğ‘¥ğ‘¥Ì… 

#### = 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### =âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 Similarly, ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¦ğ‘¦ 

#### = 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 and ğ›½ğ›½ 

#### Ì‚ 

 1 

#### = 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### = 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 c. Deterministic ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 i. You can think of ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 and ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 as being random (i.e. they depend on who you 

 interview), and this is what we did when we derived the population regression 

 parameters. But for simplicity, assume in the estimation that ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### =ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 are 

---

 known. That is, we are only considering various samples of ğ‘›ğ‘› individuals who 

 have the same education levels as the people we sampled today (and incomes 

 that potentially differ from the people we interviewed). 

 ii. If an estimator is unbiased conditional on these ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 â€™s, it is also unbiased 

 unconditionally. For example, if ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½ğ‘‹ğ‘‹ 

 1 

#### =ğ‘¥ğ‘¥ 

 1 

#### ,ğ‘‹ğ‘‹ 

 2 

#### =ğ‘¥ğ‘¥ 

 2 

#### , ...,ğ‘‹ğ‘‹ 

 ğ‘›ğ‘› 

#### =ğ‘¥ğ‘¥ 

 ğ‘›ğ‘› 

#### ï¿½=ğ›½ğ›½ 

 1 

 for 

 every sample of ğ‘¥ğ‘¥â€™s then, averaging across all such samples, ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=ğ›½ğ›½ 

 1 

 as well. 

 iii. ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 is still random because ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### +ğœ€ğœ€ 

 ğ‘–ğ‘– 

 and ğœ€ğœ€ 

 ğ‘–ğ‘– 

 is random. 

## L21 Regression Inference (WMS 11.4-9) 

Introduction 

1. Long lecture (use time efficiently) 

2. Weâ€™ve derived estimators ğ›½ğ›½ 

#### Ì‚ 

 1 

#### , ğ›½ğ›½ 

#### Ì‚ 

 0 

#### , ğ‘Œğ‘Œ 

#### ï¿½ 

 âˆ— 

 , but so far all we have are point estimates. Are these good 

 estimators (i.e. unbiased and consistent)? What are the margins of errors? To get confidence 

 intervals or do hypothesis tests, we need to know their distributions. 

3. Estimator distributions: if ğœ€ğœ€ 

 ğ‘–ğ‘– 

#### âˆ¼ğ‘ğ‘(0,ğœğœ 

 ğœ€ğœ€ 

 2 

 ) (which is plausible, by Central Limit Theorem, if each 

 error term is viewed as the sum total of a lot of smaller, independent factors) then 

 a. ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### +ğœ€ğœ€ 

 ğ‘–ğ‘– 

#### âˆ¼ğ‘ğ‘ 

 b. ğ‘Œğ‘Œ 

#### ï¿½ 

#### = 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ¼ğ‘ğ‘ 

 c. ğ›½ğ›½ 

#### Ì‚ 

 1 

#### = 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ¼ğ‘ğ‘ 

 d. ğ›½ğ›½ 

#### Ì‚ 

 0 

#### =ğ‘Œğ‘Œ 

#### ï¿½ 

#### âˆ’ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥Ì… âˆ¼ğ‘ğ‘ 

 e. ğ‘Œğ‘Œ 

#### ï¿½ 

 âˆ— 

#### =ğ›½ğ›½ 

#### Ì‚ 

 0 

#### +ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥ 

 âˆ— 

#### âˆ¼ğ‘ğ‘ 

 f. ğ‘Œğ‘Œâˆ’ğ‘Œğ‘Œ 

#### ï¿½ 

 âˆ— 

#### âˆ¼ğ‘ğ‘ 

 g. Estimation error ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

#### =ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘Œğ‘Œ 

#### ï¿½ 

 ğ‘–ğ‘– 

#### âˆ¼ğ‘ğ‘ 

 h. 

 (ğ‘›ğ‘›âˆ’2)ğ‘œğ‘œ 

 ğœ€ğœ€ 

 2 

 ğœğœ 

 ğœ€ğœ€ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### ( 

#### ğ‘›ğ‘›âˆ’ 2 

#### ) 

 (essentially because estimating ğœ€ğœ€Ì‚ 

 ğ‘–ğ‘– 

 requires estimating two 

 parameters ğ›½ğ›½ 

#### Ì‚ 

 0 

 and ğ›½ğ›½ 

#### Ì‚ 

 1 

 , leaving only ğ‘›ğ‘›âˆ’ 2 pieces of information) 

 i. Could compare ğ‘’ğ‘’ 

 ğœ€ğœ€ 

 2 

 from two regressions to see which better explains ğ‘Œğ‘Œ, using ğ¹ğ¹ 

 distribution 

Slope estimator 

1. It can be shown that ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=ğ›½ğ›½ 

 1 

 ; unbiased ïŠ! 

---

#### ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=ğ¸ğ¸ï¿½ 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### )[ 

#### ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### +ğ¸ğ¸ 

#### ( 

#### ğœ€ğœ€ 

 ğ‘–ğ‘– 

#### )] 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### [ 

#### 0 ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ] 

#### = 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ğ›½ğ›½ 

 1 

#### =ğ›½ğ›½ 

 1 

#### 2. ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=ğ‘‰ğ‘‰ï¿½ 

 1 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 ğœğœ ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### = 

 ğœğœ ğœ€ğœ€ 

 2 

 ( ğ‘›ğ‘›âˆ’1 

 ) ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ 

 2 

 â†’ 0 ; consistent ïŠ! 

#### = 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 2 

#### ğ‘‰ğ‘‰[âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ]= 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 2 

#### [âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 2 

#### ğ‘‰ğ‘‰(ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### )+ 0 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ] 

#### = 

 1 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 2 

#### âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 2 

#### (0 +0 +ğœğœ 

 ğœ€ğœ€ 

 2 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

 ğœğœ ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### = 

 ğœğœ ğœ€ğœ€ 

 2 

 ( ğ‘›ğ‘›âˆ’1 

 ) ğ‘œğ‘œ ğ‘¥ğ‘¥ 

 2 

 Note: most noisy when incomes more varied (conditional on education); least noisy when 

 education more varied (ğ‘’ğ‘’ 

 ğ‘¥ğ‘¥ 

 2 

 in denominator) 

#### 3. 

 ğ›½ğ›½ 

 ï¿½ 

 1 

 âˆ’ğ›½ğ›½ 

 1 

 ï¿½ 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 âˆ¼ğ‘ğ‘(0, 1); therefore, 

 ğ›½ğ›½ 

 ï¿½ 

 1 

 âˆ’ğ›½ğ›½ 

 1 

 ï¿½ 

 ğ‘ ğ‘  

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ¼ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’ 2 ) 

4. Example 

 a. From previous lecture, ğ‘›ğ‘›= 5 , ğ‘’ğ‘’ 

 ğœ€ğœ€ 

 2 

#### = 37 , ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### = 28 

 b. 95% Confidence interval: $8.2ğ‘˜ğ‘˜Â±3. 182ï¿½ 

 37 

 28 

#### =[$4.5ğ‘˜ğ‘˜, $11.9ğ‘˜ğ‘˜] 

 c. Hypothesis Test ğ»ğ» 

 ğ‘ğ‘ 

#### :ğ›½ğ›½ 

 1 

 >$5ğ‘˜ğ‘˜ at ğ›¼ğ›¼=.05 level 

 i. Critical value 2.353 

 ii. Test statistic 

 8 .2âˆ’5 

 ï¿½ 

 37 

 28 

 â‰ˆ2. 78; reject ğ»ğ» 

 0 

Intercept estimator 

1. It can be shown that ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ï¿½=â‹¯=ğ›½ğ›½ 

 0 

 ; unbiased ïŠ! 

 It can be shown that ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ï¿½=ğœğœ 

 ğœ€ğœ€ 

 2 

#### ï¿½ 

 1 

 ğ‘›ğ‘› 

#### + 

 ğ‘¥ğ‘¥ 

 Ì… 

 2 

 ( ğ‘›ğ‘›âˆ’1 

 ) ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ 

 2 

 ï¿½â†’ 0 ; consistent ïŠ! 

 a. [For those curious, 

#### ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ï¿½=ğ¸ğ¸ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥Ì…ï¿½= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ [ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### +ğ¸ğ¸(ğœ€ğœ€ 

 ğ‘–ğ‘– 

#### )] 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ğ‘¥ğ‘¥Ì…ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½ 

#### = 

 ğ‘›ğ‘›ğ›½ğ›½ 

 0 

 ğ‘›ğ‘› 

#### +ğ›½ğ›½ 

 1 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### âˆ’ğ‘¥ğ‘¥Ì…ğ›½ğ›½ 

 1 

#### =ğ›½ğ›½ 

 0 

#### ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ï¿½=ğ‘‰ğ‘‰ï¿½ğ‘Œğ‘Œ 

#### ï¿½ 

#### âˆ’ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥Ì…ï¿½ 

#### =ğ‘‰ğ‘‰(ğ‘Œğ‘Œ 

#### ï¿½ 

#### )+ğ‘¥ğ‘¥Ì… 

 2 

#### ğ‘‰ğ‘‰ 

#### ï¿½ 

#### ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½ 

#### âˆ’ 2 ğ‘¥ğ‘¥ Ì…ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ 

#### ï¿½ 

#### ğ‘Œğ‘Œ 

#### ï¿½ 

#### ,ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½ 

#### = 

 ğœğœ ğœ€ğœ€ 

 2 

 ğ‘›ğ‘› 

#### +ğ‘¥ğ‘¥Ì… 

 2 

 ğœğœ ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ’0 =ğœğœ 

 ğœ€ğœ€ 

 2 

#### ï¿½ 

 1 

 ğ‘›ğ‘› 

#### + 

 ğ‘¥ğ‘¥Ì… 

 2 

 ( ğ‘›ğ‘›âˆ’1 

 ) ğ‘œğ‘œ 

 ğ‘¥ğ‘¥ 

 2 

#### ï¿½â†’ 0 

 Note: ğ¶ğ¶ï¿½ğ‘Œğ‘Œ 

#### ï¿½ 

#### ,ğ›½ğ›½ 

#### Ì‚ 

 1 

 ï¿½= 0 because... 

---

#### ğ¶ğ¶ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### , 

 1 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘Œğ‘Œ 

 ğ‘—ğ‘— 

 ğ‘›ğ‘› 

 ğ‘—ğ‘—=1 

#### ï¿½= 

 1 

 ğ‘›ğ‘›ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ğ¶ğ¶ï¿½ 

#### âˆ‘ 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### , 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ‘Œğ‘Œ 

 ğ‘—ğ‘— 

 ğ‘›ğ‘› 

 ğ‘—ğ‘—=1 

#### ï¿½ 

#### = 

 1 

 ğ‘›ğ‘›ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ï¿½ 

#### âˆ‘ ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### ) 

#### ğ¶ğ¶ 

#### ( 

#### ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### ,ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### + 

#### ( 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì… 

#### )âˆ‘ 

#### ğ¶ğ¶ï¿½ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### ,ğ‘Œğ‘Œ 

 ğ‘—ğ‘— 

#### ï¿½ 

 ğ‘–ğ‘–â‰ ğ‘—ğ‘— 

#### ï¿½ 

#### = 

 1 

 ğ‘›ğ‘›ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ï¿½âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)ğ‘‰ğ‘‰(ğ‘Œğ‘Œ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### +(ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…)âˆ‘ 0 

 ğ‘–ğ‘–â‰ ğ‘—ğ‘— 

#### ï¿½ 

#### = 

 1 

 ğ‘›ğ‘›ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ï¿½ğœğœ 

 ğ‘¦ğ‘¦ 

 2 

#### âˆ‘ (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### âˆ’ğ‘¥ğ‘¥Ì…) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½ 

#### = 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 2 

 ğ‘›ğ‘›ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### [ 0 ] ] 

2. Note two pieces: small error in identifying ï¿½ğœ‡ğœ‡ 

 ğ‘¥ğ‘¥ 

#### ,ğœ‡ğœ‡ 

 ğ‘¦ğ‘¦ 

 ï¿½ and larger error in identifying slope (which 

 matters more when ğ‘¥ğ‘¥Ì… 

 2 

 high). 

#### 3. 

 ğ›½ğ›½ 

 ï¿½ 

 0 

 âˆ’ğ›½ğ›½ 0 

 ï¿½ ğœğœ ğœ€ğœ€ 

 2 

 ï¿½ 

 1 

 ğ‘›ğ‘› 

 + 

 ğ‘¥ğ‘¥ï¿½ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ï¿½ 

 âˆ¼ğ‘ğ‘(0, 1); therefore, 

 ğ›½ğ›½ 

 ï¿½ 

 0 

 âˆ’ğ›½ğ›½ 0 

 ï¿½ ğ‘œğ‘œ ğœ€ğœ€ 

 2 

 ï¿½ 

 1 

 ğ‘›ğ‘› 

 + 

 ğ‘¥ğ‘¥ï¿½ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ï¿½ 

#### âˆ¼ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’ 2 ) 

Prediction estimator 

#### 1. (ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğš¤ğš¤ 

#### ) 

#### ï¿½ 

#### =ğ›½ğ›½ 

#### Ì‚ 

 0 

#### +ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### 2. ğ¸ğ¸ï¿½ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğš¤ğš¤ 

#### ï¿½ 

#### ï¿½=ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### +ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ï¿½=ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ; unbiased ïŠ! 

#### 3. ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğš¤ğš¤ 

#### ï¿½ 

#### ï¿½=â‹¯=ğœğœ 

 ğœ€ğœ€ 

 2 

#### ï¿½ 

 1 

 ğ‘›ğ‘› 

#### + 

 (ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥Ì…) 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ï¿½â†’ 0 ; consistent ïŠ! 

#### [ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 ğš¤ğš¤ 

#### ï¿½ 

#### ï¿½=ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ï¿½+ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 2 

#### ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½+ 2 ğ¶ğ¶ ğ‘›ğ‘›ğ‘ğ‘ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ,ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ï¿½ 

#### =ğœğœ 

 ğœ€ğœ€ 

 2 

#### ï¿½ 

 1 

 ğ‘›ğ‘› 

#### + 

 ğ‘¥ğ‘¥Ì… 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ï¿½+(ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ) 

 2 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ’ 2 ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

#### ğ‘¥ğ‘¥Ì… 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 (since ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ï¿½ğ›½ğ›½ 

#### Ì‚ 

 0 

#### ,ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=ğ¶ğ¶ğ‘›ğ‘›ğ‘ğ‘ï¿½ğ‘Œğ‘Œ 

#### ï¿½ 

#### âˆ’ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ğ‘¥ğ‘¥Ì…,ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½= 0 âˆ’ğ‘¥ğ‘¥Ì… 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ) = 

#### ğœğœ 

 ğœ€ğœ€ 

 2 

#### ï¿½ 

 1 

 ğ‘›ğ‘› 

#### + 

 ( ğ‘¥ğ‘¥ ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥Ì… 

 ) 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ï¿½=ğœğœ 

 ğœ€ğœ€ 

 2 

#### ï¿½ 

 1 

 ğ‘›ğ‘› 

#### + 

 ( ğ‘¥ğ‘¥ ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥Ì… 

 ) 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### ï¿½] 

 Note: most precise close to ğ‘¥ğ‘¥Ì…; can still make predictions far away from ğ‘¥ğ‘¥Ì…, but more noisy 

#### 4. 

 (ğ›½ğ›½ 

 0 

 +ğ›½ğ›½ 

 1 

 ğ‘¥ğ‘¥ 

 ğš¤ğš¤ 

 ) 

 ï¿½ 

 âˆ’(ğ›½ğ›½ 

 0 

 +ğ›½ğ›½ 

 1 

 ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ) 

 ï¿½ ğœğœ 

 ğœ€ğœ€ 

 2 

 ï¿½ 

 1 

 ğ‘›ğ‘› 

 + 

 ï¿½ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥ 

 ï¿½ ï¿½ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ï¿½ 

#### âˆ¼ğ‘ğ‘(0, 1) 

 ( ğ›½ğ›½ 0 

 +ğ›½ğ›½ 1 

 ğ‘¥ğ‘¥ ğš¤ğš¤ 

 ) 

 ï¿½ 

 âˆ’ 

 ( ğ›½ğ›½ 0 

 +ğ›½ğ›½ 1 

 ğ‘¥ğ‘¥ ğ‘–ğ‘– 

 ) 

 ï¿½ğ‘œğ‘œ 

 ğœ€ğœ€ 

 2 

 ï¿½ 

 1 

 ğ‘›ğ‘› 

 + 

 ï¿½ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 âˆ’ğ‘¥ğ‘¥ï¿½ï¿½ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ï¿½ 

#### âˆ¼ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’2) 

Error variance 

#### 1. 

 ( 4 )ğ‘œğ‘œ 

 ğœ€ğœ€ 

 2 

 ğœğœ 

 ğœ€ğœ€ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### ( 

#### 4 

#### ) 

#### , ğ‘’ğ‘’ 

 ğœ€ğœ€ 

#### =6. 1 

2. 95% confidence interval for ğœğœ 

 ğœ€ğœ€ 

 a.. 95=ğ‘ƒğ‘ƒï¿½. 48< 

 ( 4 )ğ‘œğ‘œ 

 ğœ€ğœ€ 

 2 

 ğœğœ 

 ğœ€ğœ€ 

 2 

#### < 11 .14ï¿½=ğ‘ƒğ‘ƒï¿½ï¿½ 

 4â‹…37 

. 48 

#### >ğœğœ 

 ğœ€ğœ€ 

#### >ï¿½ 

 4â‹…37 

. 48 

#### ï¿½=ğ‘ƒğ‘ƒ 

#### ( 

#### 17 .6 >ğœğœ 

 ğœ€ğœ€ 

#### >3. 6 

#### ) 

---

3. Test ğ»ğ» 

 ğ‘ğ‘ 

#### :ğœğœ 

 ğœ€ğœ€ 

 2 

#### > 4 

 2 

 at ğ›¼ğ›¼=.05 level 

 a. Critical value 9.49 (from Table 6) 

 b. Test statistic 

 4â‹…37 

 4 

 2 

 =9. 25; not significant 

4. If you had two regressions and wanted to know which has better predictive power (i.e. lower 

 residual error variance) you could compare ğœğœ 

 ğœ€ğœ€ğ´ğ´ 

 2 

 and ğœğœ 

 ğœ€ğœ€ğµğµ 

 2 

 using F distribution 

## Review 

1. Thanks for a great semester! 

2. Thanks TAs! 

3. Recommend Econ 388: regression with multiple variables 

 a. Weâ€™re on the brink of knowledge explosion 

 b. Also Econ 210 for exploring careers in Economics 

 c. For advanced statistics/econometrics, recommend linear algebra (Math 213) 

4. Student project findings 

 a. Wide variety of projects 

 b. Value of statistics for consumers, not just producers 

5. Key concepts 

 a. Weâ€™ve seen several trees, now letâ€™s notice the forest 

 b. Pre-midterm, we discussed population distributions (discrete or continuous), including 

 moments such as mean, variance, and covariance. 

 c. From sample, we seek to estimate population parameter: ğœ‡ğœ‡, ğœğœ, ğ‘ğ‘, ğœŒğœŒ, ğœ‡ğœ‡ 

 1 

#### âˆ’ğœ‡ğœ‡ 

 2 

#### , ğ‘ğ‘ 

 1 

#### âˆ’ğ‘ğ‘ 

 2 

#### , 

 ğœğœ 

 1 

 2 

 ğœğœ 

 2 

 2 

#### , ğ›½ğ›½ 

 0 

#### , ğ›½ğ›½ 

 1 

#### , ğ‘¥ğ‘¥ 

 âˆ— 

#### =ğ›½ğ›½ 

 0 

#### +ğ›½ğ›½ 

 1 

#### ğ‘¥ğ‘¥ 

 âˆ— 

 , or ğœƒğœƒ from another distribution function (e.g. ğ‘ğ‘, ğ‘ğ‘ from 

 uniform, ğœƒğœƒ from exponential), including distributions we havenâ€™t encountered yet (e.g. ğ‘ğ‘ 

 from geometric, ğœ†ğœ† from Poisson, ğ›½ğ›½ 

 2 

 from quadratic regression) 

 d. Deriving estimators: MOM and ML 

 e. Properties of estimators: bias, efficiency, consistency 

 f. Margin of error, confidence intervals, hypothesis tests 

 g. Matrix algebra 

---

 i. This class has focused on the correlation ğœŒğœŒ between two variables, which also 

 underlies linear regression line: slope coefficient ğœŒğœŒ 

 ğœğœ ğ‘¥ğ‘¥ 

 ğœğœ 

 ğ‘¥ğ‘¥ 

 and coefficient of 

 determination ğœŒğœŒ 

 2 

### ii. Matrix algebra allows us to generalize everything to multiple variables (e.g. ğœ·ğœ·= 

### ( 

### ğ‘¿ğ‘¿ 

 â€² 

### ğ‘¿ğ‘¿ 

### ) 

 âˆ’ğŸğŸ 

### ğ‘¿ğ‘¿ 

 â€² 

### ğ’šğ’š instead of ğ›½ğ›½= 

 âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 ğ‘¦ğ‘¦ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

 âˆ‘ ğ‘¥ğ‘¥ 

 ğ‘–ğ‘– 

 2 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

### iii. Individual slope coefficient ğ›½ğ›½ 

 1 

### then reflects partial correlation between 

### education and income, holding experience, age, race, gender, etc. fixed. 

### iv. Controlling for more variable makes stronger case for correlation as 

### causation 

6. Deriving estimator distributions 

 a. Estimators depend on ğ‘‹ğ‘‹ 

 1 

#### ,ğ‘‹ğ‘‹ 

 2 

#### , ...,ğ‘‹ğ‘‹ 

 ğ‘›ğ‘› 

 , and so are random variables with distributions 

 b. ğ¸ğ¸(ğ‘‹ğ‘‹ 

#### ï¿½ 

#### )=ğ¸ğ¸ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ¸ğ¸(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

 1 

 ğ‘›ğ‘› 

#### (ğ‘›ğ‘›ğœ‡ğœ‡)=ğœ‡ğœ‡ 

 c. ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

#### ï¿½ 

#### )=ğ‘‰ğ‘‰ï¿½ 

 1 

 ğ‘›ğ‘› 

#### âˆ‘ ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### ï¿½= 

 1 

 ğ‘›ğ‘› 

 2 

#### âˆ‘ ğ‘‰ğ‘‰(ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

#### ) 

 ğ‘›ğ‘› 

 ğ‘–ğ‘–=1 

#### = 

 1 

 ğ‘›ğ‘› 

#### (ğ‘›ğ‘›ğœğœ 

 2 

#### )= 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 d. Distribution ğ‘‹ğ‘‹ 

#### ï¿½ 

#### âˆ¼ğ‘ğ‘ï¿½ğœ‡ğœ‡, 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

 ï¿½ or 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğœğœ 

 2 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 (by normality of ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 or Central Limit 

 Theorem) 

 e. CLT also implies that 

 ğ‘›ğ‘›ï¿½âˆ’ğ‘›ğ‘› 

 ï¿½ 

 ğ‘ğ‘ 

 ( 1âˆ’ğ‘ğ‘ 

 ) 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘ğ‘(0, 1) 

 f. Difference estimators 

 ( ğ‘‹ğ‘‹ 

 ï¿½ 

 1 

 âˆ’ğ‘‹ğ‘‹ 

 ï¿½ 

 2 

 ) âˆ’(ğœ‡ğœ‡ 1 

 âˆ’ğœ‡ğœ‡ 2 

 ) 

 ï¿½ 

 ğœğœ 

 1 

 2 

 ğ‘›ğ‘› 1 

 + 

 ğœğœ 

 2 

 2 

 ğ‘›ğ‘› 2 

 âˆ¼ğ‘ğ‘(0, 1) and 

 ğ‘›ğ‘› 

 ï¿½ âˆ’ğ‘›ğ‘› 

 ï¿½ 

 ğ‘ğ‘(1âˆ’ğ‘ğ‘) 

 ğ‘›ğ‘› 

#### â‰ˆğ‘ğ‘(0, 1) 

 g. ğ‘‹ğ‘‹ 

 ğ‘–ğ‘– 

 normal implies (ğ‘›ğ‘›âˆ’ 1 ) 

 ğ‘†ğ‘† 

 2 

 ğœğœ 

 2 

#### âˆ¼ğœ’ğœ’ 

 2 

#### (ğ‘›ğ‘›âˆ’1) 

 and therefore 

 ğ‘œğ‘œ 

 ğ´ğ´ 

 2 

 /ğœğœ 

 ğ´ğ´ 

 2 

 ğ‘œğ‘œ 

 ğµğµ 

 2 

 /ğœğœ 

 ğµğµ 

 2 

#### âˆ¼ğ¹ğ¹(ğ‘›ğ‘› 

 ğ´ğ´ 

#### âˆ’1,ğ‘›ğ‘› 

 ğµğµ 

 âˆ’1) and 

 ğ‘‹ğ‘‹ 

 ï¿½ âˆ’ğœ‡ğœ‡ 

 ï¿½ 

 ğ‘†ğ‘† 

 2 

 ğ‘›ğ‘› 

#### âˆ¼ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’1) 

 h. Similarly, ğ¸ğ¸ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=â‹¯=ğ›½ğ›½ 

 1 

#### , ğ‘‰ğ‘‰ï¿½ğ›½ğ›½ 

#### Ì‚ 

 1 

#### ï¿½=â‹¯= 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 and ğœ€ğœ€ 

 ğ‘–ğ‘– 

 normal implies that ğ›½ğ›½ 

#### Ì‚ 

 1 

#### âˆ¼ğ‘ğ‘ï¿½ğ›½ğ›½ 

 1 

#### , 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

 ï¿½ or 

 ğ›½ğ›½ 

 ï¿½ 

 1 

 âˆ’ğ›½ğ›½ 

 1 

 ï¿½ 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† 

 ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ¼ğ‘ğ‘ 

#### ( 

#### 0, 1 

#### ) 

 i. (ğ‘›ğ‘›âˆ’ 2 ) 

 ğ‘†ğ‘† ğœ€ğœ€ 

 2 

 ğœğœ 

 ğœ€ğœ€ 

 2 

 âˆ¼ğœ’ğœ’(ğ‘›ğ‘›âˆ’ 2 ), implying that 

 ğ›½ğ›½ 

 ï¿½ 

 1 

 âˆ’ğ›½ğ›½ 1 

 ï¿½ 

 ğ‘†ğ‘† ğœ€ğœ€ 

 2 

 ğ‘†ğ‘† ğ‘¥ğ‘¥ğ‘¥ğ‘¥ 

#### âˆ¼ğ‘¡ğ‘¡(ğ‘›ğ‘›âˆ’2) 

7. Matrix algebra True/False question tip 

---

 a. Check simple cases (e.g. 1 Ã— 1 , 2 Ã— 2 ), but not special cases 

 b. Example: â€œAll symmetric matrices are idempotentâ€ 

 i. Try simple example: not special matrix like identity matrix or ï¿½ 

#### 1 1 

#### 1 1 

 ï¿½, but 

 something with no special properties other than symmetry, such as ï¿½ 

#### 1 3 

#### 3 2 

 ï¿½ (and 

 show that ï¿½ 

#### 1 3 

#### 3 2 

#### ï¿½ï¿½ 

#### 1 3 

#### 3 2 

#### ï¿½=ï¿½ 

#### 10 9 

#### 9 13 

#### ï¿½â‰ ï¿½ 

#### 1 3 

#### 3 2 

#### ï¿½) 

 ii. Can also try really simple examples, such as scalar matrices: 

#### ( 

#### 5 

#### )( 

#### 5 

#### ) 

#### â‰  

#### ( 

#### 5 

#### ) 

#### . 

 c. T/F questions are not â€œtrickâ€ questions, but be careful. In real world, much of statistics 

 is simply a matter of careful attention to details, and knowing exactly which inferences 

 are legitimate under exactly which circumstances. 

8. Example problems 

 a. Confidence interval for ğ‘Œğ‘Œ 

#### ï¿½ 

 âˆ— 

 b. Hypothesis test for ğ‘ğ‘Ì‚ 

 1 

#### âˆ’ğ‘ğ‘Ì‚ 

 2 

